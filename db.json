{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","path":"vendors/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/bower.json","path":"vendors/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","path":"vendors/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","path":"vendors/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","path":"vendors/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","path":"vendors/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","path":"vendors/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","path":"vendors/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","path":"vendors/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","path":"vendors/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","path":"vendors/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","path":"vendors/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","path":"vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","path":"vendors/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","path":"vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","path":"vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/404.html","hash":"fb93ce3c6db313fe41b765a28e2331ef8d31432b","modified":1484733251732},{"_id":"source/.DS_Store","hash":"b11d7218814f60db5fe504ed4a0a279fb9f82f9a","modified":1484407441000},{"_id":"source/CNAME","hash":"527d7353c5ac691ed36dc491a06c3c62d9b96174","modified":1484733251732},{"_id":"source/baidu_verify_416JKMOEsC.html","hash":"acd21e1e4f567893db210c1f269235bafa47dd4e","modified":1484384740000},{"_id":"source/favicon.ico","hash":"83fdee3f26deee5ba10c8459231c9b4c0c8eac21","modified":1484407987000},{"_id":"themes/next/.DS_Store","hash":"61d4653525ee800306d888d998f91174144e6d64","modified":1484409135000},{"_id":"themes/next/.gitignore","hash":"63d003fa46cf9665b4dab1786f9dc694812a5a79","modified":1484733251779},{"_id":"themes/next/.bowerrc","hash":"20038353db532b4c40625419d396da7359f89cbe","modified":1484733251777},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1484733251777},{"_id":"themes/next/.javascript_ignore","hash":"beb0b95736650284ceb712a162cc033847a83cd3","modified":1484733251779},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1484733251779},{"_id":"themes/next/README.en.md","hash":"fa31bbc6dd8778b8dee469740c92b3b5b59702af","modified":1484733251780},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1484733251779},{"_id":"themes/next/README.md","hash":"06aaf1241e9e1619956c86d8b1397a643840a9d1","modified":1484733251780},{"_id":"themes/next/bower.json","hash":"da39b00fcdf2e7a42af412de0a4d3617cc6d7084","modified":1484733251781},{"_id":"themes/next/_config.yml","hash":"148a37eee289a0085d6456cf53f33b46c03332fd","modified":1484735740810},{"_id":"themes/next/gulpfile.coffee","hash":"4e8c1082fa82e383494ff5b5963b7936d9c7bb2e","modified":1484733251781},{"_id":"themes/next/package.json","hash":"95eaba1607544965e432d56406bae391dd11bcbb","modified":1484733251802},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1467559008000},{"_id":"source/_posts/Spark-on-yarn的内存分配问题.md","hash":"4949189d6a2bb21c9886323b5c8527ecb51c4ac7","modified":1484733251733},{"_id":"source/_posts/Storm的消息可靠处理机制.md","hash":"e2d1c3db8c2f1bbcce1125a4098b9a39a2388965","modified":1484733251733},{"_id":"source/_posts/hadoop-yarn中ResourceManager的服务模块.md","hash":"8c1ab5d24055d931f14b7ab33888ed882d222d3c","modified":1484733251734},{"_id":"source/_posts/kafka-0-10-0启动过程分析.md","hash":"91d727638af34c37aebc10b54fbef53e28b2c811","modified":1484733251734},{"_id":"source/_posts/mac系统下hadoop-2-7源码编译、导入eclipse及打包.md","hash":"fee8ecc6f2778d3e4b3b5d0af51f593bb5b19642","modified":1484733251734},{"_id":"source/_posts/shell中的IFS分隔符.md","hash":"d81a76cd3d724ad3c749637a8389160a4299f9cc","modified":1484733251735},{"_id":"source/_posts/storm集群supervisor节点异常退出问题排查.md","hash":"a4437a7702521c16b66c37127b0ea1bd55f0f2bb","modified":1484733251736},{"_id":"source/_posts/storm源码编译及本地调试方法.md","hash":"4c20aa348049dd33e924f9eb2750d8c148b6fdec","modified":1484733251735},{"_id":"source/_posts/redis服务端连接断开问题诊断.md","hash":"f6a2dce7d8e71b00cae461b13d9834bea9fa4d58","modified":1484733251735},{"_id":"source/_posts/使用hexo-gitpage搭建博客.md","hash":"ccc769f8c3b0e537feafd9bebdf2b56e18df2257","modified":1484733251736},{"_id":"source/_posts/使用shell切割文件.md","hash":"4704f6af7548ed30df17bbab708a08a38735d380","modified":1484733251737},{"_id":"source/_posts/使用httpclient引起的tcp连接数超高问题.md","hash":"7c08123721786b5bb8bfdb5293bae45447d4fdf6","modified":1484733251736},{"_id":"source/_posts/交换空间使用率过高问题分析.md","hash":"6ce3e3a7d07f762e60bf67f11e017922643327a4","modified":1484733251736},{"_id":"source/_posts/基于zookeeper的分布式独占锁实现.md","hash":"00afa28a8195ccdc813fc78beb51b3f685f70606","modified":1484733251738},{"_id":"source/_posts/将Hadoop-RPC框架应用于多节点任务调度.md","hash":"21706c283e55b7b07b1d47914c6bb19ce2d1c91e","modified":1484733251739},{"_id":"source/_posts/前后端的CharacterEncoding不一致导致提交的表单数据丢失问题.md","hash":"c9b3738c687e982c0389e73430fea02b671180bf","modified":1484733251738},{"_id":"source/_posts/使用zookeeper协调多服务器的任务处理.md","hash":"5811ad8bea6ece75cf1e56c802b03da17f88fc55","modified":1484733251737},{"_id":"source/books/index.md","hash":"c666584a956848432bd3050eaa8b4e5bb25da5e9","modified":1484733251741},{"_id":"source/tags/index.md","hash":"e469b0367dd95bb641316c6828eead5282df0f9e","modified":1484733251742},{"_id":"source/categories/index.md","hash":"80bb6d41eba51fcc8632c03d5b5b65aa8d3a5f26","modified":1484733251741},{"_id":"source/about/index.md","hash":"ebcd6f10d5f5f4373d1e6971abd0246b42909390","modified":1484733251739},{"_id":"source/archives/index.md","hash":"21aab25ae64a4f6da0a81bbd185532ad49400deb","modified":1484733251740},{"_id":"source/images/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1467558383000},{"_id":"source/images/avatar.jpg","hash":"3b7c556ef5bd852482c88dd146b7925fe8ad8c60","modified":1467532477000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"4312fb37fa2b8663006be3c4fe01125ec01171c1","modified":1484733251778},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"2692e36cc35b1594530981e7727771f601720a43","modified":1484733251778},{"_id":"themes/next/languages/de.yml","hash":"4c3ffeb0d214c807a226dd98214958cb5483df1c","modified":1484733251781},{"_id":"themes/next/languages/default.yml","hash":"d2f6784b9c6567b64e58736e36025dbf96d863d4","modified":1484733251782},{"_id":"themes/next/languages/fr-FR.yml","hash":"d8a40fe025fad6f42df0cf16d4be2d513769b062","modified":1484733251782},{"_id":"themes/next/languages/id.yml","hash":"19537c8bae42c4c2e7d06a64537e8dfd503b7e19","modified":1484733251782},{"_id":"themes/next/languages/pt.yml","hash":"4c64594f477905d5d2d9ca2422f03175b7b0c617","modified":1484733251783},{"_id":"themes/next/languages/ru.yml","hash":"c3aedb94decf05a301662afc3398ab563dd9995a","modified":1484733251784},{"_id":"themes/next/languages/en.yml","hash":"df81ab6b1cf3c88ed053d3766381cd12eb659fe3","modified":1484733251782},{"_id":"themes/next/languages/ja.yml","hash":"e594aa42a33c489e4a65065659a01bb76c3c0cb5","modified":1484733251783},{"_id":"themes/next/languages/zh-tw.yml","hash":"04479b419c72b71fd34046f3fc33ebda4fe8de84","modified":1484733251785},{"_id":"themes/next/languages/zh-Hans.yml","hash":"c5bb12db9d6c485823f12ea1988cc83c924a3c3f","modified":1484733251784},{"_id":"themes/next/languages/zh-hk.yml","hash":"88e603eb0f3fd25c35bb37bd30372fd77bba7c46","modified":1484733251784},{"_id":"themes/next/layout/.DS_Store","hash":"3846e5ec0495b1629bd13b6539ee80f6793487d7","modified":1484409002000},{"_id":"themes/next/layout/_layout.swig","hash":"eded3045c21fc44caedb707e7e0acf7abca051fe","modified":1484733251785},{"_id":"themes/next/layout/archive.swig","hash":"b867a08f6b43de8b5d700c84b943df55917407ae","modified":1484733251800},{"_id":"themes/next/layout/category.swig","hash":"58cf08388901f7549b1fca95548b2c79173aa840","modified":1484733251800},{"_id":"themes/next/layout/index.swig","hash":"e5b52e04296203262a400e8e36ae12426d31fd5b","modified":1484733251801},{"_id":"themes/next/scripts/merge-configs.js","hash":"f8cde6953939802f92da5b7a2458c6c539e9be69","modified":1484733251802},{"_id":"themes/next/layout/tag.swig","hash":"6f764ea3ab11eeb7c530df45528d449b14f5dc62","modified":1484733251801},{"_id":"themes/next/layout/page.swig","hash":"a91e3fd7aef26e8a02e339e3372801c517f400cf","modified":1484733251801},{"_id":"themes/next/layout/post.swig","hash":"b8334c479840b7724638eec71971cbd8512ae58d","modified":1484733251801},{"_id":"themes/next/test/.jshintrc","hash":"1dae9d1cf7df1ae6d5c5efd6cffb949e9b8dcebb","modified":1484733251868},{"_id":"themes/next/source/favicon.ico","hash":"83fdee3f26deee5ba10c8459231c9b4c0c8eac21","modified":1484407987000},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1484733251868},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1484733251868},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"b87a5122dbff1d5fccf8f3d09d1640bd4b01c4a0","modified":1484733251785},{"_id":"themes/next/layout/_macro/post.swig","hash":"843389ec3cb4c2baa6a6a2bb916f803c89ca723b","modified":1484733251786},{"_id":"themes/next/layout/_macro/reward.swig","hash":"b6cb171f0ed227b82b8f7601814af2df93f3a09a","modified":1484733251786},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"d5d986be5ba1f40ab22e587c9356244aa9f81e1b","modified":1484733251787},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"d569af20d20a960d534a5b60f90c20fef519d351","modified":1484733251787},{"_id":"themes/next/layout/_partials/footer.swig","hash":"a77a2fd126ede89f5f45e08e25ffa68a58b3f864","modified":1484733251788},{"_id":"themes/next/layout/_partials/comments.swig","hash":"325dd5923d845a539fc0524ca72ce40edd1e516a","modified":1484733251787},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1484733251787},{"_id":"themes/next/layout/_partials/header.swig","hash":"46f8bbe51c0334c64c3a237bc0fe8ef73c6e58e1","modified":1484733251789},{"_id":"themes/next/layout/_partials/head.swig","hash":"0065ae49406ade2848b86bd4cd520af9d2148ece","modified":1484733251788},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1484733251789},{"_id":"themes/next/layout/_partials/search.swig","hash":"95b55fe35f2d2c22f2cc055d4379b5435314c7ec","modified":1484733251790},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c5db707b46eac6a5df1d2a77f8556945a66fd181","modified":1484733251792},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1484733251793},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1484733251793},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1484733251802},{"_id":"themes/next/scripts/tags/full-image.js","hash":"86194a05a8c6499de0b2aaa525d6de135778c0ae","modified":1484733251803},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"c9d45628330ce8bf5fbe71c9f131c7d75334c1c4","modified":1484733251799},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1484733251803},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1484733251832},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1484733251834},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1484733251834},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1484733251834},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1484733251835},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1484733251835},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1484733251836},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1484733251836},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1462448087000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1462448087000},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1484733251837},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1462448087000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1462448087000},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1484733251837},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1462448087000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1484733251789},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1484733251790},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"efa7efcbb575381b508f9aa0e0c53140eef72a7b","modified":1484733251790},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1484733251791},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"3fdde03f45a80f7a85097a40b40358adde618fc7","modified":1484733251791},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1484733251791},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"bf8e9223a40748b2e3ef77d753a8e1dbbce8095e","modified":1484733251791},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"12684840de632eb16e53ffa863166306a756fd4f","modified":1484733251792},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1484733251794},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"91c5353fcb94cc3b3f265b06ad2341734bc4c826","modified":1484733251794},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"3bb70d8d68142ee27f3cc98c2a4339757e7af3d3","modified":1484733251798},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"8ba01f1ac07fbca62a4b00f5a0a3a506122c1530","modified":1484733251797},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"5bd98c26cc188a2a30504d1330a0eaae34034db0","modified":1484733251799},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"4a5c6df1579a4ca72ed17f7dbd6d16a509aa7dc8","modified":1484733251799},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1484733251799},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1484733251793},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"b8e3663996b39590509d843f674360872b0242ac","modified":1484733251831},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"e55265c8a8a6ae0c3c08e3509de92ee62c3cb5f6","modified":1484733251831},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"a0f23e75a137d8c996c70e2059e0074f1e97a127","modified":1484733251823},{"_id":"themes/next/source/css/_variables/base.styl","hash":"63d71d2c1fff5054d65cef74409c2f766f2972e3","modified":1484733251832},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"531934ea21ef4dc9f0978512050f54834f0a6cff","modified":1484733251823},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1484733251822},{"_id":"themes/next/source/vendors/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1462448087000},{"_id":"themes/next/source/vendors/font-awesome/.bower.json","hash":"bb093f2ac1f1305069d873a7941324c8e0de3135","modified":1484733251849},{"_id":"themes/next/source/vendors/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1484733251849},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","hash":"ed80b43dbc7e3009b2f436741b9796df8eb3be02","modified":1484733251850},{"_id":"themes/next/source/vendors/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1484733251850},{"_id":"themes/next/source/vendors/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1484733251850},{"_id":"themes/next/source/vendors/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1484733251860},{"_id":"themes/next/source/vendors/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1484733251858},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1484733251860},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1484733251860},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1484733251861},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1484733251861},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1484733251861},{"_id":"themes/next/source/vendors/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1484733251862},{"_id":"themes/next/source/vendors/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1484733251863},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1484733251865},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1484733251866},{"_id":"themes/next/source/vendors/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1484733251847},{"_id":"themes/next/source/vendors/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1484733251847},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1484733251867},{"_id":"themes/next/source/vendors/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1484733251847},{"_id":"themes/next/source/vendors/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1484733251848},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"4a0da1bed19e65bd7db42421b447061bc1618710","modified":1484733251838},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1484733251838},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1484733251838},{"_id":"themes/next/source/js/src/motion.js","hash":"ff9ea37d05c269e3a140c4ab448af03efc4bcc76","modified":1484733251839},{"_id":"themes/next/source/js/src/post-details.js","hash":"458af3b1bd7783c1950808e66cedfa9fb68bf21f","modified":1484733251839},{"_id":"themes/next/source/js/src/utils.js","hash":"418d09eb4df5dcc5e8d13d7f6245b1888200b51c","modified":1484733251840},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1484733251840},{"_id":"themes/next/source/vendors/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1484733251859},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"24105e62d7f26946907fa14cd02589f899bf8122","modified":1484733251795},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"61347b9cf5c42a02f28cda4b6d920d6d17099d44","modified":1484733251796},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"ae5b8597603d4e42ee66ed121544e7b1c644767e","modified":1484733251795},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"096e7a6958b3bcacaa94361266832871ccb989c0","modified":1484733251796},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"1b6af02fd0ba3f729675cd95429a0cea4aebf358","modified":1484733251796},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1484733251796},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"c1186e609d4810ebfb3e675e9045b023a557d1db","modified":1484733251798},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"2338be12ffee58bc08197cb9da8aaf31737aaf5c","modified":1484733251798},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5a35aa0381b0e1d465b952a997194441020446ea","modified":1484733251820},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1484733251821},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1484733251822},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ad69cbf94eedacc27e756cdb9c7073416db697d0","modified":1484733251804},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"22828f5141c0cecb9ef25a110e194cdfa3a36423","modified":1484733251804},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1484733251804},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"b7d5cc29586ac796a50d90974ad99d24a5982137","modified":1484733251805},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1484733251819},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"b6ee5fefa6046086a76ddbcfafc82482816fa3e0","modified":1484733251820},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"c9218b48c56e52c06af9ce3cc8fbdae737cf16fe","modified":1484733251821},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"88559b13ce94311405b170a0506ded91273beceb","modified":1484733251809},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1484733251824},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1484733251816},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1484733251824},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1484733251825},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1484733251825},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"55b44e03054cd20ed8129bf986b15fba5fd85aad","modified":1484733251825},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1484733251826},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"124b540f059fd1ed13514362007cfc70355278c6","modified":1484733251827},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1484733251826},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1484733251827},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"13af2fb21fabfc4df4b577ce5363e13d03daff71","modified":1484733251827},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1484733251828},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1484733251828},{"_id":"themes/next/source/css/_schemes/Pisces/_full-image.styl","hash":"de31e923bf5102498f06b1ae6bdf2ea22409f3e0","modified":1484733251829},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"9887bd3894db5394c1e64e800afaae55f47e8dd0","modified":1484733251830},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1484733251830},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c9875c010bebd77b4f59d459a10455fceb0a66a1","modified":1484733251829},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"0c2d56f240afb439e469950c6b08a46175749914","modified":1484733251829},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"983c0723e8cfd84b67c2e66da0c26425a8db06e0","modified":1484733251830},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"88a5e0e95f93e4adb196bff1aac17d6cfb03768a","modified":1484733251830},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1484733251851},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","hash":"811432ad1e2d6c1f6da9a63fd919bf2a02b71dd9","modified":1484733251851},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","hash":"4c2c5f5f6cc86d775a44b944661e038b7be98149","modified":1484733251852},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1484733251845},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1484733251846},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1484733251846},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1484733251862},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1484733251862},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1462448087000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1484733251848},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"a9d064d600ee35acd66508167e1ac8c6cfdbdcd8","modified":1484733251840},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1462448087000},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1462448087000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1462448087000},{"_id":"themes/next/source/vendors/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1484733251864},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"852fd77500bda2c1a6651a14aa48d7d6222adc9d","modified":1484733251806},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"4c4ef6e997d0c6e21de39c2daa0c768e12c8c6fa","modified":1484733251805},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1484733251806},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1484733251805},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1484733251807},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1484733251807},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"dd941824210733588841897457e0cc9697ca5608","modified":1484733251816},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1484733251816},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1484733251817},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"8675b4da0c0801175fcbe172a6a7adb894064ac6","modified":1484733251818},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1484733251817},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"a83f493e494f5c73fab8f6f5b686ef1670490095","modified":1484733251817},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1484733251819},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"dcb4548d07cbb38b645b1753cf3ee7157e16921a","modified":1484733251819},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"7bd182d918f3117335a5ee87a1b544e6d2b54d7d","modified":1484733251819},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1484733251818},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1484733251811},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1484733251810},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"ca20affaeaf33c0904cb6356864fc6b78e95f447","modified":1484733251811},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"a45f5fce643eec4e1b927165229d560364bcace1","modified":1484733251810},{"_id":"themes/next/source/css/_common/components/post/post-more-link.styl","hash":"2bc3e33fdfbcf348c96ca60598f629dcd7ba3617","modified":1484733251811},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"929fac3a505bacbce6ba63009fd15851e2a8669d","modified":1484733251811},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"8355b0e9375b3245508efda0e18acd069c2aa767","modified":1484733251812},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1484733251812},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"350469437b20ecfd6f3ca45e400478f8e3f71cfb","modified":1484733251812},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1484733251812},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"104b5c79cd891506e0beaf938b083685f1da8637","modified":1484733251808},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1484733251808},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"681b7c8ce4dc47130a0ca67c1ec62be7c96e4c4f","modified":1484733251813},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"b8f9c95702e87fd0b170ab586c82c9718a245f8a","modified":1484733251808},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"70ec8d38d2b3ee1906793d1dcb68032adfa65f03","modified":1484733251807},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1484733251809},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"12e366f04497e3f44388fd40111a03e02f7c26af","modified":1484733251808},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"4866fb9453d7d4c83a1c4e55d74e4afed336eb8b","modified":1484733251810},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1484733251826},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"7f2bdd6109614d35408ee5ac3335aad4464c69c7","modified":1484733251813},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"cf900c5026ab36f31118317d0ae32a213e3ec2a9","modified":1484733251827},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1484733251813},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"821991c0890966a512b43e8b1cf9537e738a09a0","modified":1484733251814},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1484733251814},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1484733251814},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"394888efec32749b353292a59ec7f1b609d6325e","modified":1484733251815},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"06b9a99d63b4d57fdbf70b88ab7036fbc47e3f52","modified":1484733251815},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1462448087000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"702be9e57dd6ff5fa99642a1f6e3df26215b8805","modified":1484733251815},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1484733251815},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"cf900c5026ab36f31118317d0ae32a213e3ec2a9","modified":1484733251828},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1484733251844},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1484733251844},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1462448087000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1484733251845},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1484733251845},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1484733251845},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"f346b8b3df147e4059e1a7d66c52c9a6e1cec3e8","modified":1484733251856},{"_id":"public/atom.xml","hash":"20647d36de415b3ee179a44e5a4bcb133d76d792","modified":1484792312502},{"_id":"public/search.xml","hash":"7d34b14e3e52f7e6d791b6e0542e54904af539ae","modified":1484750326925},{"_id":"public/404.html","hash":"8363b16b548982ca366f9e3302b1d0ce56793d1b","modified":1484792312516},{"_id":"public/baidu_verify_416JKMOEsC.html","hash":"a9d1f8d934daa0f66e2cc863d2eb95ed37e31421","modified":1484750327997},{"_id":"public/books/index.html","hash":"6b17fe78eabf23b6049bb31acbea77090e0b5a9f","modified":1484792312516},{"_id":"public/tags/index.html","hash":"d55d629c3340eb15ba396bf351d58a53e10f6865","modified":1484792312517},{"_id":"public/categories/index.html","hash":"dd372e9e520fba04b1f90e26c52439daffa21045","modified":1484792312517},{"_id":"public/about/index.html","hash":"5bcb1d228b63d7207a0f546a4e80b7ba86363b66","modified":1484792312517},{"_id":"public/archives/index.html","hash":"17b479c3b15f2f5582c633a90d66c9e4c85c362e","modified":1484750327998},{"_id":"public/2016/07/13/storm源码编译及本地调试方法/index.html","hash":"4abf5647758cff34185fb33963a6f6bd8c546c0f","modified":1484792312517},{"_id":"public/2016/07/08/kafka-0-10-0启动过程分析/index.html","hash":"23872adc4e31abf59d4dfa3b2d28ad9ae5d4831b","modified":1484792312517},{"_id":"public/2015/11/05/Storm的消息可靠处理机制/index.html","hash":"dcb6ac84a397abdef2da1869438fdb3c6b71bf67","modified":1484792312517},{"_id":"public/2015/08/11/Spark-on-yarn的内存分配问题/index.html","hash":"1cb1d130d07518e4ea4804e0282e3fb6cab75eb9","modified":1484792312517},{"_id":"public/2015/07/03/storm集群supervisor节点异常退出问题排查/index.html","hash":"04b0d3f3ff04d8ead6b9fc3ce2351822abd15b5a","modified":1484792312517},{"_id":"public/2015/06/22/交换空间使用率过高问题分析/index.html","hash":"8983d080d18e792d9ce67d10f48870d95296cfb5","modified":1484792312517},{"_id":"public/2015/06/06/hadoop-yarn中ResourceManager的服务模块/index.html","hash":"4560f998ebf9399e0f70c7fac64da615149ddf59","modified":1484792312517},{"_id":"public/2015/05/18/mac系统下hadoop-2-7源码编译、导入eclipse及打包/index.html","hash":"459db026d0f08910b8d380ced27186ca5c1367e2","modified":1484792312517},{"_id":"public/2014/09/02/使用hexo-gitpage搭建博客/index.html","hash":"1f447e83af3575c97b15e8020c1bc838372fb739","modified":1484792312517},{"_id":"public/2014/06/01/redis服务端连接断开问题诊断/index.html","hash":"156cb2483a6205f889c564f8f593286da8e5263a","modified":1484792312517},{"_id":"public/2014/05/13/基于zookeeper的分布式独占锁实现/index.html","hash":"96d1d9e3f7047ca261764035fde6d60b2e5919a0","modified":1484792312517},{"_id":"public/2014/03/28/使用httpclient引起的tcp连接数超高问题/index.html","hash":"46bd7ac049738ae4ed4074dbfccdf136d88978d5","modified":1484792312518},{"_id":"public/2014/03/20/前后端的CharacterEncoding不一致导致提交的表单数据丢失问题/index.html","hash":"dcd5ca117b651ab732bba61e759099d6f2794b81","modified":1484792312518},{"_id":"public/2013/03/07/使用shell切割文件/index.html","hash":"0c286ff051474e1b6db3a124f5e4c1afb3958668","modified":1484792312518},{"_id":"public/2013/01/29/shell中的IFS分隔符/index.html","hash":"3517be369db6cc7bfb903a63ed4010e5385997f8","modified":1484792312518},{"_id":"public/2013/01/21/将Hadoop-RPC框架应用于多节点任务调度/index.html","hash":"4589895a82b02328b17edd62576b31ed7be6fdd2","modified":1484792312518},{"_id":"public/2012/11/13/使用zookeeper协调多服务器的任务处理/index.html","hash":"5fffab8f1be54b06de05f3314ee5bc1d06383443","modified":1484792312518},{"_id":"public/categories/Spark/index.html","hash":"605805d3b699fa5f076ddc3710d4b71888c9b3e4","modified":1484750327999},{"_id":"public/categories/Hadoop/index.html","hash":"642811fe984e12ac0cd1bf7ace42a13c1ac69b5b","modified":1484750328000},{"_id":"public/categories/Storm/index.html","hash":"420a5884a6bdf42e4b326cde2c0adaa1f9ec1529","modified":1484750328000},{"_id":"public/categories/Shell/index.html","hash":"fa9df19ba601f6a6f4b894525980fe0d6af5f73d","modified":1484750328000},{"_id":"public/categories/工具/index.html","hash":"4b0edaedc37ee8728d732c9421806962f87211db","modified":1484750328000},{"_id":"public/categories/问题分析/index.html","hash":"17ba3a777d7167391885cf49d5a98b73bfa0f972","modified":1484750328000},{"_id":"public/categories/分布式应用/index.html","hash":"f4329bff5e93ecbed0329d3549d4adae1bf30fac","modified":1484750328000},{"_id":"public/archives/page/2/index.html","hash":"a1ced60852c051729cffd0a30d4b7d118046970f","modified":1484750328000},{"_id":"public/archives/2012/11/index.html","hash":"eedcc328e579f79c77842aac5d022bd660fcb0ea","modified":1484750328000},{"_id":"public/archives/2012/index.html","hash":"4869588643be51e74255925b97cb0685293c15b8","modified":1484750328000},{"_id":"public/archives/2013/index.html","hash":"bc655ed098617f04b94d506a5760acf3f5276436","modified":1484750328000},{"_id":"public/archives/2013/01/index.html","hash":"19e325254649031eea33bf8f4dff2bc18d3263d8","modified":1484750328000},{"_id":"public/archives/2013/03/index.html","hash":"cd30c572b5c395c818bc95fd6ac832cc37bfe9fb","modified":1484750328000},{"_id":"public/archives/2014/index.html","hash":"246c2aa373e05b083604ffb1bfbaf342265fbd48","modified":1484750328000},{"_id":"public/archives/2014/03/index.html","hash":"7bda0b0e466a57d1d31d161d3110d0f2f415faab","modified":1484750328000},{"_id":"public/archives/2014/06/index.html","hash":"08c25d6ea9124d973852e09a4d3325179df3ecd7","modified":1484750328000},{"_id":"public/archives/2014/05/index.html","hash":"548ac05ab7fcfaa5739222c40e858cedeb2c0f93","modified":1484750328000},{"_id":"public/archives/2014/09/index.html","hash":"770965919bdd08bbb3621a5ab3239789446eb7f3","modified":1484750328000},{"_id":"public/archives/2015/index.html","hash":"ee781e8c2ab5150c64de56163e7d6e58172cd8cd","modified":1484750328000},{"_id":"public/archives/2015/05/index.html","hash":"1286cd5009184445c84a37ed84dd8629f8f63e9b","modified":1484750328011},{"_id":"public/archives/2015/06/index.html","hash":"116231dcce6a8083e224d0e3e923d87fd7b61310","modified":1484750328011},{"_id":"public/archives/2015/07/index.html","hash":"71dfdb27956fdae8ea6c8d676500878af9cb1c29","modified":1484750328011},{"_id":"public/archives/2015/08/index.html","hash":"99f23da3711bcdd7c1a291eb052eb3cd1caa2b56","modified":1484750328011},{"_id":"public/archives/2015/11/index.html","hash":"77d5b107fb5fc8b00ed8e8dc38a97c5865aaf727","modified":1484750328011},{"_id":"public/archives/2016/index.html","hash":"24911506ab5cfd5ccc67b4ee3737aa7081e0bc3c","modified":1484750328011},{"_id":"public/index.html","hash":"a86fdc202160b415c099b7788e2774d98cb7b526","modified":1484750328011},{"_id":"public/archives/2016/07/index.html","hash":"d9e0d0b552b0ef16aaa1978f0a3fdc5b23f7f36d","modified":1484750328011},{"_id":"public/page/2/index.html","hash":"1e13b315514a900f548414aecdc9e7effe916025","modified":1484750328012},{"_id":"public/tags/Spark/index.html","hash":"5f4becca8c05be91b9753b90d303c0e3e9985ab2","modified":1484750328012},{"_id":"public/tags/Yarn/index.html","hash":"dd0747173a9be91f7f4b92c5276240a17cff1b35","modified":1484750328012},{"_id":"public/tags/内存分配/index.html","hash":"087dfe710b31a90f0c9b17c772a97ea5c7b578b7","modified":1484750328012},{"_id":"public/tags/hadoop/index.html","hash":"0a62909370354cd8cb8bf3aafd9d8263a9d0bc8d","modified":1484750328012},{"_id":"public/tags/yarn/index.html","hash":"0481d1b3d46f21393844cb76f52cde6d37c0f25e","modified":1484750328012},{"_id":"public/tags/Storm/index.html","hash":"ef46c7ae2885ff11441629c03622461efbe06d78","modified":1484750328012},{"_id":"public/tags/实时计算/index.html","hash":"35c3a1c34777a8500a3250a57962c471d2697934","modified":1484750328012},{"_id":"public/tags/kafka/index.html","hash":"b470c4958df7ee861b02e977e21c36dec6494ff3","modified":1484750328012},{"_id":"public/tags/源码分析/index.html","hash":"ba3c31b118d94cdf09da7fdb3cd70193ff19e452","modified":1484750328012},{"_id":"public/tags/eclipse/index.html","hash":"aefc18f4e5f40222e1043bc3d2d6672f043dbd5a","modified":1484750328012},{"_id":"public/tags/shell/index.html","hash":"6678ba310cbd1f47a2a4cc28a5b83cf35cf017db","modified":1484750328012},{"_id":"public/tags/Supervisor/index.html","hash":"c8dcc97c1fbef3fee00b6bee7ec01fa9df6a7dc8","modified":1484750328012},{"_id":"public/tags/异常排查/index.html","hash":"96e58b910c2ddeee3f96093486c4eff88019fc38","modified":1484750328013},{"_id":"public/tags/storm/index.html","hash":"900e23983d89d1d634a571c9d7df75eaec70cd8c","modified":1484750328013},{"_id":"public/categories/Kafka/index.html","hash":"d80ca181fe84e7b67480d7bf1fa3e3e26fd0669a","modified":1484750328013},{"_id":"public/tags/redis/index.html","hash":"50617e01ac2b04bab65ec8ec6dd50e2aae9c5b47","modified":1484750328013},{"_id":"public/categories/Redis/index.html","hash":"e170988d8b5addfe79caf779010fd809dc87ab65","modified":1484750328013},{"_id":"public/tags/连接断开/index.html","hash":"c9e28564f7b19f7ff96763f3a1ff2f930e4c9ebd","modified":1484750328013},{"_id":"public/tags/hexo/index.html","hash":"4d14bb5ec7b4446638783329e6d5d6ee981022cb","modified":1484750328013},{"_id":"public/tags/gitpage/index.html","hash":"0ff4fb226d83acafd9f1420f67f466d19ce52bdf","modified":1484750328013},{"_id":"public/tags/linux/index.html","hash":"47c7b5138ddf142803dd1a9d80dadfa9b4e0785c","modified":1484750328013},{"_id":"public/tags/httpclient/index.html","hash":"6ac1829d99eb08dabab2b1394c20ac310b98ab5a","modified":1484750328013},{"_id":"public/tags/tcp连接数/index.html","hash":"01662b91912c4e8167355ff4cc3cc1e4292e0d17","modified":1484750328013},{"_id":"public/tags/swap分区/index.html","hash":"f882b26e094ce9362e79d985b9d0dea054e01cc8","modified":1484750328013},{"_id":"public/tags/Jvm调优/index.html","hash":"6cc1405e8637f9b1f4a51e2c94b74bd65f4feb92","modified":1484750328013},{"_id":"public/tags/问题分析/index.html","hash":"c6318c75fba1a93d23ad78bdfdb9c2cf7d78dac8","modified":1484750328013},{"_id":"public/tags/zookeeper/index.html","hash":"9b71d8fb8d69b1669b94d3b4b0638dd36060ceec","modified":1484750328014},{"_id":"public/tags/源码编译/index.html","hash":"6dd09321451e2178add2958c11e693f3985c7933","modified":1484750328014},{"_id":"public/tags/分布式协调/index.html","hash":"3d6d26992f7a3f3657fd49becd4c5e1fb35fb2c5","modified":1484750328014},{"_id":"public/tags/web开发/index.html","hash":"17e5d4096111ad1572ad472d2b196d48e16bfcf6","modified":1484750328014},{"_id":"public/tags/java/index.html","hash":"aa591074cf3befcc4df3af220603b7a493db09b0","modified":1484750328014},{"_id":"public/tags/RPC/index.html","hash":"6c9ad6e8017ed72d264744f0c28bd83e943fffdc","modified":1484750328014},{"_id":"public/tags/任务调度/index.html","hash":"ec754a599359267274e7514fdf438552427dfc4c","modified":1484750328014},{"_id":"public/tags/横向扩展/index.html","hash":"6540753d407a25baf24a588ad76f19cced7efec0","modified":1484750328014},{"_id":"public/tags/本地调试/index.html","hash":"07b2b2f86bf5f914101bbc7280a71a0507388b2f","modified":1484750328014},{"_id":"public/tags/分布式应用/index.html","hash":"57b8dd9044d10a644916fda77716561aac05fcf8","modified":1484750328015},{"_id":"public/CNAME","hash":"527d7353c5ac691ed36dc491a06c3c62d9b96174","modified":1484792312524},{"_id":"public/favicon.ico","hash":"83fdee3f26deee5ba10c8459231c9b4c0c8eac21","modified":1484750328023},{"_id":"public/images/avatar.jpg","hash":"3b7c556ef5bd852482c88dd146b7925fe8ad8c60","modified":1484750328023},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1484792312524},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1484792312525},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1484792312525},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1484792312525},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1484750328023},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1484750328023},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1484792312525},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1484750328023},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1484750328023},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1484792312525},{"_id":"public/vendors/font-awesome/HELP-US-OUT.txt","hash":"ed80b43dbc7e3009b2f436741b9796df8eb3be02","modified":1484792312525},{"_id":"public/vendors/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1484792312525},{"_id":"public/vendors/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1484792312526},{"_id":"public/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1484750328024},{"_id":"public/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1484750328024},{"_id":"public/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1484750328024},{"_id":"public/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1484750328024},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1484750328024},{"_id":"public/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1484750328024},{"_id":"public/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1484750328024},{"_id":"public/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1484750328024},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1484792312525},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1484792312525},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1484792312525},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1484750329115},{"_id":"public/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1484750329118},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1484750329119},{"_id":"public/vendors/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1484750329128},{"_id":"public/vendors/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1484750329128},{"_id":"public/vendors/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1484750329128},{"_id":"public/vendors/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1484750329128},{"_id":"public/vendors/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1484750329128},{"_id":"public/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1484750329128},{"_id":"public/vendors/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1484750329129},{"_id":"public/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1484750329129},{"_id":"public/vendors/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1484750329129},{"_id":"public/vendors/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1484750329129},{"_id":"public/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1484750329129},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1484750329129},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1484750329129},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1484750329134},{"_id":"public/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1484750329135},{"_id":"public/js/src/utils.js","hash":"e5cb720894c4bc28ca8f10b33df127fb394018d9","modified":1484750329135},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1484750329135},{"_id":"public/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1484750329135},{"_id":"public/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1484750329135},{"_id":"public/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1484750329135},{"_id":"public/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1484750329135},{"_id":"public/js/src/schemes/pisces.js","hash":"7506e7490c69a200831393c38d25e91c156bd471","modified":1484750329135},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1484750329135},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1484750329135},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1484750329135},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1484750329135},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1484750329135},{"_id":"public/css/main.css","hash":"4a48b696dd14eebf46eaf5cecad4f1734dc1b4d5","modified":1484792315016},{"_id":"public/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1484750329136},{"_id":"public/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1484750329136},{"_id":"public/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1484750329136},{"_id":"public/vendors/font-awesome/css/font-awesome.css","hash":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1484750329136},{"_id":"public/vendors/font-awesome/css/font-awesome.min.css","hash":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1484750329136},{"_id":"public/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1484750329137},{"_id":"public/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1484750329137},{"_id":"public/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1484750329137},{"_id":"public/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1484750329137},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1484750329137},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"f346b8b3df147e4059e1a7d66c52c9a6e1cec3e8","modified":1484792315069},{"_id":"public/baidusitemap.xml","hash":"3a6ecd0cd25dfe7d95b5399662f81b0b9c89e369","modified":1484792312470},{"_id":"public/sitemap.xml","hash":"97a422f72fe70cf9795912d02284352787854152","modified":1484792312502}],"Category":[{"name":"Spark","_id":"ciy320ain0005ifs6dzlnba6z"},{"name":"Hadoop","_id":"ciy320aix000aifs6bleqwoza"},{"name":"Storm","_id":"ciy320aj2000fifs6l8nt5ft1"},{"name":"Kafka","_id":"ciy320aj6000kifs6g9ksjiex"},{"name":"Shell","_id":"ciy320ajh000xifs6istm24ao"},{"name":"Redis","_id":"ciy320ajt0019ifs6btbm5z6j"},{"name":"工具","_id":"ciy320aju001fifs6gl5fnma6"},{"name":"问题分析","_id":"ciy320ajv001iifs6wl78olid"},{"name":"分布式应用","_id":"ciy320ak3001qifs6y1wqwe9r"}],"Data":[],"Page":[{"_content":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n</head>\n<body>\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\"\n        charset=\"utf-8\" homePageUrl=\"https://maohong.github.io\"\n        homePageName=\"回到我的主页\">\n</script>\n</body>\n</html>\n","source":"404.html","raw":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n</head>\n<body>\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\"\n        charset=\"utf-8\" homePageUrl=\"https://maohong.github.io\"\n        homePageName=\"回到我的主页\">\n</script>\n</body>\n</html>\n","date":"2017-01-18T09:54:11.732Z","updated":"2017-01-18T09:54:11.732Z","path":"404.html","_id":"ciy320ah80000ifs6l137o8i3","title":"","comments":1,"layout":"page","content":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\">\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\n  <meta name=\"robots\" content=\"all\">\n  <meta name=\"robots\" content=\"index,follow\">\n</head>\n<body>\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\" charset=\"utf-8\" homepageurl=\"https://maohong.github.io\" homepagename=\"回到我的主页\">\n</script>\n</body>\n</html>\n","excerpt":"","more":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n</head>\n<body>\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\"\n        charset=\"utf-8\" homePageUrl=\"https://maohong.github.io\"\n        homePageName=\"回到我的主页\">\n</script>\n</body>\n</html>\n"},{"title":"books","date":"2017-01-14T16:16:30.000Z","_content":"Java：\n--\n《深入理解Java虚拟机》\n\n《Java并发编程实战》\n\nTo be added ... ","source":"books/index.md","raw":"---\ntitle: books\ndate: 2017-01-15 00:16:30\n---\nJava：\n--\n《深入理解Java虚拟机》\n\n《Java并发编程实战》\n\nTo be added ... ","updated":"2017-01-18T09:54:11.741Z","path":"books/index.html","_id":"ciy320akz003mifs6uvpo99yd","comments":1,"layout":"page","content":"<h2 id=\"Java：\"><a href=\"#Java：\" class=\"headerlink\" title=\"Java：\"></a>Java：</h2><p>《深入理解Java虚拟机》</p>\n<p>《Java并发编程实战》</p>\n<p>To be added … </p>\n","excerpt":"","more":"<h2 id=\"Java：\"><a href=\"#Java：\" class=\"headerlink\" title=\"Java：\"></a>Java：</h2><p>《深入理解Java虚拟机》</p>\n<p>《Java并发编程实战》</p>\n<p>To be added … </p>\n"},{"title":"标签","date":"2016-07-03T07:30:44.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2016-07-03 15:30:44\ntype: \"tags\"\n---\n","updated":"2017-01-18T09:54:11.742Z","path":"tags/index.html","_id":"ciy320al1003nifs64an8h76h","comments":1,"layout":"page","content":"","excerpt":"","more":""},{"title":"分类","date":"2016-07-03T07:22:46.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2016-07-03 15:22:46\ntype: \"categories\"\n---\n","updated":"2017-01-18T09:54:11.741Z","path":"categories/index.html","_id":"ciy320al4003oifs6ww7nwfis","comments":1,"layout":"page","content":"","excerpt":"","more":""},{"title":"about","date":"2016-03-03T07:16:19.000Z","_content":"Interests:\n--\nJava/Python/Scala/Shell/BigData/Hadoop/HBase/Hive/Spark/SQL/NoSQL/ML and serious of open-source projects.\n\nContact：\n--\n新浪微博：@maohong-V  \nGithub：https://github.com/maohong  \nEmail：maohong.tech@gmail.com","source":"about/index.md","raw":"---\ntitle: about\ndate: 2016-03-03 15:16:19\n---\nInterests:\n--\nJava/Python/Scala/Shell/BigData/Hadoop/HBase/Hive/Spark/SQL/NoSQL/ML and serious of open-source projects.\n\nContact：\n--\n新浪微博：@maohong-V  \nGithub：https://github.com/maohong  \nEmail：maohong.tech@gmail.com","updated":"2017-01-18T09:54:11.739Z","path":"about/index.html","_id":"ciy320al7003pifs6tmjisvlh","comments":1,"layout":"page","content":"<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests:\"></a>Interests:</h2><p>Java/Python/Scala/Shell/BigData/Hadoop/HBase/Hive/Spark/SQL/NoSQL/ML and serious of open-source projects.</p>\n<h2 id=\"Contact：\"><a href=\"#Contact：\" class=\"headerlink\" title=\"Contact：\"></a>Contact：</h2><p>新浪微博：@maohong-V<br>Github：<a href=\"https://github.com/maohong\" target=\"_blank\" rel=\"external\">https://github.com/maohong</a><br>Email：maohong.tech@gmail.com</p>\n","excerpt":"","more":"<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests:\"></a>Interests:</h2><p>Java/Python/Scala/Shell/BigData/Hadoop/HBase/Hive/Spark/SQL/NoSQL/ML and serious of open-source projects.</p>\n<h2 id=\"Contact：\"><a href=\"#Contact：\" class=\"headerlink\" title=\"Contact：\"></a>Contact：</h2><p>新浪微博：@maohong-V<br>Github：<a href=\"https://github.com/maohong\">https://github.com/maohong</a><br>Email：maohong.tech@gmail.com</p>\n"},{"title":"归档","date":"2016-07-03T07:36:16.000Z","_content":"","source":"archives/index.md","raw":"---\ntitle: 归档\ndate: 2016-07-03 15:36:16\n---\n","updated":"2017-01-18T09:54:11.740Z","path":"archives/index.html","_id":"ciy320al8003qifs62mbc8ydi","comments":1,"layout":"page","content":"","excerpt":"","more":""}],"Post":[{"title":"Spark on yarn的内存分配问题","date":"2015-08-11T05:23:13.000Z","_content":"\n问题描述\n--\n\n在测试spark on yarn时，发现一些内存分配上的问题，具体如下。\n\n在$SPARK_HOME/conf/spark-env.sh中配置如下参数：\n\n> SPARK_EXECUTOR_INSTANCES=4            *在yarn集群中启动的executor进程数*\n> \n> SPARK_EXECUTOR_MEMORY=2G              *为每个executor进程分配的内存大小*\n> \n> SPARK_DRIVER_MEMORY=1G                *为spark-driver进程分配的内存大小*\n\n执行$SPARK_HOME/bin/spark-sql –master yarn，按yarn-client模式启动spark-sql交互命令行（即driver程序运行在本地，而非yarn的container中），日志显示的关于AppMaster和Executor的内存信息如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-1.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-2.png)\n\n日志显示，AppMaster的内存是896MB，其中包含了384MB的memoryOverhead；启动了5个executor，第一个的可用内存是530.3MB，其余每个Executor的可用内存是1060.3MB。\n\n到yarnUI看下资源使用情况，共启动了5个container，占用内存13G，其中一台NodeManager启动了2个container，占用内存4G（1个AppMaster占1G、另一个占3G），另外3台各启了1个container，每个占用3G内存。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-3.png)\n\n再到sparkUI看下executors的情况，这里有5个executor，其中driver是运行在执行spark-sql命令的本地服务器上，另外4个是运行在yarn集群中。Driver的可用storage memory为530.3MB，另外4个都是1060.3MB（与日志信息一致）。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-4.png)\n\n那么问题来了：\n\n1. Yarn为container分配的最小内存由yarn.scheduler.minimum-allocation-mb参数决定，默认是1G，从yarnUI中看确实如此，可为何spark的日志里显示AppMaster的实际内存是896-384=512MB呢？384MB是怎么算出来的？\n\n2. spark配置文件里指定了每个executor的内存为2G，为何日志和sparkUI上显示的是1060.3MB？\n\n3. driver的内存配置为1G，为何sparkUI里显示的是530.3MB呢？\n\n4. 为何yarn中每个container分配的内存是3G，而不是executor需要的2G呢？\n\n问题解析\n--\n进过一番调研，发现这里有些概念容易混淆，整理如下，序号对应上面的问题：\n<!--more-->\n(1) spark的yarn-client向ResourceManager申请提交作业/启动AppMaster时，会判断是否是集群模式，如果是集群模式，则AppMaster的内存大小与driver内存大小一致，否则由spark.yarn.am.memory决定，这个参数的默认值是512MB。我们使用的是yarn-client模式，所以实际内存是512MB。\n\n<font color='red'>384MB是spark-client为appMaster额外申请的内存</font>，计算方法如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-5.png)\n\n即，默认从参数读取（集群模式从spark.yarn.driver.memoryOverhead参数读，否则从spark.yarn.am.memoryOverhead参数读），若没配此参数，则从AppMaster的内存*一定系数和默认最小overhead中取较大值。\n\n在spark-1.4.1版本中，MEMORY_OVERHEAD_FACTOR的默认值为0.10（之前是0.07），MEMORY_OVERHEAD_MIN默认为384，我们没有指定spark.yarn.driver.memoryOverhead和spark.yarn.am.memoryOverhead，而amMemory=512M（由spark.yarn.am.memory决定），因此memoryOverhead为max(512*0.10, 384)=384MB。\n\nExecutor的memoryOverhead计算方法与此一样，只是不区分是否集群模式，都默认由spark.yarn.executor.memoryOverhead配置。\n\n(2) <font color='red'>日志和sparkUI上显示的是executor内部用于缓存计算结果的内存空间，并不是executor所拥有的全部内存</font>。这部分内存是由以下公式计算：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-6.png)\n\nRuntime.getRuntime.maxMemory按2048MB算，storage memory大小为1105.92MB，sparkUI显示的略小于此值，是正常的。\n\n(3) 与上述第2点一样，storage memory的大小略小于1024*0.9*0.6=552.96MB\n\n(4) 前面提到spark会为container额外申请一部分内存（memoryOverhead），因此，实际为container提交申请的内存大小是2048 + max(2048*0.10, 384) = 2432MB，而<font color='red'>yarn在做资源分配时会做资源规整化，即应用程序申请的资源量一定是最小可申请资源量的整数倍（向上取整）</font>，最小可申请内存量由yarn.scheduler.minimum-allocation-mb指定，因此，会为container分配3G内存。\n\n验证\n--\n\n为了验证上述规则，继续修改配置参数：\n\n> SPARK_EXECUTOR_INSTANCES=4          *在yarn集群中启动的executor进程数*\n> \n> SPARK_EXECUTOR_MEMORY=4G            *为每个executor进程分配的内存大小*\n> \n> SPARK_DRIVER_MEMORY=3G              *为spark-driver进程分配的内存大小*\n\n并在启动spark-sql时指定spark.yarn.am.memory参数：\n\n**bin/spark-sql –master yarn –conf spark.yarn.am.memory=1024m**\n\n再看日志信息：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-7.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-8.png)\n\nyarnUI状态：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-9.png)\n\nsparkUI的executors信息：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-10.png)\n\n可见，AppMaster的实际内存为1024M（1408-384），而其在yarn中的container内存大小为2G（1408大于1G，yarn按资源规整化原则为其分配2G）。\n\n同理，driver的storage memory空间为3G\\*0.9\\*0.6=1.62G，executor的storage memory空间为4G\\*0.9\\*0.6=2.16G，executor所在container占用5G内存（4096+max(4096*0.10,384)= 4505.6，大于4G， yarn按资源规整化原则为其分配5G）。\n\nYarn集群的内存总占用空间为2+5*4=22G。","source":"_posts/Spark-on-yarn的内存分配问题.md","raw":"---\ntitle: Spark on yarn的内存分配问题\ndate: 2015-08-11 13:23:13\ntags:\n- Spark\n- Yarn\n- 内存分配\ncategories:\n- Spark\n---\n\n问题描述\n--\n\n在测试spark on yarn时，发现一些内存分配上的问题，具体如下。\n\n在$SPARK_HOME/conf/spark-env.sh中配置如下参数：\n\n> SPARK_EXECUTOR_INSTANCES=4            *在yarn集群中启动的executor进程数*\n> \n> SPARK_EXECUTOR_MEMORY=2G              *为每个executor进程分配的内存大小*\n> \n> SPARK_DRIVER_MEMORY=1G                *为spark-driver进程分配的内存大小*\n\n执行$SPARK_HOME/bin/spark-sql –master yarn，按yarn-client模式启动spark-sql交互命令行（即driver程序运行在本地，而非yarn的container中），日志显示的关于AppMaster和Executor的内存信息如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-1.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-2.png)\n\n日志显示，AppMaster的内存是896MB，其中包含了384MB的memoryOverhead；启动了5个executor，第一个的可用内存是530.3MB，其余每个Executor的可用内存是1060.3MB。\n\n到yarnUI看下资源使用情况，共启动了5个container，占用内存13G，其中一台NodeManager启动了2个container，占用内存4G（1个AppMaster占1G、另一个占3G），另外3台各启了1个container，每个占用3G内存。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-3.png)\n\n再到sparkUI看下executors的情况，这里有5个executor，其中driver是运行在执行spark-sql命令的本地服务器上，另外4个是运行在yarn集群中。Driver的可用storage memory为530.3MB，另外4个都是1060.3MB（与日志信息一致）。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-4.png)\n\n那么问题来了：\n\n1. Yarn为container分配的最小内存由yarn.scheduler.minimum-allocation-mb参数决定，默认是1G，从yarnUI中看确实如此，可为何spark的日志里显示AppMaster的实际内存是896-384=512MB呢？384MB是怎么算出来的？\n\n2. spark配置文件里指定了每个executor的内存为2G，为何日志和sparkUI上显示的是1060.3MB？\n\n3. driver的内存配置为1G，为何sparkUI里显示的是530.3MB呢？\n\n4. 为何yarn中每个container分配的内存是3G，而不是executor需要的2G呢？\n\n问题解析\n--\n进过一番调研，发现这里有些概念容易混淆，整理如下，序号对应上面的问题：\n<!--more-->\n(1) spark的yarn-client向ResourceManager申请提交作业/启动AppMaster时，会判断是否是集群模式，如果是集群模式，则AppMaster的内存大小与driver内存大小一致，否则由spark.yarn.am.memory决定，这个参数的默认值是512MB。我们使用的是yarn-client模式，所以实际内存是512MB。\n\n<font color='red'>384MB是spark-client为appMaster额外申请的内存</font>，计算方法如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-5.png)\n\n即，默认从参数读取（集群模式从spark.yarn.driver.memoryOverhead参数读，否则从spark.yarn.am.memoryOverhead参数读），若没配此参数，则从AppMaster的内存*一定系数和默认最小overhead中取较大值。\n\n在spark-1.4.1版本中，MEMORY_OVERHEAD_FACTOR的默认值为0.10（之前是0.07），MEMORY_OVERHEAD_MIN默认为384，我们没有指定spark.yarn.driver.memoryOverhead和spark.yarn.am.memoryOverhead，而amMemory=512M（由spark.yarn.am.memory决定），因此memoryOverhead为max(512*0.10, 384)=384MB。\n\nExecutor的memoryOverhead计算方法与此一样，只是不区分是否集群模式，都默认由spark.yarn.executor.memoryOverhead配置。\n\n(2) <font color='red'>日志和sparkUI上显示的是executor内部用于缓存计算结果的内存空间，并不是executor所拥有的全部内存</font>。这部分内存是由以下公式计算：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-6.png)\n\nRuntime.getRuntime.maxMemory按2048MB算，storage memory大小为1105.92MB，sparkUI显示的略小于此值，是正常的。\n\n(3) 与上述第2点一样，storage memory的大小略小于1024*0.9*0.6=552.96MB\n\n(4) 前面提到spark会为container额外申请一部分内存（memoryOverhead），因此，实际为container提交申请的内存大小是2048 + max(2048*0.10, 384) = 2432MB，而<font color='red'>yarn在做资源分配时会做资源规整化，即应用程序申请的资源量一定是最小可申请资源量的整数倍（向上取整）</font>，最小可申请内存量由yarn.scheduler.minimum-allocation-mb指定，因此，会为container分配3G内存。\n\n验证\n--\n\n为了验证上述规则，继续修改配置参数：\n\n> SPARK_EXECUTOR_INSTANCES=4          *在yarn集群中启动的executor进程数*\n> \n> SPARK_EXECUTOR_MEMORY=4G            *为每个executor进程分配的内存大小*\n> \n> SPARK_DRIVER_MEMORY=3G              *为spark-driver进程分配的内存大小*\n\n并在启动spark-sql时指定spark.yarn.am.memory参数：\n\n**bin/spark-sql –master yarn –conf spark.yarn.am.memory=1024m**\n\n再看日志信息：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-7.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-8.png)\n\nyarnUI状态：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-9.png)\n\nsparkUI的executors信息：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-10.png)\n\n可见，AppMaster的实际内存为1024M（1408-384），而其在yarn中的container内存大小为2G（1408大于1G，yarn按资源规整化原则为其分配2G）。\n\n同理，driver的storage memory空间为3G\\*0.9\\*0.6=1.62G，executor的storage memory空间为4G\\*0.9\\*0.6=2.16G，executor所在container占用5G内存（4096+max(4096*0.10,384)= 4505.6，大于4G， yarn按资源规整化原则为其分配5G）。\n\nYarn集群的内存总占用空间为2+5*4=22G。","slug":"Spark-on-yarn的内存分配问题","published":1,"updated":"2017-01-18T09:54:11.733Z","_id":"ciy320aid0002ifs67an9x82r","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>在测试spark on yarn时，发现一些内存分配上的问题，具体如下。</p>\n<p>在$SPARK_HOME/conf/spark-env.sh中配置如下参数：</p>\n<blockquote>\n<p>SPARK_EXECUTOR_INSTANCES=4            <em>在yarn集群中启动的executor进程数</em></p>\n<p>SPARK_EXECUTOR_MEMORY=2G              <em>为每个executor进程分配的内存大小</em></p>\n<p>SPARK_DRIVER_MEMORY=1G                <em>为spark-driver进程分配的内存大小</em></p>\n</blockquote>\n<p>执行$SPARK_HOME/bin/spark-sql –master yarn，按yarn-client模式启动spark-sql交互命令行（即driver程序运行在本地，而非yarn的container中），日志显示的关于AppMaster和Executor的内存信息如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-1.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-2.png\" alt=\"\"></p>\n<p>日志显示，AppMaster的内存是896MB，其中包含了384MB的memoryOverhead；启动了5个executor，第一个的可用内存是530.3MB，其余每个Executor的可用内存是1060.3MB。</p>\n<p>到yarnUI看下资源使用情况，共启动了5个container，占用内存13G，其中一台NodeManager启动了2个container，占用内存4G（1个AppMaster占1G、另一个占3G），另外3台各启了1个container，每个占用3G内存。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-3.png\" alt=\"\"></p>\n<p>再到sparkUI看下executors的情况，这里有5个executor，其中driver是运行在执行spark-sql命令的本地服务器上，另外4个是运行在yarn集群中。Driver的可用storage memory为530.3MB，另外4个都是1060.3MB（与日志信息一致）。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-4.png\" alt=\"\"></p>\n<p>那么问题来了：</p>\n<ol>\n<li><p>Yarn为container分配的最小内存由yarn.scheduler.minimum-allocation-mb参数决定，默认是1G，从yarnUI中看确实如此，可为何spark的日志里显示AppMaster的实际内存是896-384=512MB呢？384MB是怎么算出来的？</p>\n</li>\n<li><p>spark配置文件里指定了每个executor的内存为2G，为何日志和sparkUI上显示的是1060.3MB？</p>\n</li>\n<li><p>driver的内存配置为1G，为何sparkUI里显示的是530.3MB呢？</p>\n</li>\n<li><p>为何yarn中每个container分配的内存是3G，而不是executor需要的2G呢？</p>\n</li>\n</ol>\n<h2 id=\"问题解析\"><a href=\"#问题解析\" class=\"headerlink\" title=\"问题解析\"></a>问题解析</h2><p>进过一番调研，发现这里有些概念容易混淆，整理如下，序号对应上面的问题：<br><a id=\"more\"></a><br>(1) spark的yarn-client向ResourceManager申请提交作业/启动AppMaster时，会判断是否是集群模式，如果是集群模式，则AppMaster的内存大小与driver内存大小一致，否则由spark.yarn.am.memory决定，这个参数的默认值是512MB。我们使用的是yarn-client模式，所以实际内存是512MB。</p>\n<p><font color=\"red\">384MB是spark-client为appMaster额外申请的内存</font>，计算方法如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-5.png\" alt=\"\"></p>\n<p>即，默认从参数读取（集群模式从spark.yarn.driver.memoryOverhead参数读，否则从spark.yarn.am.memoryOverhead参数读），若没配此参数，则从AppMaster的内存*一定系数和默认最小overhead中取较大值。</p>\n<p>在spark-1.4.1版本中，MEMORY_OVERHEAD_FACTOR的默认值为0.10（之前是0.07），MEMORY_OVERHEAD_MIN默认为384，我们没有指定spark.yarn.driver.memoryOverhead和spark.yarn.am.memoryOverhead，而amMemory=512M（由spark.yarn.am.memory决定），因此memoryOverhead为max(512*0.10, 384)=384MB。</p>\n<p>Executor的memoryOverhead计算方法与此一样，只是不区分是否集群模式，都默认由spark.yarn.executor.memoryOverhead配置。</p>\n<p>(2) <font color=\"red\">日志和sparkUI上显示的是executor内部用于缓存计算结果的内存空间，并不是executor所拥有的全部内存</font>。这部分内存是由以下公式计算：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-6.png\" alt=\"\"></p>\n<p>Runtime.getRuntime.maxMemory按2048MB算，storage memory大小为1105.92MB，sparkUI显示的略小于此值，是正常的。</p>\n<p>(3) 与上述第2点一样，storage memory的大小略小于1024<em>0.9</em>0.6=552.96MB</p>\n<p>(4) 前面提到spark会为container额外申请一部分内存（memoryOverhead），因此，实际为container提交申请的内存大小是2048 + max(2048*0.10, 384) = 2432MB，而<font color=\"red\">yarn在做资源分配时会做资源规整化，即应用程序申请的资源量一定是最小可申请资源量的整数倍（向上取整）</font>，最小可申请内存量由yarn.scheduler.minimum-allocation-mb指定，因此，会为container分配3G内存。</p>\n<h2 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h2><p>为了验证上述规则，继续修改配置参数：</p>\n<blockquote>\n<p>SPARK_EXECUTOR_INSTANCES=4          <em>在yarn集群中启动的executor进程数</em></p>\n<p>SPARK_EXECUTOR_MEMORY=4G            <em>为每个executor进程分配的内存大小</em></p>\n<p>SPARK_DRIVER_MEMORY=3G              <em>为spark-driver进程分配的内存大小</em></p>\n</blockquote>\n<p>并在启动spark-sql时指定spark.yarn.am.memory参数：</p>\n<p><strong>bin/spark-sql –master yarn –conf spark.yarn.am.memory=1024m</strong></p>\n<p>再看日志信息：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-7.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-8.png\" alt=\"\"></p>\n<p>yarnUI状态：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-9.png\" alt=\"\"></p>\n<p>sparkUI的executors信息：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-10.png\" alt=\"\"></p>\n<p>可见，AppMaster的实际内存为1024M（1408-384），而其在yarn中的container内存大小为2G（1408大于1G，yarn按资源规整化原则为其分配2G）。</p>\n<p>同理，driver的storage memory空间为3G*0.9*0.6=1.62G，executor的storage memory空间为4G*0.9*0.6=2.16G，executor所在container占用5G内存（4096+max(4096*0.10,384)= 4505.6，大于4G， yarn按资源规整化原则为其分配5G）。</p>\n<p>Yarn集群的内存总占用空间为2+5*4=22G。</p>\n","excerpt":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>在测试spark on yarn时，发现一些内存分配上的问题，具体如下。</p>\n<p>在$SPARK_HOME/conf/spark-env.sh中配置如下参数：</p>\n<blockquote>\n<p>SPARK_EXECUTOR_INSTANCES=4            <em>在yarn集群中启动的executor进程数</em></p>\n<p>SPARK_EXECUTOR_MEMORY=2G              <em>为每个executor进程分配的内存大小</em></p>\n<p>SPARK_DRIVER_MEMORY=1G                <em>为spark-driver进程分配的内存大小</em></p>\n</blockquote>\n<p>执行$SPARK_HOME/bin/spark-sql –master yarn，按yarn-client模式启动spark-sql交互命令行（即driver程序运行在本地，而非yarn的container中），日志显示的关于AppMaster和Executor的内存信息如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-1.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-2.png\" alt=\"\"></p>\n<p>日志显示，AppMaster的内存是896MB，其中包含了384MB的memoryOverhead；启动了5个executor，第一个的可用内存是530.3MB，其余每个Executor的可用内存是1060.3MB。</p>\n<p>到yarnUI看下资源使用情况，共启动了5个container，占用内存13G，其中一台NodeManager启动了2个container，占用内存4G（1个AppMaster占1G、另一个占3G），另外3台各启了1个container，每个占用3G内存。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-3.png\" alt=\"\"></p>\n<p>再到sparkUI看下executors的情况，这里有5个executor，其中driver是运行在执行spark-sql命令的本地服务器上，另外4个是运行在yarn集群中。Driver的可用storage memory为530.3MB，另外4个都是1060.3MB（与日志信息一致）。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-4.png\" alt=\"\"></p>\n<p>那么问题来了：</p>\n<ol>\n<li><p>Yarn为container分配的最小内存由yarn.scheduler.minimum-allocation-mb参数决定，默认是1G，从yarnUI中看确实如此，可为何spark的日志里显示AppMaster的实际内存是896-384=512MB呢？384MB是怎么算出来的？</p>\n</li>\n<li><p>spark配置文件里指定了每个executor的内存为2G，为何日志和sparkUI上显示的是1060.3MB？</p>\n</li>\n<li><p>driver的内存配置为1G，为何sparkUI里显示的是530.3MB呢？</p>\n</li>\n<li><p>为何yarn中每个container分配的内存是3G，而不是executor需要的2G呢？</p>\n</li>\n</ol>\n<h2 id=\"问题解析\"><a href=\"#问题解析\" class=\"headerlink\" title=\"问题解析\"></a>问题解析</h2><p>进过一番调研，发现这里有些概念容易混淆，整理如下，序号对应上面的问题：<br>","more":"<br>(1) spark的yarn-client向ResourceManager申请提交作业/启动AppMaster时，会判断是否是集群模式，如果是集群模式，则AppMaster的内存大小与driver内存大小一致，否则由spark.yarn.am.memory决定，这个参数的默认值是512MB。我们使用的是yarn-client模式，所以实际内存是512MB。</p>\n<p><font color='red'>384MB是spark-client为appMaster额外申请的内存</font>，计算方法如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-5.png\" alt=\"\"></p>\n<p>即，默认从参数读取（集群模式从spark.yarn.driver.memoryOverhead参数读，否则从spark.yarn.am.memoryOverhead参数读），若没配此参数，则从AppMaster的内存*一定系数和默认最小overhead中取较大值。</p>\n<p>在spark-1.4.1版本中，MEMORY_OVERHEAD_FACTOR的默认值为0.10（之前是0.07），MEMORY_OVERHEAD_MIN默认为384，我们没有指定spark.yarn.driver.memoryOverhead和spark.yarn.am.memoryOverhead，而amMemory=512M（由spark.yarn.am.memory决定），因此memoryOverhead为max(512*0.10, 384)=384MB。</p>\n<p>Executor的memoryOverhead计算方法与此一样，只是不区分是否集群模式，都默认由spark.yarn.executor.memoryOverhead配置。</p>\n<p>(2) <font color='red'>日志和sparkUI上显示的是executor内部用于缓存计算结果的内存空间，并不是executor所拥有的全部内存</font>。这部分内存是由以下公式计算：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-6.png\" alt=\"\"></p>\n<p>Runtime.getRuntime.maxMemory按2048MB算，storage memory大小为1105.92MB，sparkUI显示的略小于此值，是正常的。</p>\n<p>(3) 与上述第2点一样，storage memory的大小略小于1024<em>0.9</em>0.6=552.96MB</p>\n<p>(4) 前面提到spark会为container额外申请一部分内存（memoryOverhead），因此，实际为container提交申请的内存大小是2048 + max(2048*0.10, 384) = 2432MB，而<font color='red'>yarn在做资源分配时会做资源规整化，即应用程序申请的资源量一定是最小可申请资源量的整数倍（向上取整）</font>，最小可申请内存量由yarn.scheduler.minimum-allocation-mb指定，因此，会为container分配3G内存。</p>\n<h2 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h2><p>为了验证上述规则，继续修改配置参数：</p>\n<blockquote>\n<p>SPARK_EXECUTOR_INSTANCES=4          <em>在yarn集群中启动的executor进程数</em></p>\n<p>SPARK_EXECUTOR_MEMORY=4G            <em>为每个executor进程分配的内存大小</em></p>\n<p>SPARK_DRIVER_MEMORY=3G              <em>为spark-driver进程分配的内存大小</em></p>\n</blockquote>\n<p>并在启动spark-sql时指定spark.yarn.am.memory参数：</p>\n<p><strong>bin/spark-sql –master yarn –conf spark.yarn.am.memory=1024m</strong></p>\n<p>再看日志信息：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-7.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-8.png\" alt=\"\"></p>\n<p>yarnUI状态：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-9.png\" alt=\"\"></p>\n<p>sparkUI的executors信息：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150813/sparkonyarn-10.png\" alt=\"\"></p>\n<p>可见，AppMaster的实际内存为1024M（1408-384），而其在yarn中的container内存大小为2G（1408大于1G，yarn按资源规整化原则为其分配2G）。</p>\n<p>同理，driver的storage memory空间为3G*0.9*0.6=1.62G，executor的storage memory空间为4G*0.9*0.6=2.16G，executor所在container占用5G内存（4096+max(4096*0.10,384)= 4505.6，大于4G， yarn按资源规整化原则为其分配5G）。</p>\n<p>Yarn集群的内存总占用空间为2+5*4=22G。</p>"},{"title":"hadoop-yarn中ResourceManager的服务模块","date":"2015-06-06T05:10:10.000Z","_content":"\nYarn简述\n--\n\nHadoop2.0引入了yarn（Yet Another Resource Negotiator）资源管理框架。1.0中的MapReduce计算框架变为运行在yarn上的一种application。\n\nYarn依然采用了master/slave结构，master是ResourceManager，负责整个集群的资源管理和调度，并且支持HA，slave是NodeManager，负责管理各子节点上的资源和任务。每个MapReduce作业提交给ResourceManager并被接受后，ResourceManager会通知某个NodeManager启动一个ApplicationMaster管理此作业的生命周期。\n\nResourceManager中的模块划分\n--\n\nYarn中的大多数服务都是带状态的service实现，并通过事件驱动机制实现服务的状态转换和服务之间的交互。ResourceManager是yarn的核心组件，与NodeManager、ApplicationMaster、Client都有交互，提供了非常多的功能，下面基于hadoop2.7版本的实现，梳理一下ResourceManager中的重要service组件及其功能。\n\nResourceManager中按功能划分的service模块如下图所示。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150606ResourceManager%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/1.png)\n\nResourceManager中核心模块主要包括客户端交互模块、NodeManager管理模块、ApplicationMaster管理模块、Application管理模块、安全管理模块、以及资源管理模块（调度、预留）等。\n\n各模块中的服务介绍\n--\n\n**客户端交互模块：**\n\n* AdminService\n\t* 管理员可通过此接口管理集群，如更新节点、更新ACL、更新队列等。内部有个EmbeddedElectorService，如果RM启用了自动HA，则通过这个service做leader election。\n\t\n* ClientRMService\n\t* 负责为客户端提供服务，是ApplicationClientProtocol协议的服务端。负责处理来自客户端的RPC请求，包括提交app、查询app运行状态、终止app等。\n\n* Webapp\n\t* 提供web页面服务，展示集群状态和资源使用情况。\n\n**NodeManager管理模块**\n\n* NMLivelinessMonitor\n\t* 用于监控NM是否存活，若NM在一定时间内（默认10分钟）未上报心跳，则认为其挂了。\n \n* NodesListManager\n\t* 负责维护节点列表，并动态加载白名单（yarn.resourcemanager.nodes.include-path）和黑名单（yarn.resourcemanager.nodes.exlude-path）节点。\n \n* RMNodeLabelsManager\n\t* 负责节点的标签管理。\n\n* ResourceTrackerService\n\t* 负责与NodeManager通信，处理来自NodeManager的请求，包括注册NodeManager和节点心跳两种。接口定义在ResourceTracker中。\n\n**ApplicationMaster管理模块**\n\n* AMLivelinessMonitor：两个实例\n\t* 用于监控ApplicationMaster是否正常，如果在指定时间内（默认10分钟）未收到AM的心跳，则认为其死掉了。\n \n* ApplicationMasterLauncher\n\t* 负责通知某个NodeManager启动或销毁ApplicationMaster。在app请求被接受后，与某个NodeManager通信，告知其为此app启动相应的ApplicationMaster。若app运行结束或被kill，则通知app所在NodeManager销毁ApplicationMaster。其内部也维护了一个阻塞队列，并有一个后台线程异步处理提交进来的启动ApplicationMaster的请求。\n \n* ApplicationMasterService\n\t* 负责与ApplicationMaster通信，是ApplicationMasterProtocol协议的服务端，ApplicationMaster在NodeManager上启动后通过此协议向ResourceManager注册自己，运行过程中向ResourceManager发送心跳，以及app运行结束后告知RM自己所在的container可以被释放了。\n\n**Application管理模块**\n\n* RMAppManager\n\t* ResourceManager接受客户端提交的app后，会通过RMAppManager来触发启动app的事件RMAppEventType.START，具体启动app的工作由RMAppImpl实现。\n\n* ApplicationACLsManager\n\t* 负责app权限控制，包括查看和修改权限。\n\n* ContainerAllocationExpirer\n\t* 用于监听NodeManager上是否正常启动了分配给ApplicationMaster的container，若在指定时间未启动（默认10分钟），ResourceManager会强制回收该container。\n\n* RMApplicationHistoryWriter\n\t* 负责异步持久化Application运行中的相关日志，主要是Container、Application、ApplicationAttempt在启动和结束时的日志信息。\n\n**安全管理模块**\n\n* RMSecretManagerService\n\t* 负责管理各种通信密钥，包括：\n\t\t* RM与NM通信的NMTokenSecretManagerInRM\n\t\t* RM与container通信的RMContainerTokenSecretManager\n\t\t* 客户端与AM通信的ClientToAMTokenSecretManagerInRM\n\t\t* AM与RM通信的AMRMTokenSecretManager\n\t\t* DelegationTokenRenewer\n\t\t* 启用了安全时，负责定时更新认证token。\n\n**资源管理模块**\n\n* ResourceScheduler\n\t* 资源调度器，可通过yarn.resourcemanager.scheduler.class指定，ResourceManager默认使用的是org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler。\n\n* SchedulerEventDispatcher\n\t* 用于处理SchedulerEventType类型的事件，其内部维护了一个存储SchedulerEvent的阻塞队列，并由一个后台线程从队列中取出资源请求事件，再调用ResourceScheduler进行处理。\n\n* ReservationSystem\n\t* 资源预留系统，对应的实现有CapacityReservationSystem和FairReservationSystem。\n\n此外，SystemMetricsPublisher负责发布RM的系统统计信息。AsyncDispatcher是中央事件处理分发器，ResourceManager启动时，通过它绑定了几种类型的事件的处理器，包括SchedulerEventType、RMAppEventType、ApplicationAttempt、RMAppAttemptEventType、RMNodeEventType、RMAppManagerEventType、AMLaunchEventType等。\n\n上述各service在ResourceManager中的启动顺序为：\n\n1. AsyncDispatcher\n\n2. AdminService\n\n3. RMActiveServices：是个CompositeService（即service列表，ResourceManager本身就是一个CompositeService），用于管理ResourceManager中的“活动”服务（必须在active的ResourceManager上启动的服务，启用HA时，备份ResourceManager上不启动这些服务），包括以下（按启动顺序）：\n\n\t* RMSecretManagerService\n\n\t* ContainerAllocationExpirer\n\t \n\t* AMLivelinessMonitor\n\t \n\t* RMNodeLabelsManager\n\t \n\t* RMApplicationHistoryWriter\n\t \n\t* SystemMetricsPublisher\n\t \n\t* NodesListManager\n\t \n\t* ResourceScheduler\n\t \n\t* SchedulerEventDispatcher\n\t \n\t* NMLivelinessMonitor\n\t \n\t* ResourceTrackerService\n\t \n\t* ApplicationMasterService\n\t \n\t* ClientRMService\n\t \n\t* ApplicationMasterLauncher\n\t \n\t* DelegationTokenRenewer","source":"_posts/hadoop-yarn中ResourceManager的服务模块.md","raw":"---\ntitle: hadoop-yarn中ResourceManager的服务模块\ndate: 2015-06-06 13:10:10\ntags:\n- hadoop\n- yarn\ncategories: \n- Hadoop\n---\n\nYarn简述\n--\n\nHadoop2.0引入了yarn（Yet Another Resource Negotiator）资源管理框架。1.0中的MapReduce计算框架变为运行在yarn上的一种application。\n\nYarn依然采用了master/slave结构，master是ResourceManager，负责整个集群的资源管理和调度，并且支持HA，slave是NodeManager，负责管理各子节点上的资源和任务。每个MapReduce作业提交给ResourceManager并被接受后，ResourceManager会通知某个NodeManager启动一个ApplicationMaster管理此作业的生命周期。\n\nResourceManager中的模块划分\n--\n\nYarn中的大多数服务都是带状态的service实现，并通过事件驱动机制实现服务的状态转换和服务之间的交互。ResourceManager是yarn的核心组件，与NodeManager、ApplicationMaster、Client都有交互，提供了非常多的功能，下面基于hadoop2.7版本的实现，梳理一下ResourceManager中的重要service组件及其功能。\n\nResourceManager中按功能划分的service模块如下图所示。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150606ResourceManager%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/1.png)\n\nResourceManager中核心模块主要包括客户端交互模块、NodeManager管理模块、ApplicationMaster管理模块、Application管理模块、安全管理模块、以及资源管理模块（调度、预留）等。\n\n各模块中的服务介绍\n--\n\n**客户端交互模块：**\n\n* AdminService\n\t* 管理员可通过此接口管理集群，如更新节点、更新ACL、更新队列等。内部有个EmbeddedElectorService，如果RM启用了自动HA，则通过这个service做leader election。\n\t\n* ClientRMService\n\t* 负责为客户端提供服务，是ApplicationClientProtocol协议的服务端。负责处理来自客户端的RPC请求，包括提交app、查询app运行状态、终止app等。\n\n* Webapp\n\t* 提供web页面服务，展示集群状态和资源使用情况。\n\n**NodeManager管理模块**\n\n* NMLivelinessMonitor\n\t* 用于监控NM是否存活，若NM在一定时间内（默认10分钟）未上报心跳，则认为其挂了。\n \n* NodesListManager\n\t* 负责维护节点列表，并动态加载白名单（yarn.resourcemanager.nodes.include-path）和黑名单（yarn.resourcemanager.nodes.exlude-path）节点。\n \n* RMNodeLabelsManager\n\t* 负责节点的标签管理。\n\n* ResourceTrackerService\n\t* 负责与NodeManager通信，处理来自NodeManager的请求，包括注册NodeManager和节点心跳两种。接口定义在ResourceTracker中。\n\n**ApplicationMaster管理模块**\n\n* AMLivelinessMonitor：两个实例\n\t* 用于监控ApplicationMaster是否正常，如果在指定时间内（默认10分钟）未收到AM的心跳，则认为其死掉了。\n \n* ApplicationMasterLauncher\n\t* 负责通知某个NodeManager启动或销毁ApplicationMaster。在app请求被接受后，与某个NodeManager通信，告知其为此app启动相应的ApplicationMaster。若app运行结束或被kill，则通知app所在NodeManager销毁ApplicationMaster。其内部也维护了一个阻塞队列，并有一个后台线程异步处理提交进来的启动ApplicationMaster的请求。\n \n* ApplicationMasterService\n\t* 负责与ApplicationMaster通信，是ApplicationMasterProtocol协议的服务端，ApplicationMaster在NodeManager上启动后通过此协议向ResourceManager注册自己，运行过程中向ResourceManager发送心跳，以及app运行结束后告知RM自己所在的container可以被释放了。\n\n**Application管理模块**\n\n* RMAppManager\n\t* ResourceManager接受客户端提交的app后，会通过RMAppManager来触发启动app的事件RMAppEventType.START，具体启动app的工作由RMAppImpl实现。\n\n* ApplicationACLsManager\n\t* 负责app权限控制，包括查看和修改权限。\n\n* ContainerAllocationExpirer\n\t* 用于监听NodeManager上是否正常启动了分配给ApplicationMaster的container，若在指定时间未启动（默认10分钟），ResourceManager会强制回收该container。\n\n* RMApplicationHistoryWriter\n\t* 负责异步持久化Application运行中的相关日志，主要是Container、Application、ApplicationAttempt在启动和结束时的日志信息。\n\n**安全管理模块**\n\n* RMSecretManagerService\n\t* 负责管理各种通信密钥，包括：\n\t\t* RM与NM通信的NMTokenSecretManagerInRM\n\t\t* RM与container通信的RMContainerTokenSecretManager\n\t\t* 客户端与AM通信的ClientToAMTokenSecretManagerInRM\n\t\t* AM与RM通信的AMRMTokenSecretManager\n\t\t* DelegationTokenRenewer\n\t\t* 启用了安全时，负责定时更新认证token。\n\n**资源管理模块**\n\n* ResourceScheduler\n\t* 资源调度器，可通过yarn.resourcemanager.scheduler.class指定，ResourceManager默认使用的是org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler。\n\n* SchedulerEventDispatcher\n\t* 用于处理SchedulerEventType类型的事件，其内部维护了一个存储SchedulerEvent的阻塞队列，并由一个后台线程从队列中取出资源请求事件，再调用ResourceScheduler进行处理。\n\n* ReservationSystem\n\t* 资源预留系统，对应的实现有CapacityReservationSystem和FairReservationSystem。\n\n此外，SystemMetricsPublisher负责发布RM的系统统计信息。AsyncDispatcher是中央事件处理分发器，ResourceManager启动时，通过它绑定了几种类型的事件的处理器，包括SchedulerEventType、RMAppEventType、ApplicationAttempt、RMAppAttemptEventType、RMNodeEventType、RMAppManagerEventType、AMLaunchEventType等。\n\n上述各service在ResourceManager中的启动顺序为：\n\n1. AsyncDispatcher\n\n2. AdminService\n\n3. RMActiveServices：是个CompositeService（即service列表，ResourceManager本身就是一个CompositeService），用于管理ResourceManager中的“活动”服务（必须在active的ResourceManager上启动的服务，启用HA时，备份ResourceManager上不启动这些服务），包括以下（按启动顺序）：\n\n\t* RMSecretManagerService\n\n\t* ContainerAllocationExpirer\n\t \n\t* AMLivelinessMonitor\n\t \n\t* RMNodeLabelsManager\n\t \n\t* RMApplicationHistoryWriter\n\t \n\t* SystemMetricsPublisher\n\t \n\t* NodesListManager\n\t \n\t* ResourceScheduler\n\t \n\t* SchedulerEventDispatcher\n\t \n\t* NMLivelinessMonitor\n\t \n\t* ResourceTrackerService\n\t \n\t* ApplicationMasterService\n\t \n\t* ClientRMService\n\t \n\t* ApplicationMasterLauncher\n\t \n\t* DelegationTokenRenewer","slug":"hadoop-yarn中ResourceManager的服务模块","published":1,"updated":"2017-01-18T09:54:11.734Z","_id":"ciy320aii0003ifs6w7m9s1oj","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Yarn简述\"><a href=\"#Yarn简述\" class=\"headerlink\" title=\"Yarn简述\"></a>Yarn简述</h2><p>Hadoop2.0引入了yarn（Yet Another Resource Negotiator）资源管理框架。1.0中的MapReduce计算框架变为运行在yarn上的一种application。</p>\n<p>Yarn依然采用了master/slave结构，master是ResourceManager，负责整个集群的资源管理和调度，并且支持HA，slave是NodeManager，负责管理各子节点上的资源和任务。每个MapReduce作业提交给ResourceManager并被接受后，ResourceManager会通知某个NodeManager启动一个ApplicationMaster管理此作业的生命周期。</p>\n<h2 id=\"ResourceManager中的模块划分\"><a href=\"#ResourceManager中的模块划分\" class=\"headerlink\" title=\"ResourceManager中的模块划分\"></a>ResourceManager中的模块划分</h2><p>Yarn中的大多数服务都是带状态的service实现，并通过事件驱动机制实现服务的状态转换和服务之间的交互。ResourceManager是yarn的核心组件，与NodeManager、ApplicationMaster、Client都有交互，提供了非常多的功能，下面基于hadoop2.7版本的实现，梳理一下ResourceManager中的重要service组件及其功能。</p>\n<p>ResourceManager中按功能划分的service模块如下图所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150606ResourceManager%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/1.png\" alt=\"\"></p>\n<p>ResourceManager中核心模块主要包括客户端交互模块、NodeManager管理模块、ApplicationMaster管理模块、Application管理模块、安全管理模块、以及资源管理模块（调度、预留）等。</p>\n<h2 id=\"各模块中的服务介绍\"><a href=\"#各模块中的服务介绍\" class=\"headerlink\" title=\"各模块中的服务介绍\"></a>各模块中的服务介绍</h2><p><strong>客户端交互模块：</strong></p>\n<ul>\n<li><p>AdminService</p>\n<ul>\n<li>管理员可通过此接口管理集群，如更新节点、更新ACL、更新队列等。内部有个EmbeddedElectorService，如果RM启用了自动HA，则通过这个service做leader election。</li>\n</ul>\n</li>\n<li><p>ClientRMService</p>\n<ul>\n<li>负责为客户端提供服务，是ApplicationClientProtocol协议的服务端。负责处理来自客户端的RPC请求，包括提交app、查询app运行状态、终止app等。</li>\n</ul>\n</li>\n<li><p>Webapp</p>\n<ul>\n<li>提供web页面服务，展示集群状态和资源使用情况。</li>\n</ul>\n</li>\n</ul>\n<p><strong>NodeManager管理模块</strong></p>\n<ul>\n<li><p>NMLivelinessMonitor</p>\n<ul>\n<li>用于监控NM是否存活，若NM在一定时间内（默认10分钟）未上报心跳，则认为其挂了。</li>\n</ul>\n</li>\n<li><p>NodesListManager</p>\n<ul>\n<li>负责维护节点列表，并动态加载白名单（yarn.resourcemanager.nodes.include-path）和黑名单（yarn.resourcemanager.nodes.exlude-path）节点。</li>\n</ul>\n</li>\n<li><p>RMNodeLabelsManager</p>\n<ul>\n<li>负责节点的标签管理。</li>\n</ul>\n</li>\n<li><p>ResourceTrackerService</p>\n<ul>\n<li>负责与NodeManager通信，处理来自NodeManager的请求，包括注册NodeManager和节点心跳两种。接口定义在ResourceTracker中。</li>\n</ul>\n</li>\n</ul>\n<p><strong>ApplicationMaster管理模块</strong></p>\n<ul>\n<li><p>AMLivelinessMonitor：两个实例</p>\n<ul>\n<li>用于监控ApplicationMaster是否正常，如果在指定时间内（默认10分钟）未收到AM的心跳，则认为其死掉了。</li>\n</ul>\n</li>\n<li><p>ApplicationMasterLauncher</p>\n<ul>\n<li>负责通知某个NodeManager启动或销毁ApplicationMaster。在app请求被接受后，与某个NodeManager通信，告知其为此app启动相应的ApplicationMaster。若app运行结束或被kill，则通知app所在NodeManager销毁ApplicationMaster。其内部也维护了一个阻塞队列，并有一个后台线程异步处理提交进来的启动ApplicationMaster的请求。</li>\n</ul>\n</li>\n<li><p>ApplicationMasterService</p>\n<ul>\n<li>负责与ApplicationMaster通信，是ApplicationMasterProtocol协议的服务端，ApplicationMaster在NodeManager上启动后通过此协议向ResourceManager注册自己，运行过程中向ResourceManager发送心跳，以及app运行结束后告知RM自己所在的container可以被释放了。</li>\n</ul>\n</li>\n</ul>\n<p><strong>Application管理模块</strong></p>\n<ul>\n<li><p>RMAppManager</p>\n<ul>\n<li>ResourceManager接受客户端提交的app后，会通过RMAppManager来触发启动app的事件RMAppEventType.START，具体启动app的工作由RMAppImpl实现。</li>\n</ul>\n</li>\n<li><p>ApplicationACLsManager</p>\n<ul>\n<li>负责app权限控制，包括查看和修改权限。</li>\n</ul>\n</li>\n<li><p>ContainerAllocationExpirer</p>\n<ul>\n<li>用于监听NodeManager上是否正常启动了分配给ApplicationMaster的container，若在指定时间未启动（默认10分钟），ResourceManager会强制回收该container。</li>\n</ul>\n</li>\n<li><p>RMApplicationHistoryWriter</p>\n<ul>\n<li>负责异步持久化Application运行中的相关日志，主要是Container、Application、ApplicationAttempt在启动和结束时的日志信息。</li>\n</ul>\n</li>\n</ul>\n<p><strong>安全管理模块</strong></p>\n<ul>\n<li>RMSecretManagerService<ul>\n<li>负责管理各种通信密钥，包括：<ul>\n<li>RM与NM通信的NMTokenSecretManagerInRM</li>\n<li>RM与container通信的RMContainerTokenSecretManager</li>\n<li>客户端与AM通信的ClientToAMTokenSecretManagerInRM</li>\n<li>AM与RM通信的AMRMTokenSecretManager</li>\n<li>DelegationTokenRenewer</li>\n<li>启用了安全时，负责定时更新认证token。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>资源管理模块</strong></p>\n<ul>\n<li><p>ResourceScheduler</p>\n<ul>\n<li>资源调度器，可通过yarn.resourcemanager.scheduler.class指定，ResourceManager默认使用的是org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler。</li>\n</ul>\n</li>\n<li><p>SchedulerEventDispatcher</p>\n<ul>\n<li>用于处理SchedulerEventType类型的事件，其内部维护了一个存储SchedulerEvent的阻塞队列，并由一个后台线程从队列中取出资源请求事件，再调用ResourceScheduler进行处理。</li>\n</ul>\n</li>\n<li><p>ReservationSystem</p>\n<ul>\n<li>资源预留系统，对应的实现有CapacityReservationSystem和FairReservationSystem。</li>\n</ul>\n</li>\n</ul>\n<p>此外，SystemMetricsPublisher负责发布RM的系统统计信息。AsyncDispatcher是中央事件处理分发器，ResourceManager启动时，通过它绑定了几种类型的事件的处理器，包括SchedulerEventType、RMAppEventType、ApplicationAttempt、RMAppAttemptEventType、RMNodeEventType、RMAppManagerEventType、AMLaunchEventType等。</p>\n<p>上述各service在ResourceManager中的启动顺序为：</p>\n<ol>\n<li><p>AsyncDispatcher</p>\n</li>\n<li><p>AdminService</p>\n</li>\n<li><p>RMActiveServices：是个CompositeService（即service列表，ResourceManager本身就是一个CompositeService），用于管理ResourceManager中的“活动”服务（必须在active的ResourceManager上启动的服务，启用HA时，备份ResourceManager上不启动这些服务），包括以下（按启动顺序）：</p>\n<ul>\n<li><p>RMSecretManagerService</p>\n</li>\n<li><p>ContainerAllocationExpirer</p>\n</li>\n<li><p>AMLivelinessMonitor</p>\n</li>\n<li><p>RMNodeLabelsManager</p>\n</li>\n<li><p>RMApplicationHistoryWriter</p>\n</li>\n<li><p>SystemMetricsPublisher</p>\n</li>\n<li><p>NodesListManager</p>\n</li>\n<li><p>ResourceScheduler</p>\n</li>\n<li><p>SchedulerEventDispatcher</p>\n</li>\n<li><p>NMLivelinessMonitor</p>\n</li>\n<li><p>ResourceTrackerService</p>\n</li>\n<li><p>ApplicationMasterService</p>\n</li>\n<li><p>ClientRMService</p>\n</li>\n<li><p>ApplicationMasterLauncher</p>\n</li>\n<li><p>DelegationTokenRenewer</p>\n</li>\n</ul>\n</li>\n</ol>\n","excerpt":"","more":"<h2 id=\"Yarn简述\"><a href=\"#Yarn简述\" class=\"headerlink\" title=\"Yarn简述\"></a>Yarn简述</h2><p>Hadoop2.0引入了yarn（Yet Another Resource Negotiator）资源管理框架。1.0中的MapReduce计算框架变为运行在yarn上的一种application。</p>\n<p>Yarn依然采用了master/slave结构，master是ResourceManager，负责整个集群的资源管理和调度，并且支持HA，slave是NodeManager，负责管理各子节点上的资源和任务。每个MapReduce作业提交给ResourceManager并被接受后，ResourceManager会通知某个NodeManager启动一个ApplicationMaster管理此作业的生命周期。</p>\n<h2 id=\"ResourceManager中的模块划分\"><a href=\"#ResourceManager中的模块划分\" class=\"headerlink\" title=\"ResourceManager中的模块划分\"></a>ResourceManager中的模块划分</h2><p>Yarn中的大多数服务都是带状态的service实现，并通过事件驱动机制实现服务的状态转换和服务之间的交互。ResourceManager是yarn的核心组件，与NodeManager、ApplicationMaster、Client都有交互，提供了非常多的功能，下面基于hadoop2.7版本的实现，梳理一下ResourceManager中的重要service组件及其功能。</p>\n<p>ResourceManager中按功能划分的service模块如下图所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150606ResourceManager%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/1.png\" alt=\"\"></p>\n<p>ResourceManager中核心模块主要包括客户端交互模块、NodeManager管理模块、ApplicationMaster管理模块、Application管理模块、安全管理模块、以及资源管理模块（调度、预留）等。</p>\n<h2 id=\"各模块中的服务介绍\"><a href=\"#各模块中的服务介绍\" class=\"headerlink\" title=\"各模块中的服务介绍\"></a>各模块中的服务介绍</h2><p><strong>客户端交互模块：</strong></p>\n<ul>\n<li><p>AdminService</p>\n<ul>\n<li>管理员可通过此接口管理集群，如更新节点、更新ACL、更新队列等。内部有个EmbeddedElectorService，如果RM启用了自动HA，则通过这个service做leader election。</li>\n</ul>\n</li>\n<li><p>ClientRMService</p>\n<ul>\n<li>负责为客户端提供服务，是ApplicationClientProtocol协议的服务端。负责处理来自客户端的RPC请求，包括提交app、查询app运行状态、终止app等。</li>\n</ul>\n</li>\n<li><p>Webapp</p>\n<ul>\n<li>提供web页面服务，展示集群状态和资源使用情况。</li>\n</ul>\n</li>\n</ul>\n<p><strong>NodeManager管理模块</strong></p>\n<ul>\n<li><p>NMLivelinessMonitor</p>\n<ul>\n<li>用于监控NM是否存活，若NM在一定时间内（默认10分钟）未上报心跳，则认为其挂了。</li>\n</ul>\n</li>\n<li><p>NodesListManager</p>\n<ul>\n<li>负责维护节点列表，并动态加载白名单（yarn.resourcemanager.nodes.include-path）和黑名单（yarn.resourcemanager.nodes.exlude-path）节点。</li>\n</ul>\n</li>\n<li><p>RMNodeLabelsManager</p>\n<ul>\n<li>负责节点的标签管理。</li>\n</ul>\n</li>\n<li><p>ResourceTrackerService</p>\n<ul>\n<li>负责与NodeManager通信，处理来自NodeManager的请求，包括注册NodeManager和节点心跳两种。接口定义在ResourceTracker中。</li>\n</ul>\n</li>\n</ul>\n<p><strong>ApplicationMaster管理模块</strong></p>\n<ul>\n<li><p>AMLivelinessMonitor：两个实例</p>\n<ul>\n<li>用于监控ApplicationMaster是否正常，如果在指定时间内（默认10分钟）未收到AM的心跳，则认为其死掉了。</li>\n</ul>\n</li>\n<li><p>ApplicationMasterLauncher</p>\n<ul>\n<li>负责通知某个NodeManager启动或销毁ApplicationMaster。在app请求被接受后，与某个NodeManager通信，告知其为此app启动相应的ApplicationMaster。若app运行结束或被kill，则通知app所在NodeManager销毁ApplicationMaster。其内部也维护了一个阻塞队列，并有一个后台线程异步处理提交进来的启动ApplicationMaster的请求。</li>\n</ul>\n</li>\n<li><p>ApplicationMasterService</p>\n<ul>\n<li>负责与ApplicationMaster通信，是ApplicationMasterProtocol协议的服务端，ApplicationMaster在NodeManager上启动后通过此协议向ResourceManager注册自己，运行过程中向ResourceManager发送心跳，以及app运行结束后告知RM自己所在的container可以被释放了。</li>\n</ul>\n</li>\n</ul>\n<p><strong>Application管理模块</strong></p>\n<ul>\n<li><p>RMAppManager</p>\n<ul>\n<li>ResourceManager接受客户端提交的app后，会通过RMAppManager来触发启动app的事件RMAppEventType.START，具体启动app的工作由RMAppImpl实现。</li>\n</ul>\n</li>\n<li><p>ApplicationACLsManager</p>\n<ul>\n<li>负责app权限控制，包括查看和修改权限。</li>\n</ul>\n</li>\n<li><p>ContainerAllocationExpirer</p>\n<ul>\n<li>用于监听NodeManager上是否正常启动了分配给ApplicationMaster的container，若在指定时间未启动（默认10分钟），ResourceManager会强制回收该container。</li>\n</ul>\n</li>\n<li><p>RMApplicationHistoryWriter</p>\n<ul>\n<li>负责异步持久化Application运行中的相关日志，主要是Container、Application、ApplicationAttempt在启动和结束时的日志信息。</li>\n</ul>\n</li>\n</ul>\n<p><strong>安全管理模块</strong></p>\n<ul>\n<li>RMSecretManagerService<ul>\n<li>负责管理各种通信密钥，包括：<ul>\n<li>RM与NM通信的NMTokenSecretManagerInRM</li>\n<li>RM与container通信的RMContainerTokenSecretManager</li>\n<li>客户端与AM通信的ClientToAMTokenSecretManagerInRM</li>\n<li>AM与RM通信的AMRMTokenSecretManager</li>\n<li>DelegationTokenRenewer</li>\n<li>启用了安全时，负责定时更新认证token。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>资源管理模块</strong></p>\n<ul>\n<li><p>ResourceScheduler</p>\n<ul>\n<li>资源调度器，可通过yarn.resourcemanager.scheduler.class指定，ResourceManager默认使用的是org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler。</li>\n</ul>\n</li>\n<li><p>SchedulerEventDispatcher</p>\n<ul>\n<li>用于处理SchedulerEventType类型的事件，其内部维护了一个存储SchedulerEvent的阻塞队列，并由一个后台线程从队列中取出资源请求事件，再调用ResourceScheduler进行处理。</li>\n</ul>\n</li>\n<li><p>ReservationSystem</p>\n<ul>\n<li>资源预留系统，对应的实现有CapacityReservationSystem和FairReservationSystem。</li>\n</ul>\n</li>\n</ul>\n<p>此外，SystemMetricsPublisher负责发布RM的系统统计信息。AsyncDispatcher是中央事件处理分发器，ResourceManager启动时，通过它绑定了几种类型的事件的处理器，包括SchedulerEventType、RMAppEventType、ApplicationAttempt、RMAppAttemptEventType、RMNodeEventType、RMAppManagerEventType、AMLaunchEventType等。</p>\n<p>上述各service在ResourceManager中的启动顺序为：</p>\n<ol>\n<li><p>AsyncDispatcher</p>\n</li>\n<li><p>AdminService</p>\n</li>\n<li><p>RMActiveServices：是个CompositeService（即service列表，ResourceManager本身就是一个CompositeService），用于管理ResourceManager中的“活动”服务（必须在active的ResourceManager上启动的服务，启用HA时，备份ResourceManager上不启动这些服务），包括以下（按启动顺序）：</p>\n<ul>\n<li><p>RMSecretManagerService</p>\n</li>\n<li><p>ContainerAllocationExpirer</p>\n</li>\n<li><p>AMLivelinessMonitor</p>\n</li>\n<li><p>RMNodeLabelsManager</p>\n</li>\n<li><p>RMApplicationHistoryWriter</p>\n</li>\n<li><p>SystemMetricsPublisher</p>\n</li>\n<li><p>NodesListManager</p>\n</li>\n<li><p>ResourceScheduler</p>\n</li>\n<li><p>SchedulerEventDispatcher</p>\n</li>\n<li><p>NMLivelinessMonitor</p>\n</li>\n<li><p>ResourceTrackerService</p>\n</li>\n<li><p>ApplicationMasterService</p>\n</li>\n<li><p>ClientRMService</p>\n</li>\n<li><p>ApplicationMasterLauncher</p>\n</li>\n<li><p>DelegationTokenRenewer</p>\n</li>\n</ul>\n</li>\n</ol>\n"},{"title":"Storm的消息可靠处理机制","date":"2015-11-05T12:51:50.000Z","_content":"\n简介\n--\n提交进入Storm运行的topology实际上是一个有向无环图（DAG），其中的节点是由spout和bolt组成，边则可以理解为消息从一个节点到传输到另一个节点的过程。对于spout产生的tuple，只有在topology上处理完毕后，才认为这个tuple被storm可靠处理。\n\nStorm提供了可靠处理消息（storm中的通用名叫tuple）的框架，我们在写一个topology程序时，若需要保证spout产生的消息的可靠处理，需要做到两点：\n\n第一是spout/bolt每生成一个新的tuple都告诉storm一下（其中spout发出的tuple有个id叫rootId），从而让storm能够追踪rootId和每个衍生tuple的处理状态。\n\n第二是每个tuple被下游bolt处理完毕后，无论处理成功或失败，也再告诉storm一下，从而让storm知道是否需要spout重新发送rootId。\n\n做了这两件事，storm就能知道这个tuple是否被处理完毕。如果是处理成功了的，就说明最初从spout发出的tuple（rootId）已在topology中处理完毕，无需spout重新发送。如果是处理失败的，storm则会告知spout重新发送rootId这个tuple。\n\n在程序中实现消息可靠处理\n--\n那在写一个topology时，我们该如何做上面提到的两件事呢？\n<!--more-->\nStorm提供了BaseRichBolt抽象类（实现了IRichBolt接口），一个示例bolt如下：\n```java\npublic class SplitSentence extends BaseRichBolt {\n    OutputCollector _collector;\n \n    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {\n        _collector = collector;\n    }\n \n    public void execute(Tuple t) {\n        String sentence = t.getString(0);\n        for(String word: sentence.split(\" \")) {\n            //1. 告诉storm生成了一个新的tuple，并且这个tuple的锚点是tuple\n            _collector.emit(t, new Values(word));\n        }\n        _collector.ack(t); //2. 告诉storm，t这个tuple已处理完毕\n    }\n \n    public void declareOutputFields(OutputFieldsDeclarer declarer) {\n        declarer.declare(new Fields(\"word\"));\n    }\n}\n```\n\n这段代码就做了这两件事，一是输出新的tuple并告知storm，二是对当前tuple t处理完毕后，告知storm。\n\n对于第一件事，这里要注意的是，在BaseRichBolt中输出一个新的tuple（示例中是word）时，必须指定其锚点（即当前bolt正在处理的tuple），因为输出新的tuple会继续被下游bolt处理，这个锚点tuple和下游tuple之间的路径就是DAG的一条边。如果不指定锚点，则可以理解为storm不知道这条边的存在，也就不会对新输出的tuple进行跟踪了。\n\n如果我们确实不需要保证消息的可靠处理，则使用以下方式输出新tuple即可。\n\n```java\n_collector.emit(new Values(word));\n```\n\n另外，一个tuple的锚点tuple可以有多个，比如如下代码，新输出的tuple的锚点就是tuple1和tuple2。\n\n```java\nList anchors = new ArrayList();\nanchors.add(tuple1);\nanchors.add(tuple2);\n_collector.emit(anchors, new Values(1, 2, 3));\n```\n\n对于第二件事，通过调用OutputCollector的ack或fail方法，即可告知storm当前tuple的处理结果。比如，假设我们在bolt中做一些操作的时候出现异常（比如访问redis、DB、hdfs等），可以调fail方法快速重放rootId，避免等到storm判断这个tuple处理超时后才重放。\n\n更简便的方式\n--\n很明显，以上方式有几个弊端：\n\n1. 输出新tuple和对tuple的ack/fail操作需要我们自己维护，代价很高，容易遗忘。\n\n2. storm是在内存中维护每个tuple的处理状态，如果只对tuple进行锚点标记但处理完毕后忘记ack/fail，在tuple量非常大时，有内存溢出的风险。\n\n鉴于此，storm提供了BaseBasicBolt抽象类（实现了IBasicBolt接口）来帮助我们实现对每个tuple的锚点标记和ack/fail。\n前面的例子可改写如下：\n\n```java\npublic class SplitSentence extends BaseBasicBolt {\n    public void execute(Tuple tuple, BasicOutputCollector collector) {\n        String sentence = tuple.getString(0);\n        for(String word: sentence.split(\" \")) {\n            collector.emit(new Values(word));\n        }\n    }\n    public void declareOutputFields(OutputFieldsDeclarer declarer) {\n        declarer.declare(new Fields(\"word\"));\n    }\n}\n```\n\n可见，在代码中，我们只需要关心bolt的处理逻辑即可，至于标记锚点和ack/fail，均不用关心。\n\n细究一下storm框架对IBasicBolt的处理可知，在创建topology时，IBasicBolt是被封装在BasicBoltExecutor类（实现了IRichBolt接口）中处理的。\n\n构建topology时的setBolt方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20151105/bolt-executor.png)\n\n原理&示例\n--\n刚刚提到对每个topology，storm都在内存中维护其tuple的处理状态，那么对于一个大规模集群，storm是如何高效的维护大量tuple的处理状态的呢？\n\n其实，topology在运行时，内部有一组特殊的任务叫acker，专门用来做tuple的ack/fail。当一个root tuple（spout输出的tuple）在DAG中处理完毕后，acker会向产生该tuple的spout发送消息来ack这个tuple。\n\n我们可通过参数Config.TOPOLOGY_ACKER_EXECUTORS指定topology中的acker任务的数量，默认是与topology中的worker数相同，在处理大量消息的场景下，可以通过此参数增加topology的acker任务数，以提高对message做ack/fail的效率。\n\nstorm通过给每个tuple设置一个全局唯一id，并在输出tuple和tuple处理完毕时收集tuple的id，并进行异或运算，巧妙的实现tuple状态的维护。先看下图示例：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20151105/storm-ack.png)\n\n在这个topology其中包含一个spout，3个bolt和一个acker bolt，紫色线表示tuple的流向，绿色线表示每个bolt处理完tuple后的ack/fail调用，红色线表示acker回调spout的ack/fail方法来标记root tuple处理完毕。\n\n以下是storm的ack框架对tuple的处理状态维护过程说明：\n\n第(1)(2)步，spout发送T1到bolt1，发送T2到bolt2，T1和T2具有相同的内容（可以认为都把spout的输出作为自己的输入）。每条消息都会有一个全局唯一id，T1的锚点为<rootId,T1>，T2的锚点为<rootId,T2>。\n\n第(3)步，spout发送完毕T1、T2后，在acker中注册一条记录rootId=T1^T2。\n\n第(4)(5)步，bolt1收到T1处理完毕后对T1进行ack并发送T3,T4到bolt3，所以在acker中注册T1,T3,T4，acker中的跟踪项变为rootId=T1^T2^T1^T3^T4=T2^T3^T4\n\n第(6)(7)步，bolt2收到T2处理完毕后对T2进行ack并发送T5,T6,T7到bolt4，所以在acker中注册T2,T5,T6,T7，acker中的跟踪项变为rootId=T2^T3^T4^T2^T5^T6^T7=T3^T4^T5^T6^T7\n\n第(8)步，bolt3收到T3,T4处理完毕后对T3,T4进行ack，没有输出新的tuple，所以在acker中注册T3,T4，acker中的跟踪项变为rootId=T3^T4^T5^T6^T7^T3^T4=T5^T6^T7\n\n第(9)步，bolt4收到T5,T6,T7处理完毕后对T5,T6,T7进行ack，没有输出新的tuple，所以在acker中注册T5,T6,T7，acker中的跟踪项变为rootId=T5^T6^T7^T5^T6^T7=0\n\n第(10)步，acker bolt发现rootId对应的追踪值为0，说明该rootId对应的源消息以及衍生出来的所有消息（bolt1,bolt2新输出的消息）都被成功处理完毕。于是acker bolt会回调spout的ack方法，标识消息rootId已被topology处理成功。\n","source":"_posts/Storm的消息可靠处理机制.md","raw":"---\ntitle: Storm的消息可靠处理机制\ndate: 2015-11-05 20:51:50\ntags:\n- Storm\n- 实时计算\ncategories:\n- Storm\n---\n\n简介\n--\n提交进入Storm运行的topology实际上是一个有向无环图（DAG），其中的节点是由spout和bolt组成，边则可以理解为消息从一个节点到传输到另一个节点的过程。对于spout产生的tuple，只有在topology上处理完毕后，才认为这个tuple被storm可靠处理。\n\nStorm提供了可靠处理消息（storm中的通用名叫tuple）的框架，我们在写一个topology程序时，若需要保证spout产生的消息的可靠处理，需要做到两点：\n\n第一是spout/bolt每生成一个新的tuple都告诉storm一下（其中spout发出的tuple有个id叫rootId），从而让storm能够追踪rootId和每个衍生tuple的处理状态。\n\n第二是每个tuple被下游bolt处理完毕后，无论处理成功或失败，也再告诉storm一下，从而让storm知道是否需要spout重新发送rootId。\n\n做了这两件事，storm就能知道这个tuple是否被处理完毕。如果是处理成功了的，就说明最初从spout发出的tuple（rootId）已在topology中处理完毕，无需spout重新发送。如果是处理失败的，storm则会告知spout重新发送rootId这个tuple。\n\n在程序中实现消息可靠处理\n--\n那在写一个topology时，我们该如何做上面提到的两件事呢？\n<!--more-->\nStorm提供了BaseRichBolt抽象类（实现了IRichBolt接口），一个示例bolt如下：\n```java\npublic class SplitSentence extends BaseRichBolt {\n    OutputCollector _collector;\n \n    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {\n        _collector = collector;\n    }\n \n    public void execute(Tuple t) {\n        String sentence = t.getString(0);\n        for(String word: sentence.split(\" \")) {\n            //1. 告诉storm生成了一个新的tuple，并且这个tuple的锚点是tuple\n            _collector.emit(t, new Values(word));\n        }\n        _collector.ack(t); //2. 告诉storm，t这个tuple已处理完毕\n    }\n \n    public void declareOutputFields(OutputFieldsDeclarer declarer) {\n        declarer.declare(new Fields(\"word\"));\n    }\n}\n```\n\n这段代码就做了这两件事，一是输出新的tuple并告知storm，二是对当前tuple t处理完毕后，告知storm。\n\n对于第一件事，这里要注意的是，在BaseRichBolt中输出一个新的tuple（示例中是word）时，必须指定其锚点（即当前bolt正在处理的tuple），因为输出新的tuple会继续被下游bolt处理，这个锚点tuple和下游tuple之间的路径就是DAG的一条边。如果不指定锚点，则可以理解为storm不知道这条边的存在，也就不会对新输出的tuple进行跟踪了。\n\n如果我们确实不需要保证消息的可靠处理，则使用以下方式输出新tuple即可。\n\n```java\n_collector.emit(new Values(word));\n```\n\n另外，一个tuple的锚点tuple可以有多个，比如如下代码，新输出的tuple的锚点就是tuple1和tuple2。\n\n```java\nList anchors = new ArrayList();\nanchors.add(tuple1);\nanchors.add(tuple2);\n_collector.emit(anchors, new Values(1, 2, 3));\n```\n\n对于第二件事，通过调用OutputCollector的ack或fail方法，即可告知storm当前tuple的处理结果。比如，假设我们在bolt中做一些操作的时候出现异常（比如访问redis、DB、hdfs等），可以调fail方法快速重放rootId，避免等到storm判断这个tuple处理超时后才重放。\n\n更简便的方式\n--\n很明显，以上方式有几个弊端：\n\n1. 输出新tuple和对tuple的ack/fail操作需要我们自己维护，代价很高，容易遗忘。\n\n2. storm是在内存中维护每个tuple的处理状态，如果只对tuple进行锚点标记但处理完毕后忘记ack/fail，在tuple量非常大时，有内存溢出的风险。\n\n鉴于此，storm提供了BaseBasicBolt抽象类（实现了IBasicBolt接口）来帮助我们实现对每个tuple的锚点标记和ack/fail。\n前面的例子可改写如下：\n\n```java\npublic class SplitSentence extends BaseBasicBolt {\n    public void execute(Tuple tuple, BasicOutputCollector collector) {\n        String sentence = tuple.getString(0);\n        for(String word: sentence.split(\" \")) {\n            collector.emit(new Values(word));\n        }\n    }\n    public void declareOutputFields(OutputFieldsDeclarer declarer) {\n        declarer.declare(new Fields(\"word\"));\n    }\n}\n```\n\n可见，在代码中，我们只需要关心bolt的处理逻辑即可，至于标记锚点和ack/fail，均不用关心。\n\n细究一下storm框架对IBasicBolt的处理可知，在创建topology时，IBasicBolt是被封装在BasicBoltExecutor类（实现了IRichBolt接口）中处理的。\n\n构建topology时的setBolt方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20151105/bolt-executor.png)\n\n原理&示例\n--\n刚刚提到对每个topology，storm都在内存中维护其tuple的处理状态，那么对于一个大规模集群，storm是如何高效的维护大量tuple的处理状态的呢？\n\n其实，topology在运行时，内部有一组特殊的任务叫acker，专门用来做tuple的ack/fail。当一个root tuple（spout输出的tuple）在DAG中处理完毕后，acker会向产生该tuple的spout发送消息来ack这个tuple。\n\n我们可通过参数Config.TOPOLOGY_ACKER_EXECUTORS指定topology中的acker任务的数量，默认是与topology中的worker数相同，在处理大量消息的场景下，可以通过此参数增加topology的acker任务数，以提高对message做ack/fail的效率。\n\nstorm通过给每个tuple设置一个全局唯一id，并在输出tuple和tuple处理完毕时收集tuple的id，并进行异或运算，巧妙的实现tuple状态的维护。先看下图示例：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20151105/storm-ack.png)\n\n在这个topology其中包含一个spout，3个bolt和一个acker bolt，紫色线表示tuple的流向，绿色线表示每个bolt处理完tuple后的ack/fail调用，红色线表示acker回调spout的ack/fail方法来标记root tuple处理完毕。\n\n以下是storm的ack框架对tuple的处理状态维护过程说明：\n\n第(1)(2)步，spout发送T1到bolt1，发送T2到bolt2，T1和T2具有相同的内容（可以认为都把spout的输出作为自己的输入）。每条消息都会有一个全局唯一id，T1的锚点为<rootId,T1>，T2的锚点为<rootId,T2>。\n\n第(3)步，spout发送完毕T1、T2后，在acker中注册一条记录rootId=T1^T2。\n\n第(4)(5)步，bolt1收到T1处理完毕后对T1进行ack并发送T3,T4到bolt3，所以在acker中注册T1,T3,T4，acker中的跟踪项变为rootId=T1^T2^T1^T3^T4=T2^T3^T4\n\n第(6)(7)步，bolt2收到T2处理完毕后对T2进行ack并发送T5,T6,T7到bolt4，所以在acker中注册T2,T5,T6,T7，acker中的跟踪项变为rootId=T2^T3^T4^T2^T5^T6^T7=T3^T4^T5^T6^T7\n\n第(8)步，bolt3收到T3,T4处理完毕后对T3,T4进行ack，没有输出新的tuple，所以在acker中注册T3,T4，acker中的跟踪项变为rootId=T3^T4^T5^T6^T7^T3^T4=T5^T6^T7\n\n第(9)步，bolt4收到T5,T6,T7处理完毕后对T5,T6,T7进行ack，没有输出新的tuple，所以在acker中注册T5,T6,T7，acker中的跟踪项变为rootId=T5^T6^T7^T5^T6^T7=0\n\n第(10)步，acker bolt发现rootId对应的追踪值为0，说明该rootId对应的源消息以及衍生出来的所有消息（bolt1,bolt2新输出的消息）都被成功处理完毕。于是acker bolt会回调spout的ack方法，标识消息rootId已被topology处理成功。\n","slug":"Storm的消息可靠处理机制","published":1,"updated":"2017-01-18T09:54:11.733Z","_id":"ciy320aio0006ifs6zlkq920m","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>提交进入Storm运行的topology实际上是一个有向无环图（DAG），其中的节点是由spout和bolt组成，边则可以理解为消息从一个节点到传输到另一个节点的过程。对于spout产生的tuple，只有在topology上处理完毕后，才认为这个tuple被storm可靠处理。</p>\n<p>Storm提供了可靠处理消息（storm中的通用名叫tuple）的框架，我们在写一个topology程序时，若需要保证spout产生的消息的可靠处理，需要做到两点：</p>\n<p>第一是spout/bolt每生成一个新的tuple都告诉storm一下（其中spout发出的tuple有个id叫rootId），从而让storm能够追踪rootId和每个衍生tuple的处理状态。</p>\n<p>第二是每个tuple被下游bolt处理完毕后，无论处理成功或失败，也再告诉storm一下，从而让storm知道是否需要spout重新发送rootId。</p>\n<p>做了这两件事，storm就能知道这个tuple是否被处理完毕。如果是处理成功了的，就说明最初从spout发出的tuple（rootId）已在topology中处理完毕，无需spout重新发送。如果是处理失败的，storm则会告知spout重新发送rootId这个tuple。</p>\n<h2 id=\"在程序中实现消息可靠处理\"><a href=\"#在程序中实现消息可靠处理\" class=\"headerlink\" title=\"在程序中实现消息可靠处理\"></a>在程序中实现消息可靠处理</h2><p>那在写一个topology时，我们该如何做上面提到的两件事呢？<br><a id=\"more\"></a><br>Storm提供了BaseRichBolt抽象类（实现了IRichBolt接口），一个示例bolt如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SplitSentence</span> <span class=\"keyword\">extends</span> <span class=\"title\">BaseRichBolt</span> </span>&#123;</div><div class=\"line\">    OutputCollector _collector;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">prepare</span><span class=\"params\">(Map conf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</div><div class=\"line\">        _collector = collector;</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple t)</span> </span>&#123;</div><div class=\"line\">        String sentence = t.getString(<span class=\"number\">0</span>);</div><div class=\"line\">        <span class=\"keyword\">for</span>(String word: sentence.split(<span class=\"string\">\" \"</span>)) &#123;</div><div class=\"line\">            <span class=\"comment\">//1. 告诉storm生成了一个新的tuple，并且这个tuple的锚点是tuple</span></div><div class=\"line\">            _collector.emit(t, <span class=\"keyword\">new</span> Values(word));</div><div class=\"line\">        &#125;</div><div class=\"line\">        _collector.ack(t); <span class=\"comment\">//2. 告诉storm，t这个tuple已处理完毕</span></div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareOutputFields</span><span class=\"params\">(OutputFieldsDeclarer declarer)</span> </span>&#123;</div><div class=\"line\">        declarer.declare(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>这段代码就做了这两件事，一是输出新的tuple并告知storm，二是对当前tuple t处理完毕后，告知storm。</p>\n<p>对于第一件事，这里要注意的是，在BaseRichBolt中输出一个新的tuple（示例中是word）时，必须指定其锚点（即当前bolt正在处理的tuple），因为输出新的tuple会继续被下游bolt处理，这个锚点tuple和下游tuple之间的路径就是DAG的一条边。如果不指定锚点，则可以理解为storm不知道这条边的存在，也就不会对新输出的tuple进行跟踪了。</p>\n<p>如果我们确实不需要保证消息的可靠处理，则使用以下方式输出新tuple即可。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">_collector.emit(<span class=\"keyword\">new</span> Values(word));</div></pre></td></tr></table></figure>\n<p>另外，一个tuple的锚点tuple可以有多个，比如如下代码，新输出的tuple的锚点就是tuple1和tuple2。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">List anchors = <span class=\"keyword\">new</span> ArrayList();</div><div class=\"line\">anchors.add(tuple1);</div><div class=\"line\">anchors.add(tuple2);</div><div class=\"line\">_collector.emit(anchors, <span class=\"keyword\">new</span> Values(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>));</div></pre></td></tr></table></figure>\n<p>对于第二件事，通过调用OutputCollector的ack或fail方法，即可告知storm当前tuple的处理结果。比如，假设我们在bolt中做一些操作的时候出现异常（比如访问redis、DB、hdfs等），可以调fail方法快速重放rootId，避免等到storm判断这个tuple处理超时后才重放。</p>\n<h2 id=\"更简便的方式\"><a href=\"#更简便的方式\" class=\"headerlink\" title=\"更简便的方式\"></a>更简便的方式</h2><p>很明显，以上方式有几个弊端：</p>\n<ol>\n<li><p>输出新tuple和对tuple的ack/fail操作需要我们自己维护，代价很高，容易遗忘。</p>\n</li>\n<li><p>storm是在内存中维护每个tuple的处理状态，如果只对tuple进行锚点标记但处理完毕后忘记ack/fail，在tuple量非常大时，有内存溢出的风险。</p>\n</li>\n</ol>\n<p>鉴于此，storm提供了BaseBasicBolt抽象类（实现了IBasicBolt接口）来帮助我们实现对每个tuple的锚点标记和ack/fail。<br>前面的例子可改写如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SplitSentence</span> <span class=\"keyword\">extends</span> <span class=\"title\">BaseBasicBolt</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</div><div class=\"line\">        String sentence = tuple.getString(<span class=\"number\">0</span>);</div><div class=\"line\">        <span class=\"keyword\">for</span>(String word: sentence.split(<span class=\"string\">\" \"</span>)) &#123;</div><div class=\"line\">            collector.emit(<span class=\"keyword\">new</span> Values(word));</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareOutputFields</span><span class=\"params\">(OutputFieldsDeclarer declarer)</span> </span>&#123;</div><div class=\"line\">        declarer.declare(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>可见，在代码中，我们只需要关心bolt的处理逻辑即可，至于标记锚点和ack/fail，均不用关心。</p>\n<p>细究一下storm框架对IBasicBolt的处理可知，在创建topology时，IBasicBolt是被封装在BasicBoltExecutor类（实现了IRichBolt接口）中处理的。</p>\n<p>构建topology时的setBolt方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20151105/bolt-executor.png\" alt=\"\"></p>\n<h2 id=\"原理-amp-示例\"><a href=\"#原理-amp-示例\" class=\"headerlink\" title=\"原理&amp;示例\"></a>原理&amp;示例</h2><p>刚刚提到对每个topology，storm都在内存中维护其tuple的处理状态，那么对于一个大规模集群，storm是如何高效的维护大量tuple的处理状态的呢？</p>\n<p>其实，topology在运行时，内部有一组特殊的任务叫acker，专门用来做tuple的ack/fail。当一个root tuple（spout输出的tuple）在DAG中处理完毕后，acker会向产生该tuple的spout发送消息来ack这个tuple。</p>\n<p>我们可通过参数Config.TOPOLOGY_ACKER_EXECUTORS指定topology中的acker任务的数量，默认是与topology中的worker数相同，在处理大量消息的场景下，可以通过此参数增加topology的acker任务数，以提高对message做ack/fail的效率。</p>\n<p>storm通过给每个tuple设置一个全局唯一id，并在输出tuple和tuple处理完毕时收集tuple的id，并进行异或运算，巧妙的实现tuple状态的维护。先看下图示例：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20151105/storm-ack.png\" alt=\"\"></p>\n<p>在这个topology其中包含一个spout，3个bolt和一个acker bolt，紫色线表示tuple的流向，绿色线表示每个bolt处理完tuple后的ack/fail调用，红色线表示acker回调spout的ack/fail方法来标记root tuple处理完毕。</p>\n<p>以下是storm的ack框架对tuple的处理状态维护过程说明：</p>\n<p>第(1)(2)步，spout发送T1到bolt1，发送T2到bolt2，T1和T2具有相同的内容（可以认为都把spout的输出作为自己的输入）。每条消息都会有一个全局唯一id，T1的锚点为<rootid,t1>，T2的锚点为<rootid,t2>。</rootid,t2></rootid,t1></p>\n<p>第(3)步，spout发送完毕T1、T2后，在acker中注册一条记录rootId=T1^T2。</p>\n<p>第(4)(5)步，bolt1收到T1处理完毕后对T1进行ack并发送T3,T4到bolt3，所以在acker中注册T1,T3,T4，acker中的跟踪项变为rootId=T1^T2^T1^T3^T4=T2^T3^T4</p>\n<p>第(6)(7)步，bolt2收到T2处理完毕后对T2进行ack并发送T5,T6,T7到bolt4，所以在acker中注册T2,T5,T6,T7，acker中的跟踪项变为rootId=T2^T3^T4^T2^T5^T6^T7=T3^T4^T5^T6^T7</p>\n<p>第(8)步，bolt3收到T3,T4处理完毕后对T3,T4进行ack，没有输出新的tuple，所以在acker中注册T3,T4，acker中的跟踪项变为rootId=T3^T4^T5^T6^T7^T3^T4=T5^T6^T7</p>\n<p>第(9)步，bolt4收到T5,T6,T7处理完毕后对T5,T6,T7进行ack，没有输出新的tuple，所以在acker中注册T5,T6,T7，acker中的跟踪项变为rootId=T5^T6^T7^T5^T6^T7=0</p>\n<p>第(10)步，acker bolt发现rootId对应的追踪值为0，说明该rootId对应的源消息以及衍生出来的所有消息（bolt1,bolt2新输出的消息）都被成功处理完毕。于是acker bolt会回调spout的ack方法，标识消息rootId已被topology处理成功。</p>\n","excerpt":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>提交进入Storm运行的topology实际上是一个有向无环图（DAG），其中的节点是由spout和bolt组成，边则可以理解为消息从一个节点到传输到另一个节点的过程。对于spout产生的tuple，只有在topology上处理完毕后，才认为这个tuple被storm可靠处理。</p>\n<p>Storm提供了可靠处理消息（storm中的通用名叫tuple）的框架，我们在写一个topology程序时，若需要保证spout产生的消息的可靠处理，需要做到两点：</p>\n<p>第一是spout/bolt每生成一个新的tuple都告诉storm一下（其中spout发出的tuple有个id叫rootId），从而让storm能够追踪rootId和每个衍生tuple的处理状态。</p>\n<p>第二是每个tuple被下游bolt处理完毕后，无论处理成功或失败，也再告诉storm一下，从而让storm知道是否需要spout重新发送rootId。</p>\n<p>做了这两件事，storm就能知道这个tuple是否被处理完毕。如果是处理成功了的，就说明最初从spout发出的tuple（rootId）已在topology中处理完毕，无需spout重新发送。如果是处理失败的，storm则会告知spout重新发送rootId这个tuple。</p>\n<h2 id=\"在程序中实现消息可靠处理\"><a href=\"#在程序中实现消息可靠处理\" class=\"headerlink\" title=\"在程序中实现消息可靠处理\"></a>在程序中实现消息可靠处理</h2><p>那在写一个topology时，我们该如何做上面提到的两件事呢？<br>","more":"<br>Storm提供了BaseRichBolt抽象类（实现了IRichBolt接口），一个示例bolt如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SplitSentence</span> <span class=\"keyword\">extends</span> <span class=\"title\">BaseRichBolt</span> </span>&#123;</div><div class=\"line\">    OutputCollector _collector;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">prepare</span><span class=\"params\">(Map conf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</div><div class=\"line\">        _collector = collector;</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple t)</span> </span>&#123;</div><div class=\"line\">        String sentence = t.getString(<span class=\"number\">0</span>);</div><div class=\"line\">        <span class=\"keyword\">for</span>(String word: sentence.split(<span class=\"string\">\" \"</span>)) &#123;</div><div class=\"line\">            <span class=\"comment\">//1. 告诉storm生成了一个新的tuple，并且这个tuple的锚点是tuple</span></div><div class=\"line\">            _collector.emit(t, <span class=\"keyword\">new</span> Values(word));</div><div class=\"line\">        &#125;</div><div class=\"line\">        _collector.ack(t); <span class=\"comment\">//2. 告诉storm，t这个tuple已处理完毕</span></div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareOutputFields</span><span class=\"params\">(OutputFieldsDeclarer declarer)</span> </span>&#123;</div><div class=\"line\">        declarer.declare(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>这段代码就做了这两件事，一是输出新的tuple并告知storm，二是对当前tuple t处理完毕后，告知storm。</p>\n<p>对于第一件事，这里要注意的是，在BaseRichBolt中输出一个新的tuple（示例中是word）时，必须指定其锚点（即当前bolt正在处理的tuple），因为输出新的tuple会继续被下游bolt处理，这个锚点tuple和下游tuple之间的路径就是DAG的一条边。如果不指定锚点，则可以理解为storm不知道这条边的存在，也就不会对新输出的tuple进行跟踪了。</p>\n<p>如果我们确实不需要保证消息的可靠处理，则使用以下方式输出新tuple即可。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">_collector.emit(<span class=\"keyword\">new</span> Values(word));</div></pre></td></tr></table></figure>\n<p>另外，一个tuple的锚点tuple可以有多个，比如如下代码，新输出的tuple的锚点就是tuple1和tuple2。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">List anchors = <span class=\"keyword\">new</span> ArrayList();</div><div class=\"line\">anchors.add(tuple1);</div><div class=\"line\">anchors.add(tuple2);</div><div class=\"line\">_collector.emit(anchors, <span class=\"keyword\">new</span> Values(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>));</div></pre></td></tr></table></figure>\n<p>对于第二件事，通过调用OutputCollector的ack或fail方法，即可告知storm当前tuple的处理结果。比如，假设我们在bolt中做一些操作的时候出现异常（比如访问redis、DB、hdfs等），可以调fail方法快速重放rootId，避免等到storm判断这个tuple处理超时后才重放。</p>\n<h2 id=\"更简便的方式\"><a href=\"#更简便的方式\" class=\"headerlink\" title=\"更简便的方式\"></a>更简便的方式</h2><p>很明显，以上方式有几个弊端：</p>\n<ol>\n<li><p>输出新tuple和对tuple的ack/fail操作需要我们自己维护，代价很高，容易遗忘。</p>\n</li>\n<li><p>storm是在内存中维护每个tuple的处理状态，如果只对tuple进行锚点标记但处理完毕后忘记ack/fail，在tuple量非常大时，有内存溢出的风险。</p>\n</li>\n</ol>\n<p>鉴于此，storm提供了BaseBasicBolt抽象类（实现了IBasicBolt接口）来帮助我们实现对每个tuple的锚点标记和ack/fail。<br>前面的例子可改写如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SplitSentence</span> <span class=\"keyword\">extends</span> <span class=\"title\">BaseBasicBolt</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</div><div class=\"line\">        String sentence = tuple.getString(<span class=\"number\">0</span>);</div><div class=\"line\">        <span class=\"keyword\">for</span>(String word: sentence.split(<span class=\"string\">\" \"</span>)) &#123;</div><div class=\"line\">            collector.emit(<span class=\"keyword\">new</span> Values(word));</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareOutputFields</span><span class=\"params\">(OutputFieldsDeclarer declarer)</span> </span>&#123;</div><div class=\"line\">        declarer.declare(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>可见，在代码中，我们只需要关心bolt的处理逻辑即可，至于标记锚点和ack/fail，均不用关心。</p>\n<p>细究一下storm框架对IBasicBolt的处理可知，在创建topology时，IBasicBolt是被封装在BasicBoltExecutor类（实现了IRichBolt接口）中处理的。</p>\n<p>构建topology时的setBolt方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20151105/bolt-executor.png\" alt=\"\"></p>\n<h2 id=\"原理-amp-示例\"><a href=\"#原理-amp-示例\" class=\"headerlink\" title=\"原理&amp;示例\"></a>原理&amp;示例</h2><p>刚刚提到对每个topology，storm都在内存中维护其tuple的处理状态，那么对于一个大规模集群，storm是如何高效的维护大量tuple的处理状态的呢？</p>\n<p>其实，topology在运行时，内部有一组特殊的任务叫acker，专门用来做tuple的ack/fail。当一个root tuple（spout输出的tuple）在DAG中处理完毕后，acker会向产生该tuple的spout发送消息来ack这个tuple。</p>\n<p>我们可通过参数Config.TOPOLOGY_ACKER_EXECUTORS指定topology中的acker任务的数量，默认是与topology中的worker数相同，在处理大量消息的场景下，可以通过此参数增加topology的acker任务数，以提高对message做ack/fail的效率。</p>\n<p>storm通过给每个tuple设置一个全局唯一id，并在输出tuple和tuple处理完毕时收集tuple的id，并进行异或运算，巧妙的实现tuple状态的维护。先看下图示例：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20151105/storm-ack.png\" alt=\"\"></p>\n<p>在这个topology其中包含一个spout，3个bolt和一个acker bolt，紫色线表示tuple的流向，绿色线表示每个bolt处理完tuple后的ack/fail调用，红色线表示acker回调spout的ack/fail方法来标记root tuple处理完毕。</p>\n<p>以下是storm的ack框架对tuple的处理状态维护过程说明：</p>\n<p>第(1)(2)步，spout发送T1到bolt1，发送T2到bolt2，T1和T2具有相同的内容（可以认为都把spout的输出作为自己的输入）。每条消息都会有一个全局唯一id，T1的锚点为<rootId,T1>，T2的锚点为<rootId,T2>。</p>\n<p>第(3)步，spout发送完毕T1、T2后，在acker中注册一条记录rootId=T1^T2。</p>\n<p>第(4)(5)步，bolt1收到T1处理完毕后对T1进行ack并发送T3,T4到bolt3，所以在acker中注册T1,T3,T4，acker中的跟踪项变为rootId=T1^T2^T1^T3^T4=T2^T3^T4</p>\n<p>第(6)(7)步，bolt2收到T2处理完毕后对T2进行ack并发送T5,T6,T7到bolt4，所以在acker中注册T2,T5,T6,T7，acker中的跟踪项变为rootId=T2^T3^T4^T2^T5^T6^T7=T3^T4^T5^T6^T7</p>\n<p>第(8)步，bolt3收到T3,T4处理完毕后对T3,T4进行ack，没有输出新的tuple，所以在acker中注册T3,T4，acker中的跟踪项变为rootId=T3^T4^T5^T6^T7^T3^T4=T5^T6^T7</p>\n<p>第(9)步，bolt4收到T5,T6,T7处理完毕后对T5,T6,T7进行ack，没有输出新的tuple，所以在acker中注册T5,T6,T7，acker中的跟踪项变为rootId=T5^T6^T7^T5^T6^T7=0</p>\n<p>第(10)步，acker bolt发现rootId对应的追踪值为0，说明该rootId对应的源消息以及衍生出来的所有消息（bolt1,bolt2新输出的消息）都被成功处理完毕。于是acker bolt会回调spout的ack方法，标识消息rootId已被topology处理成功。</p>"},{"title":"kafka-0.10.0启动过程分析","date":"2016-07-08T14:46:18.000Z","_content":"\nkafka-0.10.0是官方出的最新稳定版本，提供了大量新的feature，具体可见[这里](http://www.iteblog.com/archives/1677)，本文主要分析kafka-0.10-0的源码结构和启动过程。\n\n源码结构\n--\nkafka-0.10.0的源码可以从github上fork一份，在源码目录下执行./gradlew idea生成idea项目，然后导入idea即可。这中间需要使用gradle进行依赖包的下载，导入后可以看到其源码结构如下图所示：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160708-kafka0.10.0%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/1.png)\n\n包括几大重要模块：\n\n* clients主要是kafka-client相关的代码，包括consumer、producer，还包括一些公共逻辑，如授权认证、序列化等。\n* connect主要是kafka-connect模块的代码逻辑，Kafka connect是0.9版本增加的特性,支持创建和管理数据流管道。通过它可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统，比如数据库、elastic search等。\n* core模块是kafka的核心部分，主要包括broker的实现逻辑、producer和consumer的javaapi等。\n* streams模块主要是kafka-streaming的实现，提供了一整套描述常见流操作的高级语言API（比如 joining, filtering以及aggregation等），我们可以基于此开发流处理应用程序。\n\n<!--more-->\n\n启动入口\n--\nkafka的启动入口在core_main这个module下，入口函数如下：\n\n```scala\ndef main(args: Array[String]): Unit = {\n    try {\n      val serverProps = getPropsFromArgs(args)\n      val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps)\n\n      // attach shutdown handler to catch control-c\n      Runtime.getRuntime().addShutdownHook(new Thread() {\n        override def run() = {\n          kafkaServerStartable.shutdown\n        }\n      })\n\n      kafkaServerStartable.startup\n      kafkaServerStartable.awaitShutdown\n    }\n    catch {\n      case e: Throwable =>\n        fatal(e)\n        System.exit(1)\n    }\n    System.exit(0)\n  }\n```\n\n先从命令行指定的配置文件加载配置，然后通过KafkaServerStartable类启动broker，实际上在KafkaServerStartable中维护了一个KafkaServer对象，它通过调用KafkaServer的startup方法启动broker。\n\nbroker启动过程\n--\n\n下面并启动过程代码按启动顺序分两部分做说明。\n\n第一部分主要是核心模块的启动，代码如下：\n\n```scala\nmetrics = new Metrics(metricConfig, reporters, kafkaMetricsTime, true)\n\n        brokerState.newState(Starting)\n\n        /* start scheduler */\n        kafkaScheduler.startup()\n\n        /* setup zookeeper */\n        zkUtils = initZk()\n\n        /* start log manager */\n        logManager = createLogManager(zkUtils.zkClient, brokerState)\n        logManager.startup()\n\n        /* generate brokerId */\n        config.brokerId =  getBrokerId\n        this.logIdent = \"[Kafka Server \" + config.brokerId + \"], \"\n\n        socketServer = new SocketServer(config, metrics, kafkaMetricsTime)\n        socketServer.startup()\n\n        /* start replica manager */\n        replicaManager = new ReplicaManager(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager,\n          isShuttingDown)\n        replicaManager.startup()\n\n        /* start kafka controller */\n        kafkaController = new KafkaController(config, zkUtils, brokerState, kafkaMetricsTime, metrics, threadNamePrefix)\n        kafkaController.startup()\n\n        /* start group coordinator */\n        groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, kafkaMetricsTime)\n        groupCoordinator.startup()\n\n        /* Get the authorizer and initialize it if one is specified.*/\n        authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map { authorizerClassName =>\n          val authZ = CoreUtils.createObject[Authorizer](authorizerClassName)\n          authZ.configure(config.originals())\n          authZ\n        }\n\n        /* start processing requests */\n        apis = new KafkaApis(socketServer.requestChannel, replicaManager, groupCoordinator,\n          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer)\n        requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads)\n        brokerState.newState(RunningAsBroker)\n```\n\n1. 首先是初始化Metrics注册信息。\n2. 接着把当前broker的状态先置为Starting。\n3. 启动kafkaScheduler，其内部维护了一个ScheduledThreadPoolExecutor，用于执行broker内置的一些周期性运行的job或定时job。比如，启动自动提交时，broker会定期维护客户端的消费topic-partition的offset信息。\n4. 初始化zookeeper访问工具，建立必要的数据路径。\n5. 启动LogManager，也就是日志数据管理子系统，负责日志数据的创建、截断、滚动、和清理等。\n6. 启动SocketServer，一个基于NIO的socker服务端，其线程模型是有一个acceptor线程来接受客户端的连接，对应这个acceptor有N个processor线程，每个processor有自己的selector来从sockets读取收到的请求。另外，有M个handler线程专门处理请求并把处理结果返回给processor线程并通过socket写回给客户端。\n7. 启动ReplicaManager，也即副本管理器，用于管理每个topic-partition的副本状态，包括主从、ISR列表等。\n8. 启动KafkaController，可以理解为kafka集群的中央控制器，负责全局的协调，比如选取leader，reassignment等，其自身也支持动态选举高可用。\n9. 启动GroupCoordinator，主要用于broker组管理和offset管理。\n10. 初始化授权认证管理器，用户可以自己通过参数authorizer.class.name指定具体的Authorizer实现。kafka自带有SimpleAclAuthorizer的简单实现。\n11. 初始化KafkaApis，用于统一接收外部请求。\n12. 初始化KafkaRequestHandlerPool，内部是一个线程池，用于具体处理外部请求。\n13. 将当前broker的状态置为RunningAsBroker，这时，broker已经可以对外提供服务了。\n\n第二部分主要是辅助模块的启动，代码如下：\n\n```scala\n        Mx4jLoader.maybeLoad()\n\n        /* start dynamic config manager */\n        dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -> new TopicConfigHandler(logManager, config),\n                                                           ConfigType.Client -> new ClientIdConfigHandler(apis.quotaManagers))\n\n        // Apply all existing client configs to the ClientIdConfigHandler to bootstrap the overrides\n        // TODO: Move this logic to DynamicConfigManager\n        AdminUtils.fetchAllEntityConfigs(zkUtils, ConfigType.Client).foreach {\n          case (clientId, properties) => dynamicConfigHandlers(ConfigType.Client).processConfigChanges(clientId, properties)\n        }\n\n        // Create the config manager. start listening to notifications\n        dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers)\n        dynamicConfigManager.startup()\n\n        /* tell everyone we are alive */\n        val listeners = config.advertisedListeners.map {case(protocol, endpoint) =>\n          if (endpoint.port == 0)\n            (protocol, EndPoint(endpoint.host, socketServer.boundPort(protocol), endpoint.protocolType))\n          else\n            (protocol, endpoint)\n        }\n        kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack,\n          config.interBrokerProtocolVersion)\n        kafkaHealthcheck.startup()\n\n        // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it\n        checkpointBrokerId(config.brokerId)\n\n        /* register broker metrics */\n        registerStats()\n\n        shutdownLatch = new CountDownLatch(1)\n        startupComplete.set(true)\n        isStartingUp.set(false)\n        AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString)\n        info(\"started\")\n```\n\n1. 启动jmx，通过参数kafka_mx4jenable控制是否启用jmx，默认为false。\n2. 初始化TopicConfigHandler和ClientIdConfigHandler，前者用于处理zk上的topic配置变更信息，后者用于zk上的clientId配置变更信息。\n3. 启动DynamicConfigManager，通过动态配置管理器，监听zk上的配置节点变化，并根据具体变化的配置信息调用TopicConfigHandler或ClientIdConfigHandler更新配置。\n4. 启动KafkaHealthcheck，用于在zk上注册当前broker节点信息，以便节点退出时其他broker和consumer能监听到，目前的节点健康度判断比较简单，只是单纯的看zk上的节点是否存在。\n5. 最后，在本地对当前broker做个checkpoint，并注册jmx bean信息\n\n\n\n\n\n\n","source":"_posts/kafka-0-10-0启动过程分析.md","raw":"---\ntitle: kafka-0.10.0启动过程分析\ndate: 2016-07-08 22:46:18\ntags:\n- kafka\n- 源码分析\ncategories:\n- Kafka\n---\n\nkafka-0.10.0是官方出的最新稳定版本，提供了大量新的feature，具体可见[这里](http://www.iteblog.com/archives/1677)，本文主要分析kafka-0.10-0的源码结构和启动过程。\n\n源码结构\n--\nkafka-0.10.0的源码可以从github上fork一份，在源码目录下执行./gradlew idea生成idea项目，然后导入idea即可。这中间需要使用gradle进行依赖包的下载，导入后可以看到其源码结构如下图所示：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160708-kafka0.10.0%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/1.png)\n\n包括几大重要模块：\n\n* clients主要是kafka-client相关的代码，包括consumer、producer，还包括一些公共逻辑，如授权认证、序列化等。\n* connect主要是kafka-connect模块的代码逻辑，Kafka connect是0.9版本增加的特性,支持创建和管理数据流管道。通过它可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统，比如数据库、elastic search等。\n* core模块是kafka的核心部分，主要包括broker的实现逻辑、producer和consumer的javaapi等。\n* streams模块主要是kafka-streaming的实现，提供了一整套描述常见流操作的高级语言API（比如 joining, filtering以及aggregation等），我们可以基于此开发流处理应用程序。\n\n<!--more-->\n\n启动入口\n--\nkafka的启动入口在core_main这个module下，入口函数如下：\n\n```scala\ndef main(args: Array[String]): Unit = {\n    try {\n      val serverProps = getPropsFromArgs(args)\n      val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps)\n\n      // attach shutdown handler to catch control-c\n      Runtime.getRuntime().addShutdownHook(new Thread() {\n        override def run() = {\n          kafkaServerStartable.shutdown\n        }\n      })\n\n      kafkaServerStartable.startup\n      kafkaServerStartable.awaitShutdown\n    }\n    catch {\n      case e: Throwable =>\n        fatal(e)\n        System.exit(1)\n    }\n    System.exit(0)\n  }\n```\n\n先从命令行指定的配置文件加载配置，然后通过KafkaServerStartable类启动broker，实际上在KafkaServerStartable中维护了一个KafkaServer对象，它通过调用KafkaServer的startup方法启动broker。\n\nbroker启动过程\n--\n\n下面并启动过程代码按启动顺序分两部分做说明。\n\n第一部分主要是核心模块的启动，代码如下：\n\n```scala\nmetrics = new Metrics(metricConfig, reporters, kafkaMetricsTime, true)\n\n        brokerState.newState(Starting)\n\n        /* start scheduler */\n        kafkaScheduler.startup()\n\n        /* setup zookeeper */\n        zkUtils = initZk()\n\n        /* start log manager */\n        logManager = createLogManager(zkUtils.zkClient, brokerState)\n        logManager.startup()\n\n        /* generate brokerId */\n        config.brokerId =  getBrokerId\n        this.logIdent = \"[Kafka Server \" + config.brokerId + \"], \"\n\n        socketServer = new SocketServer(config, metrics, kafkaMetricsTime)\n        socketServer.startup()\n\n        /* start replica manager */\n        replicaManager = new ReplicaManager(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager,\n          isShuttingDown)\n        replicaManager.startup()\n\n        /* start kafka controller */\n        kafkaController = new KafkaController(config, zkUtils, brokerState, kafkaMetricsTime, metrics, threadNamePrefix)\n        kafkaController.startup()\n\n        /* start group coordinator */\n        groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, kafkaMetricsTime)\n        groupCoordinator.startup()\n\n        /* Get the authorizer and initialize it if one is specified.*/\n        authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map { authorizerClassName =>\n          val authZ = CoreUtils.createObject[Authorizer](authorizerClassName)\n          authZ.configure(config.originals())\n          authZ\n        }\n\n        /* start processing requests */\n        apis = new KafkaApis(socketServer.requestChannel, replicaManager, groupCoordinator,\n          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer)\n        requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads)\n        brokerState.newState(RunningAsBroker)\n```\n\n1. 首先是初始化Metrics注册信息。\n2. 接着把当前broker的状态先置为Starting。\n3. 启动kafkaScheduler，其内部维护了一个ScheduledThreadPoolExecutor，用于执行broker内置的一些周期性运行的job或定时job。比如，启动自动提交时，broker会定期维护客户端的消费topic-partition的offset信息。\n4. 初始化zookeeper访问工具，建立必要的数据路径。\n5. 启动LogManager，也就是日志数据管理子系统，负责日志数据的创建、截断、滚动、和清理等。\n6. 启动SocketServer，一个基于NIO的socker服务端，其线程模型是有一个acceptor线程来接受客户端的连接，对应这个acceptor有N个processor线程，每个processor有自己的selector来从sockets读取收到的请求。另外，有M个handler线程专门处理请求并把处理结果返回给processor线程并通过socket写回给客户端。\n7. 启动ReplicaManager，也即副本管理器，用于管理每个topic-partition的副本状态，包括主从、ISR列表等。\n8. 启动KafkaController，可以理解为kafka集群的中央控制器，负责全局的协调，比如选取leader，reassignment等，其自身也支持动态选举高可用。\n9. 启动GroupCoordinator，主要用于broker组管理和offset管理。\n10. 初始化授权认证管理器，用户可以自己通过参数authorizer.class.name指定具体的Authorizer实现。kafka自带有SimpleAclAuthorizer的简单实现。\n11. 初始化KafkaApis，用于统一接收外部请求。\n12. 初始化KafkaRequestHandlerPool，内部是一个线程池，用于具体处理外部请求。\n13. 将当前broker的状态置为RunningAsBroker，这时，broker已经可以对外提供服务了。\n\n第二部分主要是辅助模块的启动，代码如下：\n\n```scala\n        Mx4jLoader.maybeLoad()\n\n        /* start dynamic config manager */\n        dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -> new TopicConfigHandler(logManager, config),\n                                                           ConfigType.Client -> new ClientIdConfigHandler(apis.quotaManagers))\n\n        // Apply all existing client configs to the ClientIdConfigHandler to bootstrap the overrides\n        // TODO: Move this logic to DynamicConfigManager\n        AdminUtils.fetchAllEntityConfigs(zkUtils, ConfigType.Client).foreach {\n          case (clientId, properties) => dynamicConfigHandlers(ConfigType.Client).processConfigChanges(clientId, properties)\n        }\n\n        // Create the config manager. start listening to notifications\n        dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers)\n        dynamicConfigManager.startup()\n\n        /* tell everyone we are alive */\n        val listeners = config.advertisedListeners.map {case(protocol, endpoint) =>\n          if (endpoint.port == 0)\n            (protocol, EndPoint(endpoint.host, socketServer.boundPort(protocol), endpoint.protocolType))\n          else\n            (protocol, endpoint)\n        }\n        kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack,\n          config.interBrokerProtocolVersion)\n        kafkaHealthcheck.startup()\n\n        // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it\n        checkpointBrokerId(config.brokerId)\n\n        /* register broker metrics */\n        registerStats()\n\n        shutdownLatch = new CountDownLatch(1)\n        startupComplete.set(true)\n        isStartingUp.set(false)\n        AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString)\n        info(\"started\")\n```\n\n1. 启动jmx，通过参数kafka_mx4jenable控制是否启用jmx，默认为false。\n2. 初始化TopicConfigHandler和ClientIdConfigHandler，前者用于处理zk上的topic配置变更信息，后者用于zk上的clientId配置变更信息。\n3. 启动DynamicConfigManager，通过动态配置管理器，监听zk上的配置节点变化，并根据具体变化的配置信息调用TopicConfigHandler或ClientIdConfigHandler更新配置。\n4. 启动KafkaHealthcheck，用于在zk上注册当前broker节点信息，以便节点退出时其他broker和consumer能监听到，目前的节点健康度判断比较简单，只是单纯的看zk上的节点是否存在。\n5. 最后，在本地对当前broker做个checkpoint，并注册jmx bean信息\n\n\n\n\n\n\n","slug":"kafka-0-10-0启动过程分析","published":1,"updated":"2017-01-18T09:54:11.734Z","_id":"ciy320air0007ifs6u0ptvqng","comments":1,"layout":"post","photos":[],"link":"","content":"<p>kafka-0.10.0是官方出的最新稳定版本，提供了大量新的feature，具体可见<a href=\"http://www.iteblog.com/archives/1677\" target=\"_blank\" rel=\"external\">这里</a>，本文主要分析kafka-0.10-0的源码结构和启动过程。</p>\n<h2 id=\"源码结构\"><a href=\"#源码结构\" class=\"headerlink\" title=\"源码结构\"></a>源码结构</h2><p>kafka-0.10.0的源码可以从github上fork一份，在源码目录下执行./gradlew idea生成idea项目，然后导入idea即可。这中间需要使用gradle进行依赖包的下载，导入后可以看到其源码结构如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160708-kafka0.10.0%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/1.png\" alt=\"\"></p>\n<p>包括几大重要模块：</p>\n<ul>\n<li>clients主要是kafka-client相关的代码，包括consumer、producer，还包括一些公共逻辑，如授权认证、序列化等。</li>\n<li>connect主要是kafka-connect模块的代码逻辑，Kafka connect是0.9版本增加的特性,支持创建和管理数据流管道。通过它可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统，比如数据库、elastic search等。</li>\n<li>core模块是kafka的核心部分，主要包括broker的实现逻辑、producer和consumer的javaapi等。</li>\n<li>streams模块主要是kafka-streaming的实现，提供了一整套描述常见流操作的高级语言API（比如 joining, filtering以及aggregation等），我们可以基于此开发流处理应用程序。</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"启动入口\"><a href=\"#启动入口\" class=\"headerlink\" title=\"启动入口\"></a>启动入口</h2><p>kafka的启动入口在core_main这个module下，入口函数如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> serverProps = getPropsFromArgs(args)</div><div class=\"line\">      <span class=\"keyword\">val</span> kafkaServerStartable = <span class=\"type\">KafkaServerStartable</span>.fromProps(serverProps)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// attach shutdown handler to catch control-c</span></div><div class=\"line\">      <span class=\"type\">Runtime</span>.getRuntime().addShutdownHook(<span class=\"keyword\">new</span> <span class=\"type\">Thread</span>() &#123;</div><div class=\"line\">        <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>() = &#123;</div><div class=\"line\">          kafkaServerStartable.shutdown</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;)</div><div class=\"line\"></div><div class=\"line\">      kafkaServerStartable.startup</div><div class=\"line\">      kafkaServerStartable.awaitShutdown</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</div><div class=\"line\">        fatal(e)</div><div class=\"line\">        <span class=\"type\">System</span>.exit(<span class=\"number\">1</span>)</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"type\">System</span>.exit(<span class=\"number\">0</span>)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<p>先从命令行指定的配置文件加载配置，然后通过KafkaServerStartable类启动broker，实际上在KafkaServerStartable中维护了一个KafkaServer对象，它通过调用KafkaServer的startup方法启动broker。</p>\n<h2 id=\"broker启动过程\"><a href=\"#broker启动过程\" class=\"headerlink\" title=\"broker启动过程\"></a>broker启动过程</h2><p>下面并启动过程代码按启动顺序分两部分做说明。</p>\n<p>第一部分主要是核心模块的启动，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">metrics = <span class=\"keyword\">new</span> <span class=\"type\">Metrics</span>(metricConfig, reporters, kafkaMetricsTime, <span class=\"literal\">true</span>)</div><div class=\"line\"></div><div class=\"line\">        brokerState.newState(<span class=\"type\">Starting</span>)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start scheduler */</span></div><div class=\"line\">        kafkaScheduler.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* setup zookeeper */</span></div><div class=\"line\">        zkUtils = initZk()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start log manager */</span></div><div class=\"line\">        logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class=\"line\">        logManager.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* generate brokerId */</span></div><div class=\"line\">        config.brokerId =  getBrokerId</div><div class=\"line\">        <span class=\"keyword\">this</span>.logIdent = <span class=\"string\">\"[Kafka Server \"</span> + config.brokerId + <span class=\"string\">\"], \"</span></div><div class=\"line\"></div><div class=\"line\">        socketServer = <span class=\"keyword\">new</span> <span class=\"type\">SocketServer</span>(config, metrics, kafkaMetricsTime)</div><div class=\"line\">        socketServer.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start replica manager */</span></div><div class=\"line\">        replicaManager = <span class=\"keyword\">new</span> <span class=\"type\">ReplicaManager</span>(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager,</div><div class=\"line\">          isShuttingDown)</div><div class=\"line\">        replicaManager.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start kafka controller */</span></div><div class=\"line\">        kafkaController = <span class=\"keyword\">new</span> <span class=\"type\">KafkaController</span>(config, zkUtils, brokerState, kafkaMetricsTime, metrics, threadNamePrefix)</div><div class=\"line\">        kafkaController.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start group coordinator */</span></div><div class=\"line\">        groupCoordinator = <span class=\"type\">GroupCoordinator</span>(config, zkUtils, replicaManager, kafkaMetricsTime)</div><div class=\"line\">        groupCoordinator.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* Get the authorizer and initialize it if one is specified.*/</span></div><div class=\"line\">        authorizer = <span class=\"type\">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class=\"line\">          <span class=\"keyword\">val</span> authZ = <span class=\"type\">CoreUtils</span>.createObject[<span class=\"type\">Authorizer</span>](authorizerClassName)</div><div class=\"line\">          authZ.configure(config.originals())</div><div class=\"line\">          authZ</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start processing requests */</span></div><div class=\"line\">        apis = <span class=\"keyword\">new</span> <span class=\"type\">KafkaApis</span>(socketServer.requestChannel, replicaManager, groupCoordinator,</div><div class=\"line\">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer)</div><div class=\"line\">        requestHandlerPool = <span class=\"keyword\">new</span> <span class=\"type\">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads)</div><div class=\"line\">        brokerState.newState(<span class=\"type\">RunningAsBroker</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>首先是初始化Metrics注册信息。</li>\n<li>接着把当前broker的状态先置为Starting。</li>\n<li>启动kafkaScheduler，其内部维护了一个ScheduledThreadPoolExecutor，用于执行broker内置的一些周期性运行的job或定时job。比如，启动自动提交时，broker会定期维护客户端的消费topic-partition的offset信息。</li>\n<li>初始化zookeeper访问工具，建立必要的数据路径。</li>\n<li>启动LogManager，也就是日志数据管理子系统，负责日志数据的创建、截断、滚动、和清理等。</li>\n<li>启动SocketServer，一个基于NIO的socker服务端，其线程模型是有一个acceptor线程来接受客户端的连接，对应这个acceptor有N个processor线程，每个processor有自己的selector来从sockets读取收到的请求。另外，有M个handler线程专门处理请求并把处理结果返回给processor线程并通过socket写回给客户端。</li>\n<li>启动ReplicaManager，也即副本管理器，用于管理每个topic-partition的副本状态，包括主从、ISR列表等。</li>\n<li>启动KafkaController，可以理解为kafka集群的中央控制器，负责全局的协调，比如选取leader，reassignment等，其自身也支持动态选举高可用。</li>\n<li>启动GroupCoordinator，主要用于broker组管理和offset管理。</li>\n<li>初始化授权认证管理器，用户可以自己通过参数authorizer.class.name指定具体的Authorizer实现。kafka自带有SimpleAclAuthorizer的简单实现。</li>\n<li>初始化KafkaApis，用于统一接收外部请求。</li>\n<li>初始化KafkaRequestHandlerPool，内部是一个线程池，用于具体处理外部请求。</li>\n<li>将当前broker的状态置为RunningAsBroker，这时，broker已经可以对外提供服务了。</li>\n</ol>\n<p>第二部分主要是辅助模块的启动，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"type\">Mx4jLoader</span>.maybeLoad()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* start dynamic config manager */</span></div><div class=\"line\">dynamicConfigHandlers = <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">ConfigHandler</span>](<span class=\"type\">ConfigType</span>.<span class=\"type\">Topic</span> -&gt; <span class=\"keyword\">new</span> <span class=\"type\">TopicConfigHandler</span>(logManager, config),</div><div class=\"line\">                                                   <span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span> -&gt; <span class=\"keyword\">new</span> <span class=\"type\">ClientIdConfigHandler</span>(apis.quotaManagers))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Apply all existing client configs to the ClientIdConfigHandler to bootstrap the overrides</span></div><div class=\"line\"><span class=\"comment\">// <span class=\"doctag\">TODO:</span> Move this logic to DynamicConfigManager</span></div><div class=\"line\"><span class=\"type\">AdminUtils</span>.fetchAllEntityConfigs(zkUtils, <span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span>).foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> (clientId, properties) =&gt; dynamicConfigHandlers(<span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span>).processConfigChanges(clientId, properties)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Create the config manager. start listening to notifications</span></div><div class=\"line\">dynamicConfigManager = <span class=\"keyword\">new</span> <span class=\"type\">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class=\"line\">dynamicConfigManager.startup()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* tell everyone we are alive */</span></div><div class=\"line\"><span class=\"keyword\">val</span> listeners = config.advertisedListeners.map &#123;<span class=\"keyword\">case</span>(protocol, endpoint) =&gt;</div><div class=\"line\">  <span class=\"keyword\">if</span> (endpoint.port == <span class=\"number\">0</span>)</div><div class=\"line\">    (protocol, <span class=\"type\">EndPoint</span>(endpoint.host, socketServer.boundPort(protocol), endpoint.protocolType))</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">    (protocol, endpoint)</div><div class=\"line\">&#125;</div><div class=\"line\">kafkaHealthcheck = <span class=\"keyword\">new</span> <span class=\"type\">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class=\"line\">  config.interBrokerProtocolVersion)</div><div class=\"line\">kafkaHealthcheck.startup()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class=\"line\">checkpointBrokerId(config.brokerId)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* register broker metrics */</span></div><div class=\"line\">registerStats()</div><div class=\"line\"></div><div class=\"line\">shutdownLatch = <span class=\"keyword\">new</span> <span class=\"type\">CountDownLatch</span>(<span class=\"number\">1</span>)</div><div class=\"line\">startupComplete.set(<span class=\"literal\">true</span>)</div><div class=\"line\">isStartingUp.set(<span class=\"literal\">false</span>)</div><div class=\"line\"><span class=\"type\">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</div><div class=\"line\">info(<span class=\"string\">\"started\"</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>启动jmx，通过参数kafka_mx4jenable控制是否启用jmx，默认为false。</li>\n<li>初始化TopicConfigHandler和ClientIdConfigHandler，前者用于处理zk上的topic配置变更信息，后者用于zk上的clientId配置变更信息。</li>\n<li>启动DynamicConfigManager，通过动态配置管理器，监听zk上的配置节点变化，并根据具体变化的配置信息调用TopicConfigHandler或ClientIdConfigHandler更新配置。</li>\n<li>启动KafkaHealthcheck，用于在zk上注册当前broker节点信息，以便节点退出时其他broker和consumer能监听到，目前的节点健康度判断比较简单，只是单纯的看zk上的节点是否存在。</li>\n<li>最后，在本地对当前broker做个checkpoint，并注册jmx bean信息</li>\n</ol>\n","excerpt":"<p>kafka-0.10.0是官方出的最新稳定版本，提供了大量新的feature，具体可见<a href=\"http://www.iteblog.com/archives/1677\">这里</a>，本文主要分析kafka-0.10-0的源码结构和启动过程。</p>\n<h2 id=\"源码结构\"><a href=\"#源码结构\" class=\"headerlink\" title=\"源码结构\"></a>源码结构</h2><p>kafka-0.10.0的源码可以从github上fork一份，在源码目录下执行./gradlew idea生成idea项目，然后导入idea即可。这中间需要使用gradle进行依赖包的下载，导入后可以看到其源码结构如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160708-kafka0.10.0%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/1.png\" alt=\"\"></p>\n<p>包括几大重要模块：</p>\n<ul>\n<li>clients主要是kafka-client相关的代码，包括consumer、producer，还包括一些公共逻辑，如授权认证、序列化等。</li>\n<li>connect主要是kafka-connect模块的代码逻辑，Kafka connect是0.9版本增加的特性,支持创建和管理数据流管道。通过它可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统，比如数据库、elastic search等。</li>\n<li>core模块是kafka的核心部分，主要包括broker的实现逻辑、producer和consumer的javaapi等。</li>\n<li>streams模块主要是kafka-streaming的实现，提供了一整套描述常见流操作的高级语言API（比如 joining, filtering以及aggregation等），我们可以基于此开发流处理应用程序。</li>\n</ul>","more":"<h2 id=\"启动入口\"><a href=\"#启动入口\" class=\"headerlink\" title=\"启动入口\"></a>启动入口</h2><p>kafka的启动入口在core_main这个module下，入口函数如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> serverProps = getPropsFromArgs(args)</div><div class=\"line\">      <span class=\"keyword\">val</span> kafkaServerStartable = <span class=\"type\">KafkaServerStartable</span>.fromProps(serverProps)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// attach shutdown handler to catch control-c</span></div><div class=\"line\">      <span class=\"type\">Runtime</span>.getRuntime().addShutdownHook(<span class=\"keyword\">new</span> <span class=\"type\">Thread</span>() &#123;</div><div class=\"line\">        <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>() = &#123;</div><div class=\"line\">          kafkaServerStartable.shutdown</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;)</div><div class=\"line\"></div><div class=\"line\">      kafkaServerStartable.startup</div><div class=\"line\">      kafkaServerStartable.awaitShutdown</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</div><div class=\"line\">        fatal(e)</div><div class=\"line\">        <span class=\"type\">System</span>.exit(<span class=\"number\">1</span>)</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"type\">System</span>.exit(<span class=\"number\">0</span>)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<p>先从命令行指定的配置文件加载配置，然后通过KafkaServerStartable类启动broker，实际上在KafkaServerStartable中维护了一个KafkaServer对象，它通过调用KafkaServer的startup方法启动broker。</p>\n<h2 id=\"broker启动过程\"><a href=\"#broker启动过程\" class=\"headerlink\" title=\"broker启动过程\"></a>broker启动过程</h2><p>下面并启动过程代码按启动顺序分两部分做说明。</p>\n<p>第一部分主要是核心模块的启动，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">metrics = <span class=\"keyword\">new</span> <span class=\"type\">Metrics</span>(metricConfig, reporters, kafkaMetricsTime, <span class=\"literal\">true</span>)</div><div class=\"line\"></div><div class=\"line\">        brokerState.newState(<span class=\"type\">Starting</span>)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start scheduler */</span></div><div class=\"line\">        kafkaScheduler.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* setup zookeeper */</span></div><div class=\"line\">        zkUtils = initZk()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start log manager */</span></div><div class=\"line\">        logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class=\"line\">        logManager.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* generate brokerId */</span></div><div class=\"line\">        config.brokerId =  getBrokerId</div><div class=\"line\">        <span class=\"keyword\">this</span>.logIdent = <span class=\"string\">\"[Kafka Server \"</span> + config.brokerId + <span class=\"string\">\"], \"</span></div><div class=\"line\"></div><div class=\"line\">        socketServer = <span class=\"keyword\">new</span> <span class=\"type\">SocketServer</span>(config, metrics, kafkaMetricsTime)</div><div class=\"line\">        socketServer.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start replica manager */</span></div><div class=\"line\">        replicaManager = <span class=\"keyword\">new</span> <span class=\"type\">ReplicaManager</span>(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager,</div><div class=\"line\">          isShuttingDown)</div><div class=\"line\">        replicaManager.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start kafka controller */</span></div><div class=\"line\">        kafkaController = <span class=\"keyword\">new</span> <span class=\"type\">KafkaController</span>(config, zkUtils, brokerState, kafkaMetricsTime, metrics, threadNamePrefix)</div><div class=\"line\">        kafkaController.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start group coordinator */</span></div><div class=\"line\">        groupCoordinator = <span class=\"type\">GroupCoordinator</span>(config, zkUtils, replicaManager, kafkaMetricsTime)</div><div class=\"line\">        groupCoordinator.startup()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* Get the authorizer and initialize it if one is specified.*/</span></div><div class=\"line\">        authorizer = <span class=\"type\">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class=\"line\">          <span class=\"keyword\">val</span> authZ = <span class=\"type\">CoreUtils</span>.createObject[<span class=\"type\">Authorizer</span>](authorizerClassName)</div><div class=\"line\">          authZ.configure(config.originals())</div><div class=\"line\">          authZ</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/* start processing requests */</span></div><div class=\"line\">        apis = <span class=\"keyword\">new</span> <span class=\"type\">KafkaApis</span>(socketServer.requestChannel, replicaManager, groupCoordinator,</div><div class=\"line\">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer)</div><div class=\"line\">        requestHandlerPool = <span class=\"keyword\">new</span> <span class=\"type\">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads)</div><div class=\"line\">        brokerState.newState(<span class=\"type\">RunningAsBroker</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>首先是初始化Metrics注册信息。</li>\n<li>接着把当前broker的状态先置为Starting。</li>\n<li>启动kafkaScheduler，其内部维护了一个ScheduledThreadPoolExecutor，用于执行broker内置的一些周期性运行的job或定时job。比如，启动自动提交时，broker会定期维护客户端的消费topic-partition的offset信息。</li>\n<li>初始化zookeeper访问工具，建立必要的数据路径。</li>\n<li>启动LogManager，也就是日志数据管理子系统，负责日志数据的创建、截断、滚动、和清理等。</li>\n<li>启动SocketServer，一个基于NIO的socker服务端，其线程模型是有一个acceptor线程来接受客户端的连接，对应这个acceptor有N个processor线程，每个processor有自己的selector来从sockets读取收到的请求。另外，有M个handler线程专门处理请求并把处理结果返回给processor线程并通过socket写回给客户端。</li>\n<li>启动ReplicaManager，也即副本管理器，用于管理每个topic-partition的副本状态，包括主从、ISR列表等。</li>\n<li>启动KafkaController，可以理解为kafka集群的中央控制器，负责全局的协调，比如选取leader，reassignment等，其自身也支持动态选举高可用。</li>\n<li>启动GroupCoordinator，主要用于broker组管理和offset管理。</li>\n<li>初始化授权认证管理器，用户可以自己通过参数authorizer.class.name指定具体的Authorizer实现。kafka自带有SimpleAclAuthorizer的简单实现。</li>\n<li>初始化KafkaApis，用于统一接收外部请求。</li>\n<li>初始化KafkaRequestHandlerPool，内部是一个线程池，用于具体处理外部请求。</li>\n<li>将当前broker的状态置为RunningAsBroker，这时，broker已经可以对外提供服务了。</li>\n</ol>\n<p>第二部分主要是辅助模块的启动，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"type\">Mx4jLoader</span>.maybeLoad()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* start dynamic config manager */</span></div><div class=\"line\">dynamicConfigHandlers = <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">ConfigHandler</span>](<span class=\"type\">ConfigType</span>.<span class=\"type\">Topic</span> -&gt; <span class=\"keyword\">new</span> <span class=\"type\">TopicConfigHandler</span>(logManager, config),</div><div class=\"line\">                                                   <span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span> -&gt; <span class=\"keyword\">new</span> <span class=\"type\">ClientIdConfigHandler</span>(apis.quotaManagers))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Apply all existing client configs to the ClientIdConfigHandler to bootstrap the overrides</span></div><div class=\"line\"><span class=\"comment\">// <span class=\"doctag\">TODO:</span> Move this logic to DynamicConfigManager</span></div><div class=\"line\"><span class=\"type\">AdminUtils</span>.fetchAllEntityConfigs(zkUtils, <span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span>).foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> (clientId, properties) =&gt; dynamicConfigHandlers(<span class=\"type\">ConfigType</span>.<span class=\"type\">Client</span>).processConfigChanges(clientId, properties)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Create the config manager. start listening to notifications</span></div><div class=\"line\">dynamicConfigManager = <span class=\"keyword\">new</span> <span class=\"type\">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class=\"line\">dynamicConfigManager.startup()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* tell everyone we are alive */</span></div><div class=\"line\"><span class=\"keyword\">val</span> listeners = config.advertisedListeners.map &#123;<span class=\"keyword\">case</span>(protocol, endpoint) =&gt;</div><div class=\"line\">  <span class=\"keyword\">if</span> (endpoint.port == <span class=\"number\">0</span>)</div><div class=\"line\">    (protocol, <span class=\"type\">EndPoint</span>(endpoint.host, socketServer.boundPort(protocol), endpoint.protocolType))</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">    (protocol, endpoint)</div><div class=\"line\">&#125;</div><div class=\"line\">kafkaHealthcheck = <span class=\"keyword\">new</span> <span class=\"type\">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class=\"line\">  config.interBrokerProtocolVersion)</div><div class=\"line\">kafkaHealthcheck.startup()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class=\"line\">checkpointBrokerId(config.brokerId)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/* register broker metrics */</span></div><div class=\"line\">registerStats()</div><div class=\"line\"></div><div class=\"line\">shutdownLatch = <span class=\"keyword\">new</span> <span class=\"type\">CountDownLatch</span>(<span class=\"number\">1</span>)</div><div class=\"line\">startupComplete.set(<span class=\"literal\">true</span>)</div><div class=\"line\">isStartingUp.set(<span class=\"literal\">false</span>)</div><div class=\"line\"><span class=\"type\">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</div><div class=\"line\">info(<span class=\"string\">\"started\"</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>启动jmx，通过参数kafka_mx4jenable控制是否启用jmx，默认为false。</li>\n<li>初始化TopicConfigHandler和ClientIdConfigHandler，前者用于处理zk上的topic配置变更信息，后者用于zk上的clientId配置变更信息。</li>\n<li>启动DynamicConfigManager，通过动态配置管理器，监听zk上的配置节点变化，并根据具体变化的配置信息调用TopicConfigHandler或ClientIdConfigHandler更新配置。</li>\n<li>启动KafkaHealthcheck，用于在zk上注册当前broker节点信息，以便节点退出时其他broker和consumer能监听到，目前的节点健康度判断比较简单，只是单纯的看zk上的节点是否存在。</li>\n<li>最后，在本地对当前broker做个checkpoint，并注册jmx bean信息</li>\n</ol>"},{"title":"mac系统下hadoop-2.7源码编译、导入eclipse及打包","date":"2015-05-18T02:00:44.000Z","_content":"\n编译环境要求\n--\n\n> JDK1.7+  \n> MAVEN 3.0以上版本  \n> 如果需要编译native code，还需要CMake 2.6、Zlib devel、openssl devel（mac下一般安装了xcode后应该都会有这些包）。\n\n编译方法\n--\n\n解压源码包hadoop-2.7.0-src.tar.gz，iterm下进入文件夹hadoop-2.7.0-src，然后根据需要执行相应的mvn命令就可以了。\n\n> 仅编译：mvn compile  \n> 打包生成jar：mvn package  \n> 生成eclipse项目：eclipse:eclipse -DskipTests，加上-DskipTests可跳过test阶段。\n\n期间遇到几个问题，记录如下。\n\n问题记录\n--\n\n先执行mvn eclipse:eclipse -DskipTest生成eclipse项目，执行到一半时，提示下面的报错：\n\n<font color='red'>‘protoc –version’ did not return a version -> [Help 1]</font>\n\n意思也就是是找不到protoc命令，安装protocolbuffer后重试，又提示错误：\n\n<font color='red'>protoc version is ‘libprotoc 2.6.1′, expected version is ’2.5.0′</font>\n\n看上去是protocolbuffer版本问题，hadoop需要的版本是2.5.0，而系统安装的是2.6.1，查了很多资料，都是说protocolbuffer版本太低后来升到2.5的，而我这是2.6.1的版本，难不成还得降回去，不至于吧。因此，猜测这个版本限制是在pom.xml中写死的，于是grep了一下，发现果然在hadoop-project/pom.xml中配置了编译时使用的pb版本。\n\n<font color='red'>\\<protobuf.version>2.5.0\\</protobuf.version></font>\n\n把以上配置项改为2.6.1，再重新执行生成eclipse项目的命令就OK了。\n\n导入eclipse及打包\n--\n\n生成eclipse项目后，从eclipse里import existing project into workspace，选择hadoop-2.7.0-src目录，就会把所有代码模块导入eclipse了。接下来就可以看代码并修改了，比如增加一些日志信息等。\n\n代码修改完毕后，可以再打出一个新的hadoop-distribution包来验证代码修改效果。\n\n在hadoop-2.7.0-src目录下执行命令：<font color='red'>mvn package -Pdist -Ptar -Pdocs -skipTests </font>\n\n等上漫长的一段时间，编译成功后，可以到hadoop-dist/target下找到新的jar包。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150518-hadoop%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85/1.png)\n\n补充说明\n--\n\n编译过程需要从maven中央仓库下载大量依赖包，我使用的是oschina的库。\n\n```java\n    <mirror>\n      <id>CN</id>\n      <name>OSChina Central</name>\n      <url>http://maven.oschina.net/content/groups/public/</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n      <profile>\n    \t<id>oschina</id>\n    \t<repositories>\n    \t\t<repository>\n    \t\t\t<id>nexus</id>\n    \t\t\t<name>local private nexus</name>\n    \t\t\t<url>http://maven.oschina.net/content/groups/public/</url>\n    \t\t\t<releases>\n    \t\t\t\t<enabled>true</enabled>\n    \t\t\t</releases>\n    \t\t\t<snapshots>\n    \t\t\t\t<enabled>false</enabled>\n    \t\t\t</snapshots>\n    \t\t</repository>\n    \t</repositories>\n    \t<pluginRepositories>\n    \t\t<pluginRepository>\n    \t\t\t<id>nexus</id>\n    \t\t\t<name>local private nexus</name>\n    \t\t\t<url>http://maven.oschina.net/content/groups/public/</url>\n    \t\t\t<releases>\n    \t\t\t\t<enabled>true</enabled>\n    \t\t\t</releases>\n    \t\t\t<snapshots>\n    \t\t\t\t<enabled>false</enabled>\n    \t\t\t</snapshots>\n    \t\t</pluginRepository>\n    \t</pluginRepositories>\n      </profile>\n```","source":"_posts/mac系统下hadoop-2-7源码编译、导入eclipse及打包.md","raw":"---\ntitle: mac系统下hadoop-2.7源码编译、导入eclipse及打包\ndate: 2015-05-18 10:00:44\ntags:\n- hadoop\n- eclipse\ncategories:\n- Hadoop\n---\n\n编译环境要求\n--\n\n> JDK1.7+  \n> MAVEN 3.0以上版本  \n> 如果需要编译native code，还需要CMake 2.6、Zlib devel、openssl devel（mac下一般安装了xcode后应该都会有这些包）。\n\n编译方法\n--\n\n解压源码包hadoop-2.7.0-src.tar.gz，iterm下进入文件夹hadoop-2.7.0-src，然后根据需要执行相应的mvn命令就可以了。\n\n> 仅编译：mvn compile  \n> 打包生成jar：mvn package  \n> 生成eclipse项目：eclipse:eclipse -DskipTests，加上-DskipTests可跳过test阶段。\n\n期间遇到几个问题，记录如下。\n\n问题记录\n--\n\n先执行mvn eclipse:eclipse -DskipTest生成eclipse项目，执行到一半时，提示下面的报错：\n\n<font color='red'>‘protoc –version’ did not return a version -> [Help 1]</font>\n\n意思也就是是找不到protoc命令，安装protocolbuffer后重试，又提示错误：\n\n<font color='red'>protoc version is ‘libprotoc 2.6.1′, expected version is ’2.5.0′</font>\n\n看上去是protocolbuffer版本问题，hadoop需要的版本是2.5.0，而系统安装的是2.6.1，查了很多资料，都是说protocolbuffer版本太低后来升到2.5的，而我这是2.6.1的版本，难不成还得降回去，不至于吧。因此，猜测这个版本限制是在pom.xml中写死的，于是grep了一下，发现果然在hadoop-project/pom.xml中配置了编译时使用的pb版本。\n\n<font color='red'>\\<protobuf.version>2.5.0\\</protobuf.version></font>\n\n把以上配置项改为2.6.1，再重新执行生成eclipse项目的命令就OK了。\n\n导入eclipse及打包\n--\n\n生成eclipse项目后，从eclipse里import existing project into workspace，选择hadoop-2.7.0-src目录，就会把所有代码模块导入eclipse了。接下来就可以看代码并修改了，比如增加一些日志信息等。\n\n代码修改完毕后，可以再打出一个新的hadoop-distribution包来验证代码修改效果。\n\n在hadoop-2.7.0-src目录下执行命令：<font color='red'>mvn package -Pdist -Ptar -Pdocs -skipTests </font>\n\n等上漫长的一段时间，编译成功后，可以到hadoop-dist/target下找到新的jar包。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150518-hadoop%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85/1.png)\n\n补充说明\n--\n\n编译过程需要从maven中央仓库下载大量依赖包，我使用的是oschina的库。\n\n```java\n    <mirror>\n      <id>CN</id>\n      <name>OSChina Central</name>\n      <url>http://maven.oschina.net/content/groups/public/</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n      <profile>\n    \t<id>oschina</id>\n    \t<repositories>\n    \t\t<repository>\n    \t\t\t<id>nexus</id>\n    \t\t\t<name>local private nexus</name>\n    \t\t\t<url>http://maven.oschina.net/content/groups/public/</url>\n    \t\t\t<releases>\n    \t\t\t\t<enabled>true</enabled>\n    \t\t\t</releases>\n    \t\t\t<snapshots>\n    \t\t\t\t<enabled>false</enabled>\n    \t\t\t</snapshots>\n    \t\t</repository>\n    \t</repositories>\n    \t<pluginRepositories>\n    \t\t<pluginRepository>\n    \t\t\t<id>nexus</id>\n    \t\t\t<name>local private nexus</name>\n    \t\t\t<url>http://maven.oschina.net/content/groups/public/</url>\n    \t\t\t<releases>\n    \t\t\t\t<enabled>true</enabled>\n    \t\t\t</releases>\n    \t\t\t<snapshots>\n    \t\t\t\t<enabled>false</enabled>\n    \t\t\t</snapshots>\n    \t\t</pluginRepository>\n    \t</pluginRepositories>\n      </profile>\n```","slug":"mac系统下hadoop-2-7源码编译、导入eclipse及打包","published":1,"updated":"2017-01-18T09:54:11.734Z","_id":"ciy320ait0009ifs6vrz84mzo","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"编译环境要求\"><a href=\"#编译环境要求\" class=\"headerlink\" title=\"编译环境要求\"></a>编译环境要求</h2><blockquote>\n<p>JDK1.7+<br>MAVEN 3.0以上版本<br>如果需要编译native code，还需要CMake 2.6、Zlib devel、openssl devel（mac下一般安装了xcode后应该都会有这些包）。</p>\n</blockquote>\n<h2 id=\"编译方法\"><a href=\"#编译方法\" class=\"headerlink\" title=\"编译方法\"></a>编译方法</h2><p>解压源码包hadoop-2.7.0-src.tar.gz，iterm下进入文件夹hadoop-2.7.0-src，然后根据需要执行相应的mvn命令就可以了。</p>\n<blockquote>\n<p>仅编译：mvn compile<br>打包生成jar：mvn package<br>生成eclipse项目：eclipse:eclipse -DskipTests，加上-DskipTests可跳过test阶段。</p>\n</blockquote>\n<p>期间遇到几个问题，记录如下。</p>\n<h2 id=\"问题记录\"><a href=\"#问题记录\" class=\"headerlink\" title=\"问题记录\"></a>问题记录</h2><p>先执行mvn eclipse:eclipse -DskipTest生成eclipse项目，执行到一半时，提示下面的报错：</p>\n<font color=\"red\">‘protoc –version’ did not return a version -&gt; [Help 1]</font>\n\n<p>意思也就是是找不到protoc命令，安装protocolbuffer后重试，又提示错误：</p>\n<font color=\"red\">protoc version is ‘libprotoc 2.6.1′, expected version is ’2.5.0′</font>\n\n<p>看上去是protocolbuffer版本问题，hadoop需要的版本是2.5.0，而系统安装的是2.6.1，查了很多资料，都是说protocolbuffer版本太低后来升到2.5的，而我这是2.6.1的版本，难不成还得降回去，不至于吧。因此，猜测这个版本限制是在pom.xml中写死的，于是grep了一下，发现果然在hadoop-project/pom.xml中配置了编译时使用的pb版本。</p>\n<font color=\"red\">\\<protobuf.version>2.5.0\\</protobuf.version></font>\n\n<p>把以上配置项改为2.6.1，再重新执行生成eclipse项目的命令就OK了。</p>\n<h2 id=\"导入eclipse及打包\"><a href=\"#导入eclipse及打包\" class=\"headerlink\" title=\"导入eclipse及打包\"></a>导入eclipse及打包</h2><p>生成eclipse项目后，从eclipse里import existing project into workspace，选择hadoop-2.7.0-src目录，就会把所有代码模块导入eclipse了。接下来就可以看代码并修改了，比如增加一些日志信息等。</p>\n<p>代码修改完毕后，可以再打出一个新的hadoop-distribution包来验证代码修改效果。</p>\n<p>在hadoop-2.7.0-src目录下执行命令：<font color=\"red\">mvn package -Pdist -Ptar -Pdocs -skipTests </font></p>\n<p>等上漫长的一段时间，编译成功后，可以到hadoop-dist/target下找到新的jar包。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150518-hadoop%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85/1.png\" alt=\"\"></p>\n<h2 id=\"补充说明\"><a href=\"#补充说明\" class=\"headerlink\" title=\"补充说明\"></a>补充说明</h2><p>编译过程需要从maven中央仓库下载大量依赖包，我使用的是oschina的库。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;mirror&gt;</div><div class=\"line\">  &lt;id&gt;CN&lt;/id&gt;</div><div class=\"line\">  &lt;name&gt;OSChina Central&lt;/name&gt;</div><div class=\"line\">  &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</div><div class=\"line\">&lt;/mirror&gt;</div><div class=\"line\">  &lt;profile&gt;</div><div class=\"line\">\t&lt;id&gt;oschina&lt;/id&gt;</div><div class=\"line\">\t&lt;repositories&gt;</div><div class=\"line\">\t\t&lt;repository&gt;</div><div class=\"line\">\t\t\t&lt;id&gt;nexus&lt;/id&gt;</div><div class=\"line\">\t\t\t&lt;name&gt;local private nexus&lt;/name&gt;</div><div class=\"line\">\t\t\t&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">\t\t\t&lt;releases&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/releases&gt;</div><div class=\"line\">\t\t\t&lt;snapshots&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/snapshots&gt;</div><div class=\"line\">\t\t&lt;/repository&gt;</div><div class=\"line\">\t&lt;/repositories&gt;</div><div class=\"line\">\t&lt;pluginRepositories&gt;</div><div class=\"line\">\t\t&lt;pluginRepository&gt;</div><div class=\"line\">\t\t\t&lt;id&gt;nexus&lt;/id&gt;</div><div class=\"line\">\t\t\t&lt;name&gt;local private nexus&lt;/name&gt;</div><div class=\"line\">\t\t\t&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">\t\t\t&lt;releases&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/releases&gt;</div><div class=\"line\">\t\t\t&lt;snapshots&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/snapshots&gt;</div><div class=\"line\">\t\t&lt;/pluginRepository&gt;</div><div class=\"line\">\t&lt;/pluginRepositories&gt;</div><div class=\"line\">  &lt;/profile&gt;</div></pre></td></tr></table></figure>","excerpt":"","more":"<h2 id=\"编译环境要求\"><a href=\"#编译环境要求\" class=\"headerlink\" title=\"编译环境要求\"></a>编译环境要求</h2><blockquote>\n<p>JDK1.7+<br>MAVEN 3.0以上版本<br>如果需要编译native code，还需要CMake 2.6、Zlib devel、openssl devel（mac下一般安装了xcode后应该都会有这些包）。</p>\n</blockquote>\n<h2 id=\"编译方法\"><a href=\"#编译方法\" class=\"headerlink\" title=\"编译方法\"></a>编译方法</h2><p>解压源码包hadoop-2.7.0-src.tar.gz，iterm下进入文件夹hadoop-2.7.0-src，然后根据需要执行相应的mvn命令就可以了。</p>\n<blockquote>\n<p>仅编译：mvn compile<br>打包生成jar：mvn package<br>生成eclipse项目：eclipse:eclipse -DskipTests，加上-DskipTests可跳过test阶段。</p>\n</blockquote>\n<p>期间遇到几个问题，记录如下。</p>\n<h2 id=\"问题记录\"><a href=\"#问题记录\" class=\"headerlink\" title=\"问题记录\"></a>问题记录</h2><p>先执行mvn eclipse:eclipse -DskipTest生成eclipse项目，执行到一半时，提示下面的报错：</p>\n<font color='red'>‘protoc –version’ did not return a version -&gt; [Help 1]</font>\n\n<p>意思也就是是找不到protoc命令，安装protocolbuffer后重试，又提示错误：</p>\n<font color='red'>protoc version is ‘libprotoc 2.6.1′, expected version is ’2.5.0′</font>\n\n<p>看上去是protocolbuffer版本问题，hadoop需要的版本是2.5.0，而系统安装的是2.6.1，查了很多资料，都是说protocolbuffer版本太低后来升到2.5的，而我这是2.6.1的版本，难不成还得降回去，不至于吧。因此，猜测这个版本限制是在pom.xml中写死的，于是grep了一下，发现果然在hadoop-project/pom.xml中配置了编译时使用的pb版本。</p>\n<font color='red'>\\<protobuf.version>2.5.0\\</protobuf.version></font>\n\n<p>把以上配置项改为2.6.1，再重新执行生成eclipse项目的命令就OK了。</p>\n<h2 id=\"导入eclipse及打包\"><a href=\"#导入eclipse及打包\" class=\"headerlink\" title=\"导入eclipse及打包\"></a>导入eclipse及打包</h2><p>生成eclipse项目后，从eclipse里import existing project into workspace，选择hadoop-2.7.0-src目录，就会把所有代码模块导入eclipse了。接下来就可以看代码并修改了，比如增加一些日志信息等。</p>\n<p>代码修改完毕后，可以再打出一个新的hadoop-distribution包来验证代码修改效果。</p>\n<p>在hadoop-2.7.0-src目录下执行命令：<font color='red'>mvn package -Pdist -Ptar -Pdocs -skipTests </font></p>\n<p>等上漫长的一段时间，编译成功后，可以到hadoop-dist/target下找到新的jar包。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150518-hadoop%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85/1.png\" alt=\"\"></p>\n<h2 id=\"补充说明\"><a href=\"#补充说明\" class=\"headerlink\" title=\"补充说明\"></a>补充说明</h2><p>编译过程需要从maven中央仓库下载大量依赖包，我使用的是oschina的库。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;mirror&gt;</div><div class=\"line\">  &lt;id&gt;CN&lt;/id&gt;</div><div class=\"line\">  &lt;name&gt;OSChina Central&lt;/name&gt;</div><div class=\"line\">  &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</div><div class=\"line\">&lt;/mirror&gt;</div><div class=\"line\">  &lt;profile&gt;</div><div class=\"line\">\t&lt;id&gt;oschina&lt;/id&gt;</div><div class=\"line\">\t&lt;repositories&gt;</div><div class=\"line\">\t\t&lt;repository&gt;</div><div class=\"line\">\t\t\t&lt;id&gt;nexus&lt;/id&gt;</div><div class=\"line\">\t\t\t&lt;name&gt;local private nexus&lt;/name&gt;</div><div class=\"line\">\t\t\t&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">\t\t\t&lt;releases&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/releases&gt;</div><div class=\"line\">\t\t\t&lt;snapshots&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/snapshots&gt;</div><div class=\"line\">\t\t&lt;/repository&gt;</div><div class=\"line\">\t&lt;/repositories&gt;</div><div class=\"line\">\t&lt;pluginRepositories&gt;</div><div class=\"line\">\t\t&lt;pluginRepository&gt;</div><div class=\"line\">\t\t\t&lt;id&gt;nexus&lt;/id&gt;</div><div class=\"line\">\t\t\t&lt;name&gt;local private nexus&lt;/name&gt;</div><div class=\"line\">\t\t\t&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</div><div class=\"line\">\t\t\t&lt;releases&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/releases&gt;</div><div class=\"line\">\t\t\t&lt;snapshots&gt;</div><div class=\"line\">\t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt;</div><div class=\"line\">\t\t\t&lt;/snapshots&gt;</div><div class=\"line\">\t\t&lt;/pluginRepository&gt;</div><div class=\"line\">\t&lt;/pluginRepositories&gt;</div><div class=\"line\">  &lt;/profile&gt;</div></pre></td></tr></table></figure>"},{"title":"Shell中的IFS分隔符使用","date":"2013-01-29T12:41:54.000Z","_content":"\n在linux中，shell把每个 $IFS 字符对待成一个分隔符，且基于这些字符把其他扩展的结果分割。\n  \n工作中需要处理一个文件datafile，文件中有好几列，列与列之间以‘\\3′分割，如下(终端下’\\3′显示为方块)：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell.png)\n\n我需要拿到文件中<font color='blue'>第三列为1</font>的数据行再做具体的处理，比如取其中的某一列数据再去其他文件grep数据等等。简单点，直接逐行cat数据吧。 \n \n**脚本如下：**\n\n```c\nfor line in `awk -F\"\\3\" '{if($3==1) print $0}' datafile`\n    do\n        echo $line\ndone\n```\n\n**结果如下：**  \n\n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell2.png) \n\n本来是想要逐行打印出来的，可结果却不是我想要的，究其原因，是因为在shell的for循环中，列出集合的item时，默认是以<space>或<tab>或<newline>为分隔符，我们的数据文件中有空格，因此它就以空格分割打印了。\n  \n可以通过显式设置IFS的值来达到我们要的效果，修改后的脚本如下：  \n\n```c\noldifs=$IFS\nIFS=$'\\n'    #change seperator to '\\n' to get a line\nfor line in `awk -F\"\\3\" '{if($3==1) print $0}' datafile`\n    do\n        echo $line\ndone\nIFS=$oldifs #reset seperator\n```\n\n通过先保存当前的IFS变量的值到一个临时变量，再显式设置为我们想要的行分隔符$’\\n’，然后在for循环结束后，再重置IFS的值即可。  \n\n**结果如下：**  \n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell3.png)  \n","source":"_posts/shell中的IFS分隔符.md","raw":"---\ntitle: Shell中的IFS分隔符使用\ndate: 2013-01-29 20:41:54\ntags: \n- shell\ncategories: \n- Shell\n---\n\n在linux中，shell把每个 $IFS 字符对待成一个分隔符，且基于这些字符把其他扩展的结果分割。\n  \n工作中需要处理一个文件datafile，文件中有好几列，列与列之间以‘\\3′分割，如下(终端下’\\3′显示为方块)：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell.png)\n\n我需要拿到文件中<font color='blue'>第三列为1</font>的数据行再做具体的处理，比如取其中的某一列数据再去其他文件grep数据等等。简单点，直接逐行cat数据吧。 \n \n**脚本如下：**\n\n```c\nfor line in `awk -F\"\\3\" '{if($3==1) print $0}' datafile`\n    do\n        echo $line\ndone\n```\n\n**结果如下：**  \n\n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell2.png) \n\n本来是想要逐行打印出来的，可结果却不是我想要的，究其原因，是因为在shell的for循环中，列出集合的item时，默认是以<space>或<tab>或<newline>为分隔符，我们的数据文件中有空格，因此它就以空格分割打印了。\n  \n可以通过显式设置IFS的值来达到我们要的效果，修改后的脚本如下：  \n\n```c\noldifs=$IFS\nIFS=$'\\n'    #change seperator to '\\n' to get a line\nfor line in `awk -F\"\\3\" '{if($3==1) print $0}' datafile`\n    do\n        echo $line\ndone\nIFS=$oldifs #reset seperator\n```\n\n通过先保存当前的IFS变量的值到一个临时变量，再显式设置为我们想要的行分隔符$’\\n’，然后在for循环结束后，再重置IFS的值即可。  \n\n**结果如下：**  \n![](https://raw.githubusercontent.com/maohong/picture/master/20130129/shell3.png)  \n","slug":"shell中的IFS分隔符","published":1,"updated":"2017-01-18T09:54:11.735Z","_id":"ciy320aiy000bifs65yltt3lk","comments":1,"layout":"post","photos":[],"link":"","content":"<p>在linux中，shell把每个 $IFS 字符对待成一个分隔符，且基于这些字符把其他扩展的结果分割。</p>\n<p>工作中需要处理一个文件datafile，文件中有好几列，列与列之间以‘\\3′分割，如下(终端下’\\3′显示为方块)：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell.png\" alt=\"\"></p>\n<p>我需要拿到文件中<font color=\"blue\">第三列为1</font>的数据行再做具体的处理，比如取其中的某一列数据再去其他文件grep数据等等。简单点，直接逐行cat数据吧。 </p>\n<p><strong>脚本如下：</strong></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">for line in `awk -F\"\\3\" '&#123;if($3==1) print $0&#125;' datafile`</div><div class=\"line\">    do</div><div class=\"line\">        echo $line</div><div class=\"line\">done</div></pre></td></tr></table></figure>\n<p><strong>结果如下：</strong>  </p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell2.png\" alt=\"\"> </p>\n<p>本来是想要逐行打印出来的，可结果却不是我想要的，究其原因，是因为在shell的for循环中，列出集合的item时，默认是以<space>或<tab>或<newline>为分隔符，我们的数据文件中有空格，因此它就以空格分割打印了。</newline></tab></space></p>\n<p>可以通过显式设置IFS的值来达到我们要的效果，修改后的脚本如下：  </p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">oldifs=$IFS</div><div class=\"line\">IFS=$'\\n'    #change seperator to '\\n' to get a line</div><div class=\"line\">for line in `awk -F\"\\3\" '&#123;if($3==1) print $0&#125;' datafile`</div><div class=\"line\">    do</div><div class=\"line\">        echo $line</div><div class=\"line\">done</div><div class=\"line\">IFS=$oldifs #reset seperator</div></pre></td></tr></table></figure>\n<p>通过先保存当前的IFS变量的值到一个临时变量，再显式设置为我们想要的行分隔符$’\\n’，然后在for循环结束后，再重置IFS的值即可。  </p>\n<p><strong>结果如下：</strong><br><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell3.png\" alt=\"\">  </p>\n","excerpt":"","more":"<p>在linux中，shell把每个 $IFS 字符对待成一个分隔符，且基于这些字符把其他扩展的结果分割。</p>\n<p>工作中需要处理一个文件datafile，文件中有好几列，列与列之间以‘\\3′分割，如下(终端下’\\3′显示为方块)：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell.png\" alt=\"\"></p>\n<p>我需要拿到文件中<font color='blue'>第三列为1</font>的数据行再做具体的处理，比如取其中的某一列数据再去其他文件grep数据等等。简单点，直接逐行cat数据吧。 </p>\n<p><strong>脚本如下：</strong></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">for line in `awk -F\"\\3\" '&#123;if($3==1) print $0&#125;' datafile`</div><div class=\"line\">    do</div><div class=\"line\">        echo $line</div><div class=\"line\">done</div></pre></td></tr></table></figure>\n<p><strong>结果如下：</strong>  </p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell2.png\" alt=\"\"> </p>\n<p>本来是想要逐行打印出来的，可结果却不是我想要的，究其原因，是因为在shell的for循环中，列出集合的item时，默认是以<space>或<tab>或<newline>为分隔符，我们的数据文件中有空格，因此它就以空格分割打印了。</p>\n<p>可以通过显式设置IFS的值来达到我们要的效果，修改后的脚本如下：  </p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">oldifs=$IFS</div><div class=\"line\">IFS=$'\\n'    #change seperator to '\\n' to get a line</div><div class=\"line\">for line in `awk -F\"\\3\" '&#123;if($3==1) print $0&#125;' datafile`</div><div class=\"line\">    do</div><div class=\"line\">        echo $line</div><div class=\"line\">done</div><div class=\"line\">IFS=$oldifs #reset seperator</div></pre></td></tr></table></figure>\n<p>通过先保存当前的IFS变量的值到一个临时变量，再显式设置为我们想要的行分隔符$’\\n’，然后在for循环结束后，再重置IFS的值即可。  </p>\n<p><strong>结果如下：</strong><br><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20130129/shell3.png\" alt=\"\">  </p>\n"},{"title":"storm集群supervisor节点异常退出问题排查","date":"2015-07-03T12:48:50.000Z","_content":"\n问题出现\n--\n\n测试storm集群为0.9.4版本，前段时间出现supervisor进程挂掉，而其上work进程仍然运行的诡异情况，通过日志看到supervisor进程挂掉之前出现以下异常：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/1.png)\n\n问题排查过程\n--\n\n很明显，是commons-io包的FileUtils工具类抛出的异常，原因是在调用commons-io包的FileUtils工具类做move directory操作时，目的文件夹已存在。\n\n查看调用代码（supervisor.clj的第374行），是调用download-storm-code方法从nimbus下载topology的代码，并且download-storm-code方法中做代码下载前加了锁避免并发写文件。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/2.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/3.png)\n\n果然，这里没有判断stormroot文件夹是否已存在，是个bug，具体可见这个issue：[https://issues.apache.org/jira/browse/STORM-805](https://issues.apache.org/jira/browse/STORM-805)。\n\n这个问题在0.9.5版本中随着STORM-130一起修复了，代码如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/4.png)\n\n但这里有三个问题：\n\n1. 在调用download-storm-code方法前，代码中已做判断是否已下载topology代码，若已下载就不会调用download-storm-code方法了。为何进入这个方法后，做move directory操作时，代码却已经下载好了呢？\n\n2. storm的历史发布版本有很多，为何0.9.4版本里会出现这个不该出现的问题，0.9.4相对老的版本是不是做了什么修改？\n\n3. 为何抛出异常后，supervisor进程就这么直接退出了？太弱了吧。。\n<!--more-->\n继续看0.9.4的源码发现，**supervisor中有以下两个事件线程，都会调用download-storm-code方法**：\n\n**一个是synchronize-supervisor，用于同步nimbus任务**，每隔10秒执行一次，会调用mk-synchronize-supervisor方法，以及时获取nimbus分配给该supervisor的新任务并移除已分配但不再需要执行的任务。\n\n**另一个是sync-processes，用于根据任务变化同步管理worker进程**，执行周期由SUPERVISOR-MONITOR-FREQUENCY-SECS（默认3秒）指定，会调用sync-processes方法，以关闭当前不处于valid状态的worker和启动新分配给该supervisor的worker。\n\n其中，mk-synchronize-supervisor方法和sync-processes方法都会调用download-storm-code方法。\n\n两个事件线程的定义：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/5.png)\n\nmk-synchronize-supervisor方法调用download-storm-code方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/6.png)\n\nsync-processes调用download-storm-code方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/7.png)\n\nmk-synchronize-supervisor方法和sync-processes方法在调用前都会判断topology代码是否已下载，所以，出现上述异常的原因很可能是两个线程再调用download-storm-code方法时不同步引起的，即同时判断到需要下载topology代码并进入了download-storm-code方法，从而产生两次move directory的操作引发异常。\n\n虽然download-storm-code方法内部通过加锁控制了写文件时的并发，但对进入download-storm-code方法并没有做好同步。\n\n再回过头看0.9.5版本的代码，虽然在move directory前判断了目的文件夹是否存在以避免问题，但实际上还是存在两个线程同时进入download-storm-code方法的问题。\n\n最后再比较了下0.9.3和0.9.4的代码（supervisor.clj），发现0.9.4的sync-processes方法中调用download-storm-code的逻辑是新加进去的，也就是说这个bug是0.9.4新引入的，以前的版本不会存在这个问题。\n\n左边为0.9.3，右边为0.9.4：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/8.png)\n\n关于第3个问题，再回看定义synchronize-supervisor事件线程的代码，是通过事件管理器event-manager来实现的，查看event.clj中的实现，event-manager会从一个LinkedBlockingQueue取出新事件并启动线程处理，线程若抛出非Interrupted异常，则直接退出进程了。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/9.png)\n\n至此，问题分析完毕。","source":"_posts/storm集群supervisor节点异常退出问题排查.md","raw":"---\ntitle: storm集群supervisor节点异常退出问题排查\ndate: 2015-07-03 20:48:50\ntags:\n- Storm\n- Supervisor\n- 异常排查\ncategories:\n- Storm\n---\n\n问题出现\n--\n\n测试storm集群为0.9.4版本，前段时间出现supervisor进程挂掉，而其上work进程仍然运行的诡异情况，通过日志看到supervisor进程挂掉之前出现以下异常：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/1.png)\n\n问题排查过程\n--\n\n很明显，是commons-io包的FileUtils工具类抛出的异常，原因是在调用commons-io包的FileUtils工具类做move directory操作时，目的文件夹已存在。\n\n查看调用代码（supervisor.clj的第374行），是调用download-storm-code方法从nimbus下载topology的代码，并且download-storm-code方法中做代码下载前加了锁避免并发写文件。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/2.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/3.png)\n\n果然，这里没有判断stormroot文件夹是否已存在，是个bug，具体可见这个issue：[https://issues.apache.org/jira/browse/STORM-805](https://issues.apache.org/jira/browse/STORM-805)。\n\n这个问题在0.9.5版本中随着STORM-130一起修复了，代码如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/4.png)\n\n但这里有三个问题：\n\n1. 在调用download-storm-code方法前，代码中已做判断是否已下载topology代码，若已下载就不会调用download-storm-code方法了。为何进入这个方法后，做move directory操作时，代码却已经下载好了呢？\n\n2. storm的历史发布版本有很多，为何0.9.4版本里会出现这个不该出现的问题，0.9.4相对老的版本是不是做了什么修改？\n\n3. 为何抛出异常后，supervisor进程就这么直接退出了？太弱了吧。。\n<!--more-->\n继续看0.9.4的源码发现，**supervisor中有以下两个事件线程，都会调用download-storm-code方法**：\n\n**一个是synchronize-supervisor，用于同步nimbus任务**，每隔10秒执行一次，会调用mk-synchronize-supervisor方法，以及时获取nimbus分配给该supervisor的新任务并移除已分配但不再需要执行的任务。\n\n**另一个是sync-processes，用于根据任务变化同步管理worker进程**，执行周期由SUPERVISOR-MONITOR-FREQUENCY-SECS（默认3秒）指定，会调用sync-processes方法，以关闭当前不处于valid状态的worker和启动新分配给该supervisor的worker。\n\n其中，mk-synchronize-supervisor方法和sync-processes方法都会调用download-storm-code方法。\n\n两个事件线程的定义：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/5.png)\n\nmk-synchronize-supervisor方法调用download-storm-code方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/6.png)\n\nsync-processes调用download-storm-code方法：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/7.png)\n\nmk-synchronize-supervisor方法和sync-processes方法在调用前都会判断topology代码是否已下载，所以，出现上述异常的原因很可能是两个线程再调用download-storm-code方法时不同步引起的，即同时判断到需要下载topology代码并进入了download-storm-code方法，从而产生两次move directory的操作引发异常。\n\n虽然download-storm-code方法内部通过加锁控制了写文件时的并发，但对进入download-storm-code方法并没有做好同步。\n\n再回过头看0.9.5版本的代码，虽然在move directory前判断了目的文件夹是否存在以避免问题，但实际上还是存在两个线程同时进入download-storm-code方法的问题。\n\n最后再比较了下0.9.3和0.9.4的代码（supervisor.clj），发现0.9.4的sync-processes方法中调用download-storm-code的逻辑是新加进去的，也就是说这个bug是0.9.4新引入的，以前的版本不会存在这个问题。\n\n左边为0.9.3，右边为0.9.4：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/8.png)\n\n关于第3个问题，再回看定义synchronize-supervisor事件线程的代码，是通过事件管理器event-manager来实现的，查看event.clj中的实现，event-manager会从一个LinkedBlockingQueue取出新事件并启动线程处理，线程若抛出非Interrupted异常，则直接退出进程了。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150701/9.png)\n\n至此，问题分析完毕。","slug":"storm集群supervisor节点异常退出问题排查","published":1,"updated":"2017-01-18T09:54:11.736Z","_id":"ciy320aj1000difs6qafrusls","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"问题出现\"><a href=\"#问题出现\" class=\"headerlink\" title=\"问题出现\"></a>问题出现</h2><p>测试storm集群为0.9.4版本，前段时间出现supervisor进程挂掉，而其上work进程仍然运行的诡异情况，通过日志看到supervisor进程挂掉之前出现以下异常：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/1.png\" alt=\"\"></p>\n<h2 id=\"问题排查过程\"><a href=\"#问题排查过程\" class=\"headerlink\" title=\"问题排查过程\"></a>问题排查过程</h2><p>很明显，是commons-io包的FileUtils工具类抛出的异常，原因是在调用commons-io包的FileUtils工具类做move directory操作时，目的文件夹已存在。</p>\n<p>查看调用代码（supervisor.clj的第374行），是调用download-storm-code方法从nimbus下载topology的代码，并且download-storm-code方法中做代码下载前加了锁避免并发写文件。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/2.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/3.png\" alt=\"\"></p>\n<p>果然，这里没有判断stormroot文件夹是否已存在，是个bug，具体可见这个issue：<a href=\"https://issues.apache.org/jira/browse/STORM-805\" target=\"_blank\" rel=\"external\">https://issues.apache.org/jira/browse/STORM-805</a>。</p>\n<p>这个问题在0.9.5版本中随着STORM-130一起修复了，代码如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/4.png\" alt=\"\"></p>\n<p>但这里有三个问题：</p>\n<ol>\n<li><p>在调用download-storm-code方法前，代码中已做判断是否已下载topology代码，若已下载就不会调用download-storm-code方法了。为何进入这个方法后，做move directory操作时，代码却已经下载好了呢？</p>\n</li>\n<li><p>storm的历史发布版本有很多，为何0.9.4版本里会出现这个不该出现的问题，0.9.4相对老的版本是不是做了什么修改？</p>\n</li>\n<li><p>为何抛出异常后，supervisor进程就这么直接退出了？太弱了吧。。</p>\n<a id=\"more\"></a>\n<p>继续看0.9.4的源码发现，<strong>supervisor中有以下两个事件线程，都会调用download-storm-code方法</strong>：</p>\n</li>\n</ol>\n<p><strong>一个是synchronize-supervisor，用于同步nimbus任务</strong>，每隔10秒执行一次，会调用mk-synchronize-supervisor方法，以及时获取nimbus分配给该supervisor的新任务并移除已分配但不再需要执行的任务。</p>\n<p><strong>另一个是sync-processes，用于根据任务变化同步管理worker进程</strong>，执行周期由SUPERVISOR-MONITOR-FREQUENCY-SECS（默认3秒）指定，会调用sync-processes方法，以关闭当前不处于valid状态的worker和启动新分配给该supervisor的worker。</p>\n<p>其中，mk-synchronize-supervisor方法和sync-processes方法都会调用download-storm-code方法。</p>\n<p>两个事件线程的定义：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/5.png\" alt=\"\"></p>\n<p>mk-synchronize-supervisor方法调用download-storm-code方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/6.png\" alt=\"\"></p>\n<p>sync-processes调用download-storm-code方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/7.png\" alt=\"\"></p>\n<p>mk-synchronize-supervisor方法和sync-processes方法在调用前都会判断topology代码是否已下载，所以，出现上述异常的原因很可能是两个线程再调用download-storm-code方法时不同步引起的，即同时判断到需要下载topology代码并进入了download-storm-code方法，从而产生两次move directory的操作引发异常。</p>\n<p>虽然download-storm-code方法内部通过加锁控制了写文件时的并发，但对进入download-storm-code方法并没有做好同步。</p>\n<p>再回过头看0.9.5版本的代码，虽然在move directory前判断了目的文件夹是否存在以避免问题，但实际上还是存在两个线程同时进入download-storm-code方法的问题。</p>\n<p>最后再比较了下0.9.3和0.9.4的代码（supervisor.clj），发现0.9.4的sync-processes方法中调用download-storm-code的逻辑是新加进去的，也就是说这个bug是0.9.4新引入的，以前的版本不会存在这个问题。</p>\n<p>左边为0.9.3，右边为0.9.4：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/8.png\" alt=\"\"></p>\n<p>关于第3个问题，再回看定义synchronize-supervisor事件线程的代码，是通过事件管理器event-manager来实现的，查看event.clj中的实现，event-manager会从一个LinkedBlockingQueue取出新事件并启动线程处理，线程若抛出非Interrupted异常，则直接退出进程了。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/9.png\" alt=\"\"></p>\n<p>至此，问题分析完毕。</p>\n","excerpt":"<h2 id=\"问题出现\"><a href=\"#问题出现\" class=\"headerlink\" title=\"问题出现\"></a>问题出现</h2><p>测试storm集群为0.9.4版本，前段时间出现supervisor进程挂掉，而其上work进程仍然运行的诡异情况，通过日志看到supervisor进程挂掉之前出现以下异常：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/1.png\" alt=\"\"></p>\n<h2 id=\"问题排查过程\"><a href=\"#问题排查过程\" class=\"headerlink\" title=\"问题排查过程\"></a>问题排查过程</h2><p>很明显，是commons-io包的FileUtils工具类抛出的异常，原因是在调用commons-io包的FileUtils工具类做move directory操作时，目的文件夹已存在。</p>\n<p>查看调用代码（supervisor.clj的第374行），是调用download-storm-code方法从nimbus下载topology的代码，并且download-storm-code方法中做代码下载前加了锁避免并发写文件。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/2.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/3.png\" alt=\"\"></p>\n<p>果然，这里没有判断stormroot文件夹是否已存在，是个bug，具体可见这个issue：<a href=\"https://issues.apache.org/jira/browse/STORM-805\">https://issues.apache.org/jira/browse/STORM-805</a>。</p>\n<p>这个问题在0.9.5版本中随着STORM-130一起修复了，代码如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/4.png\" alt=\"\"></p>\n<p>但这里有三个问题：</p>\n<ol>\n<li><p>在调用download-storm-code方法前，代码中已做判断是否已下载topology代码，若已下载就不会调用download-storm-code方法了。为何进入这个方法后，做move directory操作时，代码却已经下载好了呢？</p>\n</li>\n<li><p>storm的历史发布版本有很多，为何0.9.4版本里会出现这个不该出现的问题，0.9.4相对老的版本是不是做了什么修改？</p>\n</li>\n<li><p>为何抛出异常后，supervisor进程就这么直接退出了？太弱了吧。。</p>","more":"<p>继续看0.9.4的源码发现，<strong>supervisor中有以下两个事件线程，都会调用download-storm-code方法</strong>：</p>\n</li>\n</ol>\n<p><strong>一个是synchronize-supervisor，用于同步nimbus任务</strong>，每隔10秒执行一次，会调用mk-synchronize-supervisor方法，以及时获取nimbus分配给该supervisor的新任务并移除已分配但不再需要执行的任务。</p>\n<p><strong>另一个是sync-processes，用于根据任务变化同步管理worker进程</strong>，执行周期由SUPERVISOR-MONITOR-FREQUENCY-SECS（默认3秒）指定，会调用sync-processes方法，以关闭当前不处于valid状态的worker和启动新分配给该supervisor的worker。</p>\n<p>其中，mk-synchronize-supervisor方法和sync-processes方法都会调用download-storm-code方法。</p>\n<p>两个事件线程的定义：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/5.png\" alt=\"\"></p>\n<p>mk-synchronize-supervisor方法调用download-storm-code方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/6.png\" alt=\"\"></p>\n<p>sync-processes调用download-storm-code方法：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/7.png\" alt=\"\"></p>\n<p>mk-synchronize-supervisor方法和sync-processes方法在调用前都会判断topology代码是否已下载，所以，出现上述异常的原因很可能是两个线程再调用download-storm-code方法时不同步引起的，即同时判断到需要下载topology代码并进入了download-storm-code方法，从而产生两次move directory的操作引发异常。</p>\n<p>虽然download-storm-code方法内部通过加锁控制了写文件时的并发，但对进入download-storm-code方法并没有做好同步。</p>\n<p>再回过头看0.9.5版本的代码，虽然在move directory前判断了目的文件夹是否存在以避免问题，但实际上还是存在两个线程同时进入download-storm-code方法的问题。</p>\n<p>最后再比较了下0.9.3和0.9.4的代码（supervisor.clj），发现0.9.4的sync-processes方法中调用download-storm-code的逻辑是新加进去的，也就是说这个bug是0.9.4新引入的，以前的版本不会存在这个问题。</p>\n<p>左边为0.9.3，右边为0.9.4：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/8.png\" alt=\"\"></p>\n<p>关于第3个问题，再回看定义synchronize-supervisor事件线程的代码，是通过事件管理器event-manager来实现的，查看event.clj中的实现，event-manager会从一个LinkedBlockingQueue取出新事件并启动线程处理，线程若抛出非Interrupted异常，则直接退出进程了。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150701/9.png\" alt=\"\"></p>\n<p>至此，问题分析完毕。</p>"},{"title":"storm源码编译及本地调试方法","date":"2016-07-13T15:53:12.000Z","_content":"\n基础环境\n--\n\n* IDE开发环境：intelliJIdea\n* JDK1.7  64bit\n* intelliJIdea安装maven插件，配置好仓库源\n* intelliJIdea安装clojure插件Cursive（需要注册并获取一个license，否则只能使用30天）\n* 如果需要自己创建clojure项目进行开发，需要安装leiningen，[下载地址](http://leiningen.org/)\n\n源码获取\n--\n\n从github checkout代码到本地即可，https://github.com/apache/storm.git\n\n我这里编译的是我们目前正在用的0.10.0版本的代码。\n\n\n导入idea及编译\n--\n\n打开idea，新建project，从源码导入，如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/1.png)\n\n导入后，idea会自动根据pom.xml下载相关依赖包，部分依赖包如果下载不到，需要手动添加。完成后，可以看到project的module如下图所示：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/2.png)\n\n<!--more-->\n\n这时候，通过idea就可以直接跟踪看源码了，但直接运行storm-starter中的例子还是会报错并提示有些类找不到，经查看是clojure的代码还未编译出class文件。可以在源码目录下执行mvn compile进行编译。\n\n使用idea调试源码\n--\n\n编译完成后，可以直接启动storm-starter中的例子运行。期间可能出现找不到类，检查classpath，依赖包的scope由provided改为compile。\n\n在源代码中加断点，run或者debug即可。\n\n> 2739 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 4546 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 5218 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2000 and dir /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//eeb57be9-5478-4fa9-ab31-6dfce38e7695\n> 5243 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 5340 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 5342 [main] INFO  b.s.d.nimbus - Using default scheduler\n> 5360 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 5457 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 5529 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 5531 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6569 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6569 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6574 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6605 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6605 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6609 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6609 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6617 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6618 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6620 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6621 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6623 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6625 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6649 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6652 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6653 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6657 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6671 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 6693 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6694 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6697 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6697 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6700 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6701 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6704 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6722 [main] INFO  b.s.d.supervisor - Starting supervisor with id 913c90f6-3f78-4646-8998-aa901ae3c360 at host localhost\n> 6725 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, .....\n> 6732 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6732 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6736 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6736 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6740 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6741 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6744 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6753 [main] INFO  b.s.d.supervisor - Starting supervisor with id 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 at host localhost\n> 7035 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:submitTopology\n> 7113 [main] INFO  b.s.d.nimbus - Received topology submission for wordCounter with conf {\"topology.max.task.parallelism\" nil, \"topology.submitter.principal\" \"\", \"topology.acker.executors\" nil, \"topology.max.spout.pending\" 20, \"storm.zookeeper.superACL\" nil, \"topology.users\" (), \"topology.submitter.user\" \"\", \"topology.kryo.register\" {\"storm.trident.topology.TransactionAttempt\" nil, \"storm.trident.spout.RichSpoutBatchId\" \"storm.trident.spout.RichSpoutBatchIdSerializer\"}, \"topology.kryo.decorators\" (), \"storm.id\" \"wordCounter-1-1468420782\", \"topology.name\" \"wordCounter\"}\n> 7123 [main] INFO  b.s.d.nimbus - nimbus file location:/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782\n> 7152 [main] INFO  b.s.d.nimbus - Activating wordCounter: wordCounter-1-1468420782\n> 7346 [main] INFO  b.s.s.EvenScheduler - Available slots: ([\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1028] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1029] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1024] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1025] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1026])\n> 7398 [main] INFO  b.s.d.nimbus - Setting new assignment for topology id wordCounter-1-1468420782: #backtype.storm.daemon.common.Assignment{:master-code-dir \"/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782\", :node->host {\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" \"localhost\"}, :executor->node+port {[8 8] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [12 12] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [2 2] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [7 7] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [22 22] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [3 3] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [24 24] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [1 1] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [18 18] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [6 6] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [20 20] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [9 9] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [23 23] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [11 11] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [16 16] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [13 13] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [19 19] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [21 21] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [5 5] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [26 26] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [10 10] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [14 14] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [4 4] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [15 15] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [25 25] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [17 17] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027]}, :executor->start-time-secs {[8 8] 1468420782, [12 12] 1468420782, [2 2] 1468420782, [7 7] 1468420782, [22 22] 1468420782, [3 3] 1468420782, [24 24] 1468420782, [1 1] 1468420782, [18 18] 1468420782, [6 6] 1468420782, [20 20] 1468420782, [9 9] 1468420782, [23 23] 1468420782, [11 11] 1468420782, [16 16] 1468420782, [13 13] 1468420782, [19 19] 1468420782, [21 21] 1468420782, [5 5] 1468420782, [26 26] 1468420782, [10 10] 1468420782, [14 14] 1468420782, [4 4] 1468420782, [15 15] 1468420782, [25 25] 1468420782, [17 17] 1468420782}}\n> 7751 [Thread-7] INFO  b.s.d.supervisor - Extracting resources from jar at /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/ant-javafx.jar to /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//29645b09-90e9-4b9a-a657-60c418f92841/supervisor/stormdist/wordCounter-1-1468420782/resources\n> 7788 [Thread-8] INFO  b.s.d.supervisor - Launching worker with assignment {:storm-id \"wordCounter-1-1468420782\", :executors [[8 8] [12 12] [2 2] [7 7] [22 22] [3 3] [24 24] [1 1] [18 18] [6 6] [20 20] [9 9] [23 23] [11 11] [16 16] [13 13] [19 19] [21 21] [5 5] [26 26] [10 10] [14 14] [4 4] [15 15] [25 25] [17 17]]} for this supervisor 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 on port 1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99\n> 7791 [Thread-8] INFO  b.s.d.worker - Launching worker for wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99 and conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 7793 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 7794 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 7798 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 7798 [Thread-8-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 7801 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 7802 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 7805 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 7809 [Thread-8] INFO  b.s.s.a.AuthUtils - Got AutoCreds []\n> 7811 [Thread-8] INFO  b.s.d.worker - Reading Assignments.\n> 7881 [Thread-8] INFO  b.s.d.worker - Launching receive-thread for 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027\n> 7884 [Thread-9-worker-receiver-thread-0] INFO  b.s.m.loader - Starting receive-thread: [stormId: wordCounter-1-1468420782, port: 1027, thread-id: 0 ]\n> 8261 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[8 8]\n> 8285 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[8 8]\n> 8300 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[8 8]\n> 8311 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[12 12]\n> 8329 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[12 12]\n> 8331 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[12 12]\n> 8340 [Thread-8] INFO  b.s.d.executor - Loading executor $spoutcoord-spout0:[2 2]\n> 8343 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $spoutcoord-spout0:[2 2]\n> 8346 [Thread-8] INFO  b.s.d.executor - Finished loading executor $spoutcoord-spout0:[2 2]\n> 8355 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[7 7]\n> 8372 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[7 7]\n> 8375 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[7 7]\n> 8381 [Thread-8] INFO  b.s.d.executor - Loading executor b-3:[22 22]\n> 8401 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-3:[22 22]\n> 8404 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-3:[22 22]\n> 8412 [Thread-8] INFO  b.s.d.executor - Loading executor __acker:[3 3]\n> 8414 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks __acker:[3 3]\n> 8424 [Thread-8] INFO  b.s.d.executor - Timeouts disabled for executor __acker:[3 3]\n> 8425 [Thread-8] INFO  b.s.d.executor - Finished loading executor __acker:[3 3]\n> 8443 [Thread-8] INFO  b.s.d.executor - Loading executor b-5:[24 24]\n> 8465 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-5:[24 24]\n> 8467 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-5:[24 24]\n> 8530 [Thread-8] INFO  b.s.d.executor - Loading executor $mastercoord-bg0:[1 1]\n> 8539 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $mastercoord-bg0:[1 1]\n> 8576 [Thread-8] INFO  b.s.d.executor - Finished loading executor $mastercoord-bg0:[1 1]\n> 8603 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[18 18]\n> 8633 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[18 18]\n> 8635 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[18 18]\n> 8646 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[6 6]\n> 8681 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[6 6]\n> 8683 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[6 6]\n> 8719 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[20 20]\n> 8757 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[20 20]\n> 8763 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[20 20]\n> 8782 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[9 9]\n> 8808 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[9 9]\n> 8818 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[9 9]\n> 8828 [Thread-8] INFO  b.s.d.executor - Loading executor b-4:[23 23]\n> 8847 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-4:[23 23]\n> 8851 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-4:[23 23]\n> 8858 [refresh-active-timer] INFO  b.s.d.worker - All connections are ready for worker 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99\n> 8864 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[11 11]\n> 8877 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[11 11]\n> 8879 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[11 11]\n> 8886 [Thread-8] INFO  b.s.d.executor - Loading executor __system:[-1 -1]\n> 8887 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks __system:[-1 -1]\n> 8890 [Thread-8] INFO  b.s.d.executor - Finished loading executor __system:[-1 -1]\n> 8914 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[16 16]\n> 9052 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[16 16]\n> 9055 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[16 16]\n> 9070 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[13 13]\n> 9081 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[13 13]\n> 9089 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[13 13]\n> 9116 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[19 19]\n> 9129 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[19 19]\n> 9132 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[19 19]\n> 9148 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[21 21]\n> 9160 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[21 21]\n> 9163 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[21 21]\n> 9178 [Thread-8] INFO  b.s.d.executor - Loading executor b-1:[5 5]\n> 9192 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-1:[5 5]\n> 9194 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-1:[5 5]\n> 9204 [Thread-8] INFO  b.s.d.executor - Loading executor spout1:[26 26]\n> 9205 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout1:[26 26]\n> 9208 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout1:[26 26]\n> 9220 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[10 10]\n> 9226 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[10 10]\n> 9228 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[10 10]\n> 9234 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[14 14]\n> 9237 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[14 14]\n> 9239 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[14 14]\n> 9244 [Thread-8] INFO  b.s.d.executor - Loading executor b-0:[4 4]\n> 9248 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-0:[4 4]\n> 9249 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-0:[4 4]\n> 9255 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[15 15]\n> 9260 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[15 15]\n> 9261 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[15 15]\n> 9273 [Thread-8] INFO  b.s.d.executor - Loading executor spout0:[25 25]\n> 9275 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout0:[25 25]\n> 9277 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout0:[25 25]\n> 9284 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[17 17]\n> 9289 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[17 17]\n> 9291 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[17 17]\n> 9298 [Thread-8] INFO  b.s.d.worker - Worker has topology config {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 9298 [Thread-8] INFO  b.s.d.worker - Worker 9dd8aeac-1cd6-467a-a84c-2637d0825d99 for storm wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 has finished loading\n> 9298 [Thread-8] INFO  b.s.config - SET worker-user 9dd8aeac-1cd6-467a-a84c-2637d0825d99 \n> 9875 [Thread-27-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(18)\n> 9882 [Thread-35-b-4] INFO  b.s.d.executor - Preparing bolt b-4:(23)\n> 9882 [Thread-41-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(16)\n> 9883 [Thread-13-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(12)\n> 9883 [Thread-59-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(15)\n> 9883 [Thread-47-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(21)\n> 9893 [Thread-35-b-4] INFO  b.s.d.executor - Prepared bolt b-4:(23)\n> 9896 [Thread-47-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(21)\n> 9896 [Thread-59-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(15)\n> 9896 [Thread-27-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(18)\n> 9896 [Thread-13-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(12)\n> 9896 [Thread-41-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(16)\n> 9898 [Thread-31-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(20)\n> 9898 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Preparing bolt $spoutcoord-spout0:(2)\n> 9899 [Thread-61-spout0] INFO  b.s.d.executor - Preparing bolt spout0:(25)\n> 9900 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9900 [Thread-61-spout0] INFO  b.s.d.executor - Prepared bolt spout0:(25)\n> 9901 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9901 [Thread-31-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(20)\n> 9907 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9908 [Thread-43-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(13)\n> 9908 [Thread-37-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(11)\n> 9908 [Thread-63-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(17)\n> 9910 [Thread-43-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(13)\n> 9910 [Thread-37-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(11)\n> 9911 [Thread-63-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(17)\n> 9918 [Thread-49-b-1] INFO  b.s.d.executor - Preparing bolt b-1:(5)\n> 9918 [Thread-39-__system] INFO  b.s.d.executor - Preparing bolt __system:(-1)\n> 9918 [Thread-29-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(6)\n> 9920 [Thread-49-b-1] INFO  b.s.d.executor - Prepared bolt b-1:(5)\n> 9920 [Thread-29-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(6)\n> 9921 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9922 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9924 [Thread-39-__system] INFO  b.s.d.executor - Prepared bolt __system:(-1)\n> 9929 [Thread-51-spout1] INFO  b.s.d.executor - Opening spout spout1:(26)\n> 9929 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opening spout $mastercoord-bg0:(1)\n> 9929 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9938 [Thread-51-spout1] INFO  b.s.d.executor - Opened spout spout1:(26)\n> 9937 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9940 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9940 [Thread-33-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(9)\n> 9942 [Thread-51-spout1] INFO  b.s.d.executor - Activating spout spout1:(26)\n> 9942 [Thread-33-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(9)\n> 9947 [Thread-53-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(10)\n> 9950 [Thread-53-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(10)\n> 9956 [Thread-11-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(8)\n> 9956 [Thread-45-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(19)\n> 9957 [Thread-23-b-5] INFO  b.s.d.executor - Preparing bolt b-5:(24)\n> 9958 [Thread-23-b-5] INFO  b.s.d.executor - Prepared bolt b-5:(24)\n> 9958 [Thread-11-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(8)\n> 9958 [Thread-17-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(7)\n> 9959 [Thread-55-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(14)\n> 9959 [Thread-19-b-3] INFO  b.s.d.executor - Preparing bolt b-3:(22)\n> 9960 [Thread-19-b-3] INFO  b.s.d.executor - Prepared bolt b-3:(22)\n> 9960 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9960 [Thread-17-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(7)\n> 9962 [Thread-45-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(19)\n> 9963 [Thread-55-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(14)\n> 9964 [Thread-57-b-0] INFO  b.s.d.executor - Preparing bolt b-0:(4)\n> 9964 [Thread-21-__acker] INFO  b.s.d.executor - Preparing bolt __acker:(3)\n> 9965 [Thread-57-b-0] INFO  b.s.d.executor - Prepared bolt b-0:(4)\n> 9966 [Thread-21-__acker] INFO  b.s.d.executor - Prepared bolt __acker:(3)\n> 9969 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Prepared bolt $spoutcoord-spout0:(2)\n> 9971 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9972 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9984 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> DRPC RESULT: [[0]]\n> 9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opened spout $mastercoord-bg0:(1)\n> 9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Activating spout $mastercoord-bg0:(1)\n> DRPC RESULT: [[60]]\n> DRPC RESULT: [[120]]\n> DRPC RESULT: [[179]]\n> DRPC RESULT: [[239]]\n> DRPC RESULT: [[299]]\n> DRPC RESULT: [[359]]\n> DRPC RESULT: [[414]]\n> DRPC RESULT: [[474]]\n> DRPC RESULT: [[534]]\n> DRPC RESULT: [[593]]\n> DRPC RESULT: [[653]]\n> DRPC RESULT: [[713]]\n> DRPC RESULT: [[768]]\n> \n> Process finished with exit code 130\n\n\n\n","source":"_posts/storm源码编译及本地调试方法.md","raw":"---\ntitle: storm源码编译及本地调试方法\ndate: 2016-07-13 23:53:12\ntags: \n- storm\n- 源码编译\n- 本地调试\ncategories:\n- Storm\n---\n\n基础环境\n--\n\n* IDE开发环境：intelliJIdea\n* JDK1.7  64bit\n* intelliJIdea安装maven插件，配置好仓库源\n* intelliJIdea安装clojure插件Cursive（需要注册并获取一个license，否则只能使用30天）\n* 如果需要自己创建clojure项目进行开发，需要安装leiningen，[下载地址](http://leiningen.org/)\n\n源码获取\n--\n\n从github checkout代码到本地即可，https://github.com/apache/storm.git\n\n我这里编译的是我们目前正在用的0.10.0版本的代码。\n\n\n导入idea及编译\n--\n\n打开idea，新建project，从源码导入，如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/1.png)\n\n导入后，idea会自动根据pom.xml下载相关依赖包，部分依赖包如果下载不到，需要手动添加。完成后，可以看到project的module如下图所示：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/2.png)\n\n<!--more-->\n\n这时候，通过idea就可以直接跟踪看源码了，但直接运行storm-starter中的例子还是会报错并提示有些类找不到，经查看是clojure的代码还未编译出class文件。可以在源码目录下执行mvn compile进行编译。\n\n使用idea调试源码\n--\n\n编译完成后，可以直接启动storm-starter中的例子运行。期间可能出现找不到类，检查classpath，依赖包的scope由provided改为compile。\n\n在源代码中加断点，run或者debug即可。\n\n> 2739 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 4546 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 5218 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2000 and dir /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//eeb57be9-5478-4fa9-ab31-6dfce38e7695\n> 5243 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources\n> 5340 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 5342 [main] INFO  b.s.d.nimbus - Using default scheduler\n> 5360 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 5457 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 5529 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 5531 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6569 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6569 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6574 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6605 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6605 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6609 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6609 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6617 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6618 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6620 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6621 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6623 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6625 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6649 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6652 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6653 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6657 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6671 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 6693 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6694 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6697 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6697 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6700 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6701 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6704 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6722 [main] INFO  b.s.d.supervisor - Starting supervisor with id 913c90f6-3f78-4646-8998-aa901ae3c360 at host localhost\n> 6725 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {\"topology.builtin.metrics.bucket.size.secs\" 60, .....\n> 6732 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6732 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6736 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6736 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 6740 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 6741 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 6744 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 6753 [main] INFO  b.s.d.supervisor - Starting supervisor with id 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 at host localhost\n> 7035 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:submitTopology\n> 7113 [main] INFO  b.s.d.nimbus - Received topology submission for wordCounter with conf {\"topology.max.task.parallelism\" nil, \"topology.submitter.principal\" \"\", \"topology.acker.executors\" nil, \"topology.max.spout.pending\" 20, \"storm.zookeeper.superACL\" nil, \"topology.users\" (), \"topology.submitter.user\" \"\", \"topology.kryo.register\" {\"storm.trident.topology.TransactionAttempt\" nil, \"storm.trident.spout.RichSpoutBatchId\" \"storm.trident.spout.RichSpoutBatchIdSerializer\"}, \"topology.kryo.decorators\" (), \"storm.id\" \"wordCounter-1-1468420782\", \"topology.name\" \"wordCounter\"}\n> 7123 [main] INFO  b.s.d.nimbus - nimbus file location:/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782\n> 7152 [main] INFO  b.s.d.nimbus - Activating wordCounter: wordCounter-1-1468420782\n> 7346 [main] INFO  b.s.s.EvenScheduler - Available slots: ([\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1028] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1029] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1024] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1025] [\"913c90f6-3f78-4646-8998-aa901ae3c360\" 1026])\n> 7398 [main] INFO  b.s.d.nimbus - Setting new assignment for topology id wordCounter-1-1468420782: #backtype.storm.daemon.common.Assignment{:master-code-dir \"/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782\", :node->host {\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" \"localhost\"}, :executor->node+port {[8 8] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [12 12] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [2 2] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [7 7] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [22 22] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [3 3] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [24 24] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [1 1] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [18 18] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [6 6] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [20 20] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [9 9] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [23 23] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [11 11] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [16 16] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [13 13] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [19 19] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [21 21] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [5 5] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [26 26] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [10 10] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [14 14] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [4 4] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [15 15] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [25 25] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027], [17 17] [\"49c35a73-7500-4ea4-aaa2-4b1c1f231fd4\" 1027]}, :executor->start-time-secs {[8 8] 1468420782, [12 12] 1468420782, [2 2] 1468420782, [7 7] 1468420782, [22 22] 1468420782, [3 3] 1468420782, [24 24] 1468420782, [1 1] 1468420782, [18 18] 1468420782, [6 6] 1468420782, [20 20] 1468420782, [9 9] 1468420782, [23 23] 1468420782, [11 11] 1468420782, [16 16] 1468420782, [13 13] 1468420782, [19 19] 1468420782, [21 21] 1468420782, [5 5] 1468420782, [26 26] 1468420782, [10 10] 1468420782, [14 14] 1468420782, [4 4] 1468420782, [15 15] 1468420782, [25 25] 1468420782, [17 17] 1468420782}}\n> 7751 [Thread-7] INFO  b.s.d.supervisor - Extracting resources from jar at /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/ant-javafx.jar to /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//29645b09-90e9-4b9a-a657-60c418f92841/supervisor/stormdist/wordCounter-1-1468420782/resources\n> 7788 [Thread-8] INFO  b.s.d.supervisor - Launching worker with assignment {:storm-id \"wordCounter-1-1468420782\", :executors [[8 8] [12 12] [2 2] [7 7] [22 22] [3 3] [24 24] [1 1] [18 18] [6 6] [20 20] [9 9] [23 23] [11 11] [16 16] [13 13] [19 19] [21 21] [5 5] [26 26] [10 10] [14 14] [4 4] [15 15] [25 25] [17 17]]} for this supervisor 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 on port 1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99\n> 7791 [Thread-8] INFO  b.s.d.worker - Launching worker for wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99 and conf {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 7793 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 7794 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 7798 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 7798 [Thread-8-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none\n> 7801 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 7802 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 7805 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 7809 [Thread-8] INFO  b.s.s.a.AuthUtils - Got AutoCreds []\n> 7811 [Thread-8] INFO  b.s.d.worker - Reading Assignments.\n> 7881 [Thread-8] INFO  b.s.d.worker - Launching receive-thread for 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027\n> 7884 [Thread-9-worker-receiver-thread-0] INFO  b.s.m.loader - Starting receive-thread: [stormId: wordCounter-1-1468420782, port: 1027, thread-id: 0 ]\n> 8261 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[8 8]\n> 8285 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[8 8]\n> 8300 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[8 8]\n> 8311 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[12 12]\n> 8329 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[12 12]\n> 8331 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[12 12]\n> 8340 [Thread-8] INFO  b.s.d.executor - Loading executor $spoutcoord-spout0:[2 2]\n> 8343 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $spoutcoord-spout0:[2 2]\n> 8346 [Thread-8] INFO  b.s.d.executor - Finished loading executor $spoutcoord-spout0:[2 2]\n> 8355 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[7 7]\n> 8372 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[7 7]\n> 8375 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[7 7]\n> 8381 [Thread-8] INFO  b.s.d.executor - Loading executor b-3:[22 22]\n> 8401 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-3:[22 22]\n> 8404 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-3:[22 22]\n> 8412 [Thread-8] INFO  b.s.d.executor - Loading executor __acker:[3 3]\n> 8414 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks __acker:[3 3]\n> 8424 [Thread-8] INFO  b.s.d.executor - Timeouts disabled for executor __acker:[3 3]\n> 8425 [Thread-8] INFO  b.s.d.executor - Finished loading executor __acker:[3 3]\n> 8443 [Thread-8] INFO  b.s.d.executor - Loading executor b-5:[24 24]\n> 8465 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-5:[24 24]\n> 8467 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-5:[24 24]\n> 8530 [Thread-8] INFO  b.s.d.executor - Loading executor $mastercoord-bg0:[1 1]\n> 8539 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $mastercoord-bg0:[1 1]\n> 8576 [Thread-8] INFO  b.s.d.executor - Finished loading executor $mastercoord-bg0:[1 1]\n> 8603 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[18 18]\n> 8633 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[18 18]\n> 8635 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[18 18]\n> 8646 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[6 6]\n> 8681 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[6 6]\n> 8683 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[6 6]\n> 8719 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[20 20]\n> 8757 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[20 20]\n> 8763 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[20 20]\n> 8782 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[9 9]\n> 8808 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[9 9]\n> 8818 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[9 9]\n> 8828 [Thread-8] INFO  b.s.d.executor - Loading executor b-4:[23 23]\n> 8847 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-4:[23 23]\n> 8851 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-4:[23 23]\n> 8858 [refresh-active-timer] INFO  b.s.d.worker - All connections are ready for worker 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99\n> 8864 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[11 11]\n> 8877 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[11 11]\n> 8879 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[11 11]\n> 8886 [Thread-8] INFO  b.s.d.executor - Loading executor __system:[-1 -1]\n> 8887 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks __system:[-1 -1]\n> 8890 [Thread-8] INFO  b.s.d.executor - Finished loading executor __system:[-1 -1]\n> 8914 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[16 16]\n> 9052 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[16 16]\n> 9055 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[16 16]\n> 9070 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[13 13]\n> 9081 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[13 13]\n> 9089 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[13 13]\n> 9116 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[19 19]\n> 9129 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[19 19]\n> 9132 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[19 19]\n> 9148 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[21 21]\n> 9160 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[21 21]\n> 9163 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[21 21]\n> 9178 [Thread-8] INFO  b.s.d.executor - Loading executor b-1:[5 5]\n> 9192 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-1:[5 5]\n> 9194 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-1:[5 5]\n> 9204 [Thread-8] INFO  b.s.d.executor - Loading executor spout1:[26 26]\n> 9205 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout1:[26 26]\n> 9208 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout1:[26 26]\n> 9220 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[10 10]\n> 9226 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[10 10]\n> 9228 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[10 10]\n> 9234 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[14 14]\n> 9237 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[14 14]\n> 9239 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[14 14]\n> 9244 [Thread-8] INFO  b.s.d.executor - Loading executor b-0:[4 4]\n> 9248 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-0:[4 4]\n> 9249 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-0:[4 4]\n> 9255 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[15 15]\n> 9260 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[15 15]\n> 9261 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[15 15]\n> 9273 [Thread-8] INFO  b.s.d.executor - Loading executor spout0:[25 25]\n> 9275 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout0:[25 25]\n> 9277 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout0:[25 25]\n> 9284 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[17 17]\n> 9289 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[17 17]\n> 9291 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[17 17]\n> 9298 [Thread-8] INFO  b.s.d.worker - Worker has topology config {\"topology.builtin.metrics.bucket.size.secs\" 60, ......\n> 9298 [Thread-8] INFO  b.s.d.worker - Worker 9dd8aeac-1cd6-467a-a84c-2637d0825d99 for storm wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 has finished loading\n> 9298 [Thread-8] INFO  b.s.config - SET worker-user 9dd8aeac-1cd6-467a-a84c-2637d0825d99 \n> 9875 [Thread-27-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(18)\n> 9882 [Thread-35-b-4] INFO  b.s.d.executor - Preparing bolt b-4:(23)\n> 9882 [Thread-41-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(16)\n> 9883 [Thread-13-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(12)\n> 9883 [Thread-59-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(15)\n> 9883 [Thread-47-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(21)\n> 9893 [Thread-35-b-4] INFO  b.s.d.executor - Prepared bolt b-4:(23)\n> 9896 [Thread-47-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(21)\n> 9896 [Thread-59-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(15)\n> 9896 [Thread-27-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(18)\n> 9896 [Thread-13-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(12)\n> 9896 [Thread-41-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(16)\n> 9898 [Thread-31-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(20)\n> 9898 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Preparing bolt $spoutcoord-spout0:(2)\n> 9899 [Thread-61-spout0] INFO  b.s.d.executor - Preparing bolt spout0:(25)\n> 9900 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9900 [Thread-61-spout0] INFO  b.s.d.executor - Prepared bolt spout0:(25)\n> 9901 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9901 [Thread-31-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(20)\n> 9907 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9908 [Thread-43-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(13)\n> 9908 [Thread-37-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(11)\n> 9908 [Thread-63-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(17)\n> 9910 [Thread-43-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(13)\n> 9910 [Thread-37-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(11)\n> 9911 [Thread-63-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(17)\n> 9918 [Thread-49-b-1] INFO  b.s.d.executor - Preparing bolt b-1:(5)\n> 9918 [Thread-39-__system] INFO  b.s.d.executor - Preparing bolt __system:(-1)\n> 9918 [Thread-29-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(6)\n> 9920 [Thread-49-b-1] INFO  b.s.d.executor - Prepared bolt b-1:(5)\n> 9920 [Thread-29-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(6)\n> 9921 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9922 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9924 [Thread-39-__system] INFO  b.s.d.executor - Prepared bolt __system:(-1)\n> 9929 [Thread-51-spout1] INFO  b.s.d.executor - Opening spout spout1:(26)\n> 9929 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opening spout $mastercoord-bg0:(1)\n> 9929 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9938 [Thread-51-spout1] INFO  b.s.d.executor - Opened spout spout1:(26)\n> 9937 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9940 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9940 [Thread-33-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(9)\n> 9942 [Thread-51-spout1] INFO  b.s.d.executor - Activating spout spout1:(26)\n> 9942 [Thread-33-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(9)\n> 9947 [Thread-53-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(10)\n> 9950 [Thread-53-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(10)\n> 9956 [Thread-11-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(8)\n> 9956 [Thread-45-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(19)\n> 9957 [Thread-23-b-5] INFO  b.s.d.executor - Preparing bolt b-5:(24)\n> 9958 [Thread-23-b-5] INFO  b.s.d.executor - Prepared bolt b-5:(24)\n> 9958 [Thread-11-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(8)\n> 9958 [Thread-17-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(7)\n> 9959 [Thread-55-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(14)\n> 9959 [Thread-19-b-3] INFO  b.s.d.executor - Preparing bolt b-3:(22)\n> 9960 [Thread-19-b-3] INFO  b.s.d.executor - Prepared bolt b-3:(22)\n> 9960 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> 9960 [Thread-17-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(7)\n> 9962 [Thread-45-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(19)\n> 9963 [Thread-55-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(14)\n> 9964 [Thread-57-b-0] INFO  b.s.d.executor - Preparing bolt b-0:(4)\n> 9964 [Thread-21-__acker] INFO  b.s.d.executor - Preparing bolt __acker:(3)\n> 9965 [Thread-57-b-0] INFO  b.s.d.executor - Prepared bolt b-0:(4)\n> 9966 [Thread-21-__acker] INFO  b.s.d.executor - Prepared bolt __acker:(3)\n> 9969 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Prepared bolt $spoutcoord-spout0:(2)\n> 9971 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]\n> 9972 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting\n> 9984 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED\n> DRPC RESULT: [[0]]\n> 9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opened spout $mastercoord-bg0:(1)\n> 9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Activating spout $mastercoord-bg0:(1)\n> DRPC RESULT: [[60]]\n> DRPC RESULT: [[120]]\n> DRPC RESULT: [[179]]\n> DRPC RESULT: [[239]]\n> DRPC RESULT: [[299]]\n> DRPC RESULT: [[359]]\n> DRPC RESULT: [[414]]\n> DRPC RESULT: [[474]]\n> DRPC RESULT: [[534]]\n> DRPC RESULT: [[593]]\n> DRPC RESULT: [[653]]\n> DRPC RESULT: [[713]]\n> DRPC RESULT: [[768]]\n> \n> Process finished with exit code 130\n\n\n\n","slug":"storm源码编译及本地调试方法","published":1,"updated":"2017-01-18T09:54:11.735Z","_id":"ciy320aj3000gifs632u1shq2","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"基础环境\"><a href=\"#基础环境\" class=\"headerlink\" title=\"基础环境\"></a>基础环境</h2><ul>\n<li>IDE开发环境：intelliJIdea</li>\n<li>JDK1.7  64bit</li>\n<li>intelliJIdea安装maven插件，配置好仓库源</li>\n<li>intelliJIdea安装clojure插件Cursive（需要注册并获取一个license，否则只能使用30天）</li>\n<li>如果需要自己创建clojure项目进行开发，需要安装leiningen，<a href=\"http://leiningen.org/\" target=\"_blank\" rel=\"external\">下载地址</a></li>\n</ul>\n<h2 id=\"源码获取\"><a href=\"#源码获取\" class=\"headerlink\" title=\"源码获取\"></a>源码获取</h2><p>从github checkout代码到本地即可，<a href=\"https://github.com/apache/storm.git\" target=\"_blank\" rel=\"external\">https://github.com/apache/storm.git</a></p>\n<p>我这里编译的是我们目前正在用的0.10.0版本的代码。</p>\n<h2 id=\"导入idea及编译\"><a href=\"#导入idea及编译\" class=\"headerlink\" title=\"导入idea及编译\"></a>导入idea及编译</h2><p>打开idea，新建project，从源码导入，如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/1.png\" alt=\"\"></p>\n<p>导入后，idea会自动根据pom.xml下载相关依赖包，部分依赖包如果下载不到，需要手动添加。完成后，可以看到project的module如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/2.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n<p>这时候，通过idea就可以直接跟踪看源码了，但直接运行storm-starter中的例子还是会报错并提示有些类找不到，经查看是clojure的代码还未编译出class文件。可以在源码目录下执行mvn compile进行编译。</p>\n<h2 id=\"使用idea调试源码\"><a href=\"#使用idea调试源码\" class=\"headerlink\" title=\"使用idea调试源码\"></a>使用idea调试源码</h2><p>编译完成后，可以直接启动storm-starter中的例子运行。期间可能出现找不到类，检查classpath，依赖包的scope由provided改为compile。</p>\n<p>在源代码中加断点，run或者debug即可。</p>\n<blockquote>\n<p>2739 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>4546 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>5218 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2000 and dir /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//eeb57be9-5478-4fa9-ab31-6dfce38e7695<br>5243 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>5340 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>5342 [main] INFO  b.s.d.nimbus - Using default scheduler<br>5360 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>5457 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>5529 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>5531 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6569 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6569 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6574 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6605 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6605 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6609 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6609 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6617 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6618 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6620 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6621 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6623 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6625 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6649 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6652 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6653 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6657 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6671 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>6693 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6694 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6697 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6697 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6700 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6701 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6704 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6722 [main] INFO  b.s.d.supervisor - Starting supervisor with id 913c90f6-3f78-4646-8998-aa901ae3c360 at host localhost<br>6725 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {“topology.builtin.metrics.bucket.size.secs” 60, …..<br>6732 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6732 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6736 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6736 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6740 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6741 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6744 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6753 [main] INFO  b.s.d.supervisor - Starting supervisor with id 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 at host localhost<br>7035 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:submitTopology<br>7113 [main] INFO  b.s.d.nimbus - Received topology submission for wordCounter with conf {“topology.max.task.parallelism” nil, “topology.submitter.principal” “”, “topology.acker.executors” nil, “topology.max.spout.pending” 20, “storm.zookeeper.superACL” nil, “topology.users” (), “topology.submitter.user” “”, “topology.kryo.register” {“storm.trident.topology.TransactionAttempt” nil, “storm.trident.spout.RichSpoutBatchId” “storm.trident.spout.RichSpoutBatchIdSerializer”}, “topology.kryo.decorators” (), “storm.id” “wordCounter-1-1468420782”, “topology.name” “wordCounter”}<br>7123 [main] INFO  b.s.d.nimbus - nimbus file location:/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782<br>7152 [main] INFO  b.s.d.nimbus - Activating wordCounter: wordCounter-1-1468420782<br>7346 [main] INFO  b.s.s.EvenScheduler - Available slots: ([“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1028] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1029] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1024] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1025] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1026])<br>7398 [main] INFO  b.s.d.nimbus - Setting new assignment for topology id wordCounter-1-1468420782: #backtype.storm.daemon.common.Assignment{:master-code-dir “/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782”, :node-&gt;host {“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” “localhost”}, :executor-&gt;node+port {[8 8] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [12 12] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [2 2] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [7 7] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [22 22] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [3 3] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [24 24] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [1 1] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [18 18] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [6 6] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [20 20] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [9 9] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [23 23] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [11 11] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [16 16] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [13 13] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [19 19] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [21 21] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [5 5] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [26 26] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [10 10] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [14 14] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [4 4] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [15 15] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [25 25] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [17 17] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027]}, :executor-&gt;start-time-secs {[8 8] 1468420782, [12 12] 1468420782, [2 2] 1468420782, [7 7] 1468420782, [22 22] 1468420782, [3 3] 1468420782, [24 24] 1468420782, [1 1] 1468420782, [18 18] 1468420782, [6 6] 1468420782, [20 20] 1468420782, [9 9] 1468420782, [23 23] 1468420782, [11 11] 1468420782, [16 16] 1468420782, [13 13] 1468420782, [19 19] 1468420782, [21 21] 1468420782, [5 5] 1468420782, [26 26] 1468420782, [10 10] 1468420782, [14 14] 1468420782, [4 4] 1468420782, [15 15] 1468420782, [25 25] 1468420782, [17 17] 1468420782}}<br>7751 [Thread-7] INFO  b.s.d.supervisor - Extracting resources from jar at /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/ant-javafx.jar to /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//29645b09-90e9-4b9a-a657-60c418f92841/supervisor/stormdist/wordCounter-1-1468420782/resources<br>7788 [Thread-8] INFO  b.s.d.supervisor - Launching worker with assignment {:storm-id “wordCounter-1-1468420782”, :executors [[8 8] [12 12] [2 2] [7 7] [22 22] [3 3] [24 24] [1 1] [18 18] [6 6] [20 20] [9 9] [23 23] [11 11] [16 16] [13 13] [19 19] [21 21] [5 5] [26 26] [10 10] [14 14] [4 4] [15 15] [25 25] [17 17]]} for this supervisor 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 on port 1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>7791 [Thread-8] INFO  b.s.d.worker - Launching worker for wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99 and conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>7793 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>7794 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>7798 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>7798 [Thread-8-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>7801 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>7802 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>7805 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>7809 [Thread-8] INFO  b.s.s.a.AuthUtils - Got AutoCreds []<br>7811 [Thread-8] INFO  b.s.d.worker - Reading Assignments.<br>7881 [Thread-8] INFO  b.s.d.worker - Launching receive-thread for 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027<br>7884 [Thread-9-worker-receiver-thread-0] INFO  b.s.m.loader - Starting receive-thread: [stormId: wordCounter-1-1468420782, port: 1027, thread-id: 0 ]<br>8261 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[8 8]<br>8285 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[8 8]<br>8300 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[8 8]<br>8311 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[12 12]<br>8329 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[12 12]<br>8331 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[12 12]<br>8340 [Thread-8] INFO  b.s.d.executor - Loading executor $spoutcoord-spout0:[2 2]<br>8343 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $spoutcoord-spout0:[2 2]<br>8346 [Thread-8] INFO  b.s.d.executor - Finished loading executor $spoutcoord-spout0:[2 2]<br>8355 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[7 7]<br>8372 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[7 7]<br>8375 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[7 7]<br>8381 [Thread-8] INFO  b.s.d.executor - Loading executor b-3:[22 22]<br>8401 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-3:[22 22]<br>8404 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-3:[22 22]<br>8412 [Thread-8] INFO  b.s.d.executor - Loading executor <strong>acker:[3 3]<br>8414 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks </strong>acker:[3 3]<br>8424 [Thread-8] INFO  b.s.d.executor - Timeouts disabled for executor <strong>acker:[3 3]<br>8425 [Thread-8] INFO  b.s.d.executor - Finished loading executor </strong>acker:[3 3]<br>8443 [Thread-8] INFO  b.s.d.executor - Loading executor b-5:[24 24]<br>8465 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-5:[24 24]<br>8467 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-5:[24 24]<br>8530 [Thread-8] INFO  b.s.d.executor - Loading executor $mastercoord-bg0:[1 1]<br>8539 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $mastercoord-bg0:[1 1]<br>8576 [Thread-8] INFO  b.s.d.executor - Finished loading executor $mastercoord-bg0:[1 1]<br>8603 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[18 18]<br>8633 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[18 18]<br>8635 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[18 18]<br>8646 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[6 6]<br>8681 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[6 6]<br>8683 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[6 6]<br>8719 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[20 20]<br>8757 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[20 20]<br>8763 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[20 20]<br>8782 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[9 9]<br>8808 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[9 9]<br>8818 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[9 9]<br>8828 [Thread-8] INFO  b.s.d.executor - Loading executor b-4:[23 23]<br>8847 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-4:[23 23]<br>8851 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-4:[23 23]<br>8858 [refresh-active-timer] INFO  b.s.d.worker - All connections are ready for worker 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>8864 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[11 11]<br>8877 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[11 11]<br>8879 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[11 11]<br>8886 [Thread-8] INFO  b.s.d.executor - Loading executor <strong>system:[-1 -1]<br>8887 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks </strong>system:[-1 -1]<br>8890 [Thread-8] INFO  b.s.d.executor - Finished loading executor <strong>system:[-1 -1]<br>8914 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[16 16]<br>9052 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[16 16]<br>9055 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[16 16]<br>9070 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[13 13]<br>9081 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[13 13]<br>9089 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[13 13]<br>9116 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[19 19]<br>9129 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[19 19]<br>9132 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[19 19]<br>9148 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[21 21]<br>9160 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[21 21]<br>9163 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[21 21]<br>9178 [Thread-8] INFO  b.s.d.executor - Loading executor b-1:[5 5]<br>9192 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-1:[5 5]<br>9194 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-1:[5 5]<br>9204 [Thread-8] INFO  b.s.d.executor - Loading executor spout1:[26 26]<br>9205 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout1:[26 26]<br>9208 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout1:[26 26]<br>9220 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[10 10]<br>9226 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[10 10]<br>9228 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[10 10]<br>9234 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[14 14]<br>9237 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[14 14]<br>9239 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[14 14]<br>9244 [Thread-8] INFO  b.s.d.executor - Loading executor b-0:[4 4]<br>9248 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-0:[4 4]<br>9249 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-0:[4 4]<br>9255 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[15 15]<br>9260 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[15 15]<br>9261 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[15 15]<br>9273 [Thread-8] INFO  b.s.d.executor - Loading executor spout0:[25 25]<br>9275 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout0:[25 25]<br>9277 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout0:[25 25]<br>9284 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[17 17]<br>9289 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[17 17]<br>9291 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[17 17]<br>9298 [Thread-8] INFO  b.s.d.worker - Worker has topology config {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>9298 [Thread-8] INFO  b.s.d.worker - Worker 9dd8aeac-1cd6-467a-a84c-2637d0825d99 for storm wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 has finished loading<br>9298 [Thread-8] INFO  b.s.config - SET worker-user 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>9875 [Thread-27-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(18)<br>9882 [Thread-35-b-4] INFO  b.s.d.executor - Preparing bolt b-4:(23)<br>9882 [Thread-41-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(16)<br>9883 [Thread-13-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(12)<br>9883 [Thread-59-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(15)<br>9883 [Thread-47-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(21)<br>9893 [Thread-35-b-4] INFO  b.s.d.executor - Prepared bolt b-4:(23)<br>9896 [Thread-47-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(21)<br>9896 [Thread-59-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(15)<br>9896 [Thread-27-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(18)<br>9896 [Thread-13-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(12)<br>9896 [Thread-41-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(16)<br>9898 [Thread-31-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(20)<br>9898 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Preparing bolt $spoutcoord-spout0:(2)<br>9899 [Thread-61-spout0] INFO  b.s.d.executor - Preparing bolt spout0:(25)<br>9900 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9900 [Thread-61-spout0] INFO  b.s.d.executor - Prepared bolt spout0:(25)<br>9901 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9901 [Thread-31-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(20)<br>9907 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9908 [Thread-43-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(13)<br>9908 [Thread-37-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(11)<br>9908 [Thread-63-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(17)<br>9910 [Thread-43-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(13)<br>9910 [Thread-37-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(11)<br>9911 [Thread-63-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(17)<br>9918 [Thread-49-b-1] INFO  b.s.d.executor - Preparing bolt b-1:(5)<br>9918 [Thread-39-</strong>system] INFO  b.s.d.executor - Preparing bolt <strong>system:(-1)<br>9918 [Thread-29-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(6)<br>9920 [Thread-49-b-1] INFO  b.s.d.executor - Prepared bolt b-1:(5)<br>9920 [Thread-29-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(6)<br>9921 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9922 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9924 [Thread-39-</strong>system] INFO  b.s.d.executor - Prepared bolt <strong>system:(-1)<br>9929 [Thread-51-spout1] INFO  b.s.d.executor - Opening spout spout1:(26)<br>9929 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opening spout $mastercoord-bg0:(1)<br>9929 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9938 [Thread-51-spout1] INFO  b.s.d.executor - Opened spout spout1:(26)<br>9937 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9940 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9940 [Thread-33-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(9)<br>9942 [Thread-51-spout1] INFO  b.s.d.executor - Activating spout spout1:(26)<br>9942 [Thread-33-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(9)<br>9947 [Thread-53-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(10)<br>9950 [Thread-53-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(10)<br>9956 [Thread-11-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(8)<br>9956 [Thread-45-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(19)<br>9957 [Thread-23-b-5] INFO  b.s.d.executor - Preparing bolt b-5:(24)<br>9958 [Thread-23-b-5] INFO  b.s.d.executor - Prepared bolt b-5:(24)<br>9958 [Thread-11-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(8)<br>9958 [Thread-17-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(7)<br>9959 [Thread-55-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(14)<br>9959 [Thread-19-b-3] INFO  b.s.d.executor - Preparing bolt b-3:(22)<br>9960 [Thread-19-b-3] INFO  b.s.d.executor - Prepared bolt b-3:(22)<br>9960 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9960 [Thread-17-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(7)<br>9962 [Thread-45-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(19)<br>9963 [Thread-55-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(14)<br>9964 [Thread-57-b-0] INFO  b.s.d.executor - Preparing bolt b-0:(4)<br>9964 [Thread-21-</strong>acker] INFO  b.s.d.executor - Preparing bolt <strong>acker:(3)<br>9965 [Thread-57-b-0] INFO  b.s.d.executor - Prepared bolt b-0:(4)<br>9966 [Thread-21-</strong>acker] INFO  b.s.d.executor - Prepared bolt __acker:(3)<br>9969 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Prepared bolt $spoutcoord-spout0:(2)<br>9971 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9972 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9984 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>DRPC RESULT: [[0]]<br>9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opened spout $mastercoord-bg0:(1)<br>9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Activating spout $mastercoord-bg0:(1)<br>DRPC RESULT: [[60]]<br>DRPC RESULT: [[120]]<br>DRPC RESULT: [[179]]<br>DRPC RESULT: [[239]]<br>DRPC RESULT: [[299]]<br>DRPC RESULT: [[359]]<br>DRPC RESULT: [[414]]<br>DRPC RESULT: [[474]]<br>DRPC RESULT: [[534]]<br>DRPC RESULT: [[593]]<br>DRPC RESULT: [[653]]<br>DRPC RESULT: [[713]]<br>DRPC RESULT: [[768]]</p>\n<p>Process finished with exit code 130</p>\n</blockquote>\n","excerpt":"<h2 id=\"基础环境\"><a href=\"#基础环境\" class=\"headerlink\" title=\"基础环境\"></a>基础环境</h2><ul>\n<li>IDE开发环境：intelliJIdea</li>\n<li>JDK1.7  64bit</li>\n<li>intelliJIdea安装maven插件，配置好仓库源</li>\n<li>intelliJIdea安装clojure插件Cursive（需要注册并获取一个license，否则只能使用30天）</li>\n<li>如果需要自己创建clojure项目进行开发，需要安装leiningen，<a href=\"http://leiningen.org/\">下载地址</a></li>\n</ul>\n<h2 id=\"源码获取\"><a href=\"#源码获取\" class=\"headerlink\" title=\"源码获取\"></a>源码获取</h2><p>从github checkout代码到本地即可，<a href=\"https://github.com/apache/storm.git\">https://github.com/apache/storm.git</a></p>\n<p>我这里编译的是我们目前正在用的0.10.0版本的代码。</p>\n<h2 id=\"导入idea及编译\"><a href=\"#导入idea及编译\" class=\"headerlink\" title=\"导入idea及编译\"></a>导入idea及编译</h2><p>打开idea，新建project，从源码导入，如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/1.png\" alt=\"\"></p>\n<p>导入后，idea会自动根据pom.xml下载相关依赖包，部分依赖包如果下载不到，需要手动添加。完成后，可以看到project的module如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20160713-storm%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E8%B0%83%E8%AF%95/2.png\" alt=\"\"></p>","more":"<p>这时候，通过idea就可以直接跟踪看源码了，但直接运行storm-starter中的例子还是会报错并提示有些类找不到，经查看是clojure的代码还未编译出class文件。可以在源码目录下执行mvn compile进行编译。</p>\n<h2 id=\"使用idea调试源码\"><a href=\"#使用idea调试源码\" class=\"headerlink\" title=\"使用idea调试源码\"></a>使用idea调试源码</h2><p>编译完成后，可以直接启动storm-starter中的例子运行。期间可能出现找不到类，检查classpath，依赖包的scope由provided改为compile。</p>\n<p>在源代码中加断点，run或者debug即可。</p>\n<blockquote>\n<p>2739 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>4546 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>5218 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2000 and dir /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//eeb57be9-5478-4fa9-ab31-6dfce38e7695<br>5243 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources<br>5340 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>5342 [main] INFO  b.s.d.nimbus - Using default scheduler<br>5360 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>5457 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>5529 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>5531 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6569 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6569 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6574 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6605 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6605 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6609 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6609 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6617 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6618 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6620 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6621 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6623 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6625 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6649 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6652 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6653 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6657 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6671 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>6693 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6694 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6697 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6697 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6700 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6701 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6704 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6722 [main] INFO  b.s.d.supervisor - Starting supervisor with id 913c90f6-3f78-4646-8998-aa901ae3c360 at host localhost<br>6725 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {“topology.builtin.metrics.bucket.size.secs” 60, …..<br>6732 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6732 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6736 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6736 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>6740 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>6741 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>6744 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>6753 [main] INFO  b.s.d.supervisor - Starting supervisor with id 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 at host localhost<br>7035 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:submitTopology<br>7113 [main] INFO  b.s.d.nimbus - Received topology submission for wordCounter with conf {“topology.max.task.parallelism” nil, “topology.submitter.principal” “”, “topology.acker.executors” nil, “topology.max.spout.pending” 20, “storm.zookeeper.superACL” nil, “topology.users” (), “topology.submitter.user” “”, “topology.kryo.register” {“storm.trident.topology.TransactionAttempt” nil, “storm.trident.spout.RichSpoutBatchId” “storm.trident.spout.RichSpoutBatchIdSerializer”}, “topology.kryo.decorators” (), “storm.id” “wordCounter-1-1468420782”, “topology.name” “wordCounter”}<br>7123 [main] INFO  b.s.d.nimbus - nimbus file location:/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782<br>7152 [main] INFO  b.s.d.nimbus - Activating wordCounter: wordCounter-1-1468420782<br>7346 [main] INFO  b.s.s.EvenScheduler - Available slots: ([“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1028] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1029] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1024] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1025] [“913c90f6-3f78-4646-8998-aa901ae3c360” 1026])<br>7398 [main] INFO  b.s.d.nimbus - Setting new assignment for topology id wordCounter-1-1468420782: #backtype.storm.daemon.common.Assignment{:master-code-dir “/var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//333ed6da-9ef5-4781-bd82-4f315facd4a8/nimbus/stormdist/wordCounter-1-1468420782”, :node-&gt;host {“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” “localhost”}, :executor-&gt;node+port {[8 8] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [12 12] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [2 2] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [7 7] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [22 22] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [3 3] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [24 24] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [1 1] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [18 18] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [6 6] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [20 20] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [9 9] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [23 23] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [11 11] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [16 16] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [13 13] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [19 19] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [21 21] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [5 5] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [26 26] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [10 10] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [14 14] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [4 4] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [15 15] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [25 25] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027], [17 17] [“49c35a73-7500-4ea4-aaa2-4b1c1f231fd4” 1027]}, :executor-&gt;start-time-secs {[8 8] 1468420782, [12 12] 1468420782, [2 2] 1468420782, [7 7] 1468420782, [22 22] 1468420782, [3 3] 1468420782, [24 24] 1468420782, [1 1] 1468420782, [18 18] 1468420782, [6 6] 1468420782, [20 20] 1468420782, [9 9] 1468420782, [23 23] 1468420782, [11 11] 1468420782, [16 16] 1468420782, [13 13] 1468420782, [19 19] 1468420782, [21 21] 1468420782, [5 5] 1468420782, [26 26] 1468420782, [10 10] 1468420782, [14 14] 1468420782, [4 4] 1468420782, [15 15] 1468420782, [25 25] 1468420782, [17 17] 1468420782}}<br>7751 [Thread-7] INFO  b.s.d.supervisor - Extracting resources from jar at /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/ant-javafx.jar to /var/folders/c0/0bgvmbb10jz1609_1xjqdsj00000gn/T//29645b09-90e9-4b9a-a657-60c418f92841/supervisor/stormdist/wordCounter-1-1468420782/resources<br>7788 [Thread-8] INFO  b.s.d.supervisor - Launching worker with assignment {:storm-id “wordCounter-1-1468420782”, :executors [[8 8] [12 12] [2 2] [7 7] [22 22] [3 3] [24 24] [1 1] [18 18] [6 6] [20 20] [9 9] [23 23] [11 11] [16 16] [13 13] [19 19] [21 21] [5 5] [26 26] [10 10] [14 14] [4 4] [15 15] [25 25] [17 17]]} for this supervisor 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4 on port 1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>7791 [Thread-8] INFO  b.s.d.worker - Launching worker for wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99 and conf {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>7793 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>7794 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>7798 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>7798 [Thread-8-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none<br>7801 [Thread-8] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>7802 [Thread-8] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>7805 [Thread-8-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>7809 [Thread-8] INFO  b.s.s.a.AuthUtils - Got AutoCreds []<br>7811 [Thread-8] INFO  b.s.d.worker - Reading Assignments.<br>7881 [Thread-8] INFO  b.s.d.worker - Launching receive-thread for 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027<br>7884 [Thread-9-worker-receiver-thread-0] INFO  b.s.m.loader - Starting receive-thread: [stormId: wordCounter-1-1468420782, port: 1027, thread-id: 0 ]<br>8261 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[8 8]<br>8285 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[8 8]<br>8300 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[8 8]<br>8311 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[12 12]<br>8329 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[12 12]<br>8331 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[12 12]<br>8340 [Thread-8] INFO  b.s.d.executor - Loading executor $spoutcoord-spout0:[2 2]<br>8343 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $spoutcoord-spout0:[2 2]<br>8346 [Thread-8] INFO  b.s.d.executor - Finished loading executor $spoutcoord-spout0:[2 2]<br>8355 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[7 7]<br>8372 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[7 7]<br>8375 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[7 7]<br>8381 [Thread-8] INFO  b.s.d.executor - Loading executor b-3:[22 22]<br>8401 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-3:[22 22]<br>8404 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-3:[22 22]<br>8412 [Thread-8] INFO  b.s.d.executor - Loading executor <strong>acker:[3 3]<br>8414 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks </strong>acker:[3 3]<br>8424 [Thread-8] INFO  b.s.d.executor - Timeouts disabled for executor <strong>acker:[3 3]<br>8425 [Thread-8] INFO  b.s.d.executor - Finished loading executor </strong>acker:[3 3]<br>8443 [Thread-8] INFO  b.s.d.executor - Loading executor b-5:[24 24]<br>8465 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-5:[24 24]<br>8467 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-5:[24 24]<br>8530 [Thread-8] INFO  b.s.d.executor - Loading executor $mastercoord-bg0:[1 1]<br>8539 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks $mastercoord-bg0:[1 1]<br>8576 [Thread-8] INFO  b.s.d.executor - Finished loading executor $mastercoord-bg0:[1 1]<br>8603 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[18 18]<br>8633 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[18 18]<br>8635 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[18 18]<br>8646 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[6 6]<br>8681 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[6 6]<br>8683 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[6 6]<br>8719 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[20 20]<br>8757 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[20 20]<br>8763 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[20 20]<br>8782 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[9 9]<br>8808 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[9 9]<br>8818 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[9 9]<br>8828 [Thread-8] INFO  b.s.d.executor - Loading executor b-4:[23 23]<br>8847 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-4:[23 23]<br>8851 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-4:[23 23]<br>8858 [refresh-active-timer] INFO  b.s.d.worker - All connections are ready for worker 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 with id 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>8864 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[11 11]<br>8877 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[11 11]<br>8879 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[11 11]<br>8886 [Thread-8] INFO  b.s.d.executor - Loading executor <strong>system:[-1 -1]<br>8887 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks </strong>system:[-1 -1]<br>8890 [Thread-8] INFO  b.s.d.executor - Finished loading executor <strong>system:[-1 -1]<br>8914 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[16 16]<br>9052 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[16 16]<br>9055 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[16 16]<br>9070 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[13 13]<br>9081 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[13 13]<br>9089 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[13 13]<br>9116 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[19 19]<br>9129 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[19 19]<br>9132 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[19 19]<br>9148 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[21 21]<br>9160 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[21 21]<br>9163 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[21 21]<br>9178 [Thread-8] INFO  b.s.d.executor - Loading executor b-1:[5 5]<br>9192 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-1:[5 5]<br>9194 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-1:[5 5]<br>9204 [Thread-8] INFO  b.s.d.executor - Loading executor spout1:[26 26]<br>9205 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout1:[26 26]<br>9208 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout1:[26 26]<br>9220 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[10 10]<br>9226 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[10 10]<br>9228 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[10 10]<br>9234 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[14 14]<br>9237 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[14 14]<br>9239 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[14 14]<br>9244 [Thread-8] INFO  b.s.d.executor - Loading executor b-0:[4 4]<br>9248 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-0:[4 4]<br>9249 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-0:[4 4]<br>9255 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[15 15]<br>9260 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[15 15]<br>9261 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[15 15]<br>9273 [Thread-8] INFO  b.s.d.executor - Loading executor spout0:[25 25]<br>9275 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks spout0:[25 25]<br>9277 [Thread-8] INFO  b.s.d.executor - Finished loading executor spout0:[25 25]<br>9284 [Thread-8] INFO  b.s.d.executor - Loading executor b-2:[17 17]<br>9289 [Thread-8] INFO  b.s.d.executor - Loaded executor tasks b-2:[17 17]<br>9291 [Thread-8] INFO  b.s.d.executor - Finished loading executor b-2:[17 17]<br>9298 [Thread-8] INFO  b.s.d.worker - Worker has topology config {“topology.builtin.metrics.bucket.size.secs” 60, ……<br>9298 [Thread-8] INFO  b.s.d.worker - Worker 9dd8aeac-1cd6-467a-a84c-2637d0825d99 for storm wordCounter-1-1468420782 on 49c35a73-7500-4ea4-aaa2-4b1c1f231fd4:1027 has finished loading<br>9298 [Thread-8] INFO  b.s.config - SET worker-user 9dd8aeac-1cd6-467a-a84c-2637d0825d99<br>9875 [Thread-27-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(18)<br>9882 [Thread-35-b-4] INFO  b.s.d.executor - Preparing bolt b-4:(23)<br>9882 [Thread-41-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(16)<br>9883 [Thread-13-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(12)<br>9883 [Thread-59-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(15)<br>9883 [Thread-47-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(21)<br>9893 [Thread-35-b-4] INFO  b.s.d.executor - Prepared bolt b-4:(23)<br>9896 [Thread-47-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(21)<br>9896 [Thread-59-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(15)<br>9896 [Thread-27-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(18)<br>9896 [Thread-13-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(12)<br>9896 [Thread-41-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(16)<br>9898 [Thread-31-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(20)<br>9898 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Preparing bolt $spoutcoord-spout0:(2)<br>9899 [Thread-61-spout0] INFO  b.s.d.executor - Preparing bolt spout0:(25)<br>9900 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9900 [Thread-61-spout0] INFO  b.s.d.executor - Prepared bolt spout0:(25)<br>9901 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9901 [Thread-31-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(20)<br>9907 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9908 [Thread-43-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(13)<br>9908 [Thread-37-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(11)<br>9908 [Thread-63-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(17)<br>9910 [Thread-43-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(13)<br>9910 [Thread-37-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(11)<br>9911 [Thread-63-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(17)<br>9918 [Thread-49-b-1] INFO  b.s.d.executor - Preparing bolt b-1:(5)<br>9918 [Thread-39-</strong>system] INFO  b.s.d.executor - Preparing bolt <strong>system:(-1)<br>9918 [Thread-29-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(6)<br>9920 [Thread-49-b-1] INFO  b.s.d.executor - Prepared bolt b-1:(5)<br>9920 [Thread-29-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(6)<br>9921 [Thread-15-$spoutcoord-spout0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9922 [Thread-15-$spoutcoord-spout0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9924 [Thread-39-</strong>system] INFO  b.s.d.executor - Prepared bolt <strong>system:(-1)<br>9929 [Thread-51-spout1] INFO  b.s.d.executor - Opening spout spout1:(26)<br>9929 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opening spout $mastercoord-bg0:(1)<br>9929 [Thread-15-$spoutcoord-spout0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9938 [Thread-51-spout1] INFO  b.s.d.executor - Opened spout spout1:(26)<br>9937 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9940 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9940 [Thread-33-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(9)<br>9942 [Thread-51-spout1] INFO  b.s.d.executor - Activating spout spout1:(26)<br>9942 [Thread-33-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(9)<br>9947 [Thread-53-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(10)<br>9950 [Thread-53-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(10)<br>9956 [Thread-11-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(8)<br>9956 [Thread-45-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(19)<br>9957 [Thread-23-b-5] INFO  b.s.d.executor - Preparing bolt b-5:(24)<br>9958 [Thread-23-b-5] INFO  b.s.d.executor - Prepared bolt b-5:(24)<br>9958 [Thread-11-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(8)<br>9958 [Thread-17-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(7)<br>9959 [Thread-55-b-2] INFO  b.s.d.executor - Preparing bolt b-2:(14)<br>9959 [Thread-19-b-3] INFO  b.s.d.executor - Preparing bolt b-3:(22)<br>9960 [Thread-19-b-3] INFO  b.s.d.executor - Prepared bolt b-3:(22)<br>9960 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>9960 [Thread-17-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(7)<br>9962 [Thread-45-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(19)<br>9963 [Thread-55-b-2] INFO  b.s.d.executor - Prepared bolt b-2:(14)<br>9964 [Thread-57-b-0] INFO  b.s.d.executor - Preparing bolt b-0:(4)<br>9964 [Thread-21-</strong>acker] INFO  b.s.d.executor - Preparing bolt <strong>acker:(3)<br>9965 [Thread-57-b-0] INFO  b.s.d.executor - Prepared bolt b-0:(4)<br>9966 [Thread-21-</strong>acker] INFO  b.s.d.executor - Prepared bolt __acker:(3)<br>9969 [Thread-15-$spoutcoord-spout0] INFO  b.s.d.executor - Prepared bolt $spoutcoord-spout0:(2)<br>9971 [Thread-25-$mastercoord-bg0] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]<br>9972 [Thread-25-$mastercoord-bg0] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting<br>9984 [Thread-25-$mastercoord-bg0-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED<br>DRPC RESULT: [[0]]<br>9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Opened spout $mastercoord-bg0:(1)<br>9988 [Thread-25-$mastercoord-bg0] INFO  b.s.d.executor - Activating spout $mastercoord-bg0:(1)<br>DRPC RESULT: [[60]]<br>DRPC RESULT: [[120]]<br>DRPC RESULT: [[179]]<br>DRPC RESULT: [[239]]<br>DRPC RESULT: [[299]]<br>DRPC RESULT: [[359]]<br>DRPC RESULT: [[414]]<br>DRPC RESULT: [[474]]<br>DRPC RESULT: [[534]]<br>DRPC RESULT: [[593]]<br>DRPC RESULT: [[653]]<br>DRPC RESULT: [[713]]<br>DRPC RESULT: [[768]]</p>\n<p>Process finished with exit code 130</p>\n</blockquote>"},{"title":"redis服务端连接断开问题诊断","date":"2014-06-01T13:29:28.000Z","_content":"\n问题现象\n--\n\n前段时间，由于线上redis服务器的内存使用率达到了机器总内存的50%以上，导致内存数据的dump持久化一直失败。扩展到多台redis后，应用系统访问redis时，在业务量较少时，时不时会出现以下异常，当业务量较大，redis访问频率很高时，却不会发生这个异常，一时觉得很诡异。\n\n> redis.clients.jedis.exceptions.JedisConnectionException: It seems like server has closed the connection.\n> at redis.clients.util.RedisInputStream.readLine(RedisInputStream.java:90) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.processInteger(Protocol.java:110) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.process(Protocol.java:70) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.read(Protocol.java:131) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Connection.getIntegerReply(Connection.java:188) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Jedis.sismember(Jedis.java:1266) ~[jedis-2.1.0.jar:na]\n\n看提示，应该是服务端主动关闭了连接。查看了新上线的redis服务器的配置，有这么一项：\n\n> \\# Close the connection after a client is idle for N seconds (0 to disable)\n> timeout 120\n\n这项配置指的是客户端连接空闲超过多少秒后，服务端主动关闭连接，默认值0表示服务端永远不主动关闭。而op人员把服务器端的超时时间设置为了120秒。\n\n这就解释了发生这个异常的原因。客户端使用了一个连接池管理访问redis的所有连接，这些连接是长连接，当业务量较小时，客户端部分连接使用率较低，当两次使用之间的间隔超过120秒时，redis服务端就主动关闭了这个连接，而等客户端下次再使用这个连接对象时，发现服务端已经关闭了连接，进而报错。\n\n于是，再查看访问redis的系统（客户端）的配置：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/1.png)\n\n客户端使用的是jedis内置的连接池，看其源码本质上是基于apache commons-pool实现的，其中有一个eviction线程，用于回收idle对象，对于redis连接池来说，也就是回收空闲连接。\n\nJedisPoolConfig类继承自GenericObjectPoolConfig并覆盖了几项关于eviction线程的配置，具体如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/2.png)\n\n*<font color=red>_timeBetweenEvictionRunsMillis</font>*：eviction线程的运行周期。默认是-1，表示不启动eviction线程。这里设置为30秒。\n\n*<font color=red>_minEvictableIdleTimeMillis</font>*：对象处于idle状态的最长时间，默认是30分钟，这里设置为60秒。\n\n通过客户端的默认配置看，对象的最大空闲时长是小于服务端的配置的，应该不是配置上的问题了。\n\n于是，继续看是不是客户端代码使用上的问题。追踪到客户端代码如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/3.png)\n\n可见，客户端首先尝试从本线程的ThreadLocal对象中获取jedis对象，若获取不到，再从masterJedisPool中取得jedis对象并放入ThreadLocal对象以便下次使用，并且jedis对象使用完毕后，没有从ThreadLocal中清除，也没有returnResource给masterJedisPool。\n\n因此，问题产生的原因就在于此。ThreadLocal中的这个jedis对象被取出后没有return，对于对象池来说是处于非idle状态，因此不会被对象池evict。<font color=red>当业务量大时，这个jedis会被频繁使用，服务端认为这个jedis对应的连接是非空闲的，或者空闲时间达不到120秒，不会主动关闭，所以没什么问题。然而当业务量小时，这个jedis使用频率很低，当两次之间的使用间隔超出120秒时，服务端会主动把这个jedis的连接关闭，第二次调用时，就会出现上面的报错。</font>\n\n从代码开发者的角度来说，这么做的目的是避免频繁从pool中获取jedis对象和return jedis对象以提高性能。\n\n解决方案有两个：\n\n1. 在redis-cli下在线修改redis 的配置，把timeout改回为0，无需重启redis即可直接生效，但redis若重启，配置会恢复。\n\n2. 修改客户端代码，使用完jedis对象后，从ThreadLocal中清除，再返回给连接池。\n\n出于改动成本考虑，先采用了第一种方案，在线修改redis配置后，报错不再出现。","source":"_posts/redis服务端连接断开问题诊断.md","raw":"---\ntitle: redis服务端连接断开问题诊断\ndate: 2014-06-01 21:29:28\ntags:\n- redis\n- 连接断开\ncategories: \n- Redis\n---\n\n问题现象\n--\n\n前段时间，由于线上redis服务器的内存使用率达到了机器总内存的50%以上，导致内存数据的dump持久化一直失败。扩展到多台redis后，应用系统访问redis时，在业务量较少时，时不时会出现以下异常，当业务量较大，redis访问频率很高时，却不会发生这个异常，一时觉得很诡异。\n\n> redis.clients.jedis.exceptions.JedisConnectionException: It seems like server has closed the connection.\n> at redis.clients.util.RedisInputStream.readLine(RedisInputStream.java:90) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.processInteger(Protocol.java:110) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.process(Protocol.java:70) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Protocol.read(Protocol.java:131) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Connection.getIntegerReply(Connection.java:188) ~[jedis-2.1.0.jar:na]\n> at redis.clients.jedis.Jedis.sismember(Jedis.java:1266) ~[jedis-2.1.0.jar:na]\n\n看提示，应该是服务端主动关闭了连接。查看了新上线的redis服务器的配置，有这么一项：\n\n> \\# Close the connection after a client is idle for N seconds (0 to disable)\n> timeout 120\n\n这项配置指的是客户端连接空闲超过多少秒后，服务端主动关闭连接，默认值0表示服务端永远不主动关闭。而op人员把服务器端的超时时间设置为了120秒。\n\n这就解释了发生这个异常的原因。客户端使用了一个连接池管理访问redis的所有连接，这些连接是长连接，当业务量较小时，客户端部分连接使用率较低，当两次使用之间的间隔超过120秒时，redis服务端就主动关闭了这个连接，而等客户端下次再使用这个连接对象时，发现服务端已经关闭了连接，进而报错。\n\n于是，再查看访问redis的系统（客户端）的配置：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/1.png)\n\n客户端使用的是jedis内置的连接池，看其源码本质上是基于apache commons-pool实现的，其中有一个eviction线程，用于回收idle对象，对于redis连接池来说，也就是回收空闲连接。\n\nJedisPoolConfig类继承自GenericObjectPoolConfig并覆盖了几项关于eviction线程的配置，具体如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/2.png)\n\n*<font color=red>_timeBetweenEvictionRunsMillis</font>*：eviction线程的运行周期。默认是-1，表示不启动eviction线程。这里设置为30秒。\n\n*<font color=red>_minEvictableIdleTimeMillis</font>*：对象处于idle状态的最长时间，默认是30分钟，这里设置为60秒。\n\n通过客户端的默认配置看，对象的最大空闲时长是小于服务端的配置的，应该不是配置上的问题了。\n\n于是，继续看是不是客户端代码使用上的问题。追踪到客户端代码如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/3.png)\n\n可见，客户端首先尝试从本线程的ThreadLocal对象中获取jedis对象，若获取不到，再从masterJedisPool中取得jedis对象并放入ThreadLocal对象以便下次使用，并且jedis对象使用完毕后，没有从ThreadLocal中清除，也没有returnResource给masterJedisPool。\n\n因此，问题产生的原因就在于此。ThreadLocal中的这个jedis对象被取出后没有return，对于对象池来说是处于非idle状态，因此不会被对象池evict。<font color=red>当业务量大时，这个jedis会被频繁使用，服务端认为这个jedis对应的连接是非空闲的，或者空闲时间达不到120秒，不会主动关闭，所以没什么问题。然而当业务量小时，这个jedis使用频率很低，当两次之间的使用间隔超出120秒时，服务端会主动把这个jedis的连接关闭，第二次调用时，就会出现上面的报错。</font>\n\n从代码开发者的角度来说，这么做的目的是避免频繁从pool中获取jedis对象和return jedis对象以提高性能。\n\n解决方案有两个：\n\n1. 在redis-cli下在线修改redis 的配置，把timeout改回为0，无需重启redis即可直接生效，但redis若重启，配置会恢复。\n\n2. 修改客户端代码，使用完jedis对象后，从ThreadLocal中清除，再返回给连接池。\n\n出于改动成本考虑，先采用了第一种方案，在线修改redis配置后，报错不再出现。","slug":"redis服务端连接断开问题诊断","published":1,"updated":"2017-01-18T09:54:11.735Z","_id":"ciy320aj5000hifs6ql2q06q0","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>前段时间，由于线上redis服务器的内存使用率达到了机器总内存的50%以上，导致内存数据的dump持久化一直失败。扩展到多台redis后，应用系统访问redis时，在业务量较少时，时不时会出现以下异常，当业务量较大，redis访问频率很高时，却不会发生这个异常，一时觉得很诡异。</p>\n<blockquote>\n<p>redis.clients.jedis.exceptions.JedisConnectionException: It seems like server has closed the connection.<br>at redis.clients.util.RedisInputStream.readLine(RedisInputStream.java:90) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.processInteger(Protocol.java:110) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.process(Protocol.java:70) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.read(Protocol.java:131) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Connection.getIntegerReply(Connection.java:188) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Jedis.sismember(Jedis.java:1266) ~[jedis-2.1.0.jar:na]</p>\n</blockquote>\n<p>看提示，应该是服务端主动关闭了连接。查看了新上线的redis服务器的配置，有这么一项：</p>\n<blockquote>\n<p># Close the connection after a client is idle for N seconds (0 to disable)<br>timeout 120</p>\n</blockquote>\n<p>这项配置指的是客户端连接空闲超过多少秒后，服务端主动关闭连接，默认值0表示服务端永远不主动关闭。而op人员把服务器端的超时时间设置为了120秒。</p>\n<p>这就解释了发生这个异常的原因。客户端使用了一个连接池管理访问redis的所有连接，这些连接是长连接，当业务量较小时，客户端部分连接使用率较低，当两次使用之间的间隔超过120秒时，redis服务端就主动关闭了这个连接，而等客户端下次再使用这个连接对象时，发现服务端已经关闭了连接，进而报错。</p>\n<p>于是，再查看访问redis的系统（客户端）的配置：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/1.png\" alt=\"\"></p>\n<p>客户端使用的是jedis内置的连接池，看其源码本质上是基于apache commons-pool实现的，其中有一个eviction线程，用于回收idle对象，对于redis连接池来说，也就是回收空闲连接。</p>\n<p>JedisPoolConfig类继承自GenericObjectPoolConfig并覆盖了几项关于eviction线程的配置，具体如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/2.png\" alt=\"\"></p>\n<p><em><font color=\"red\">_timeBetweenEvictionRunsMillis</font></em>：eviction线程的运行周期。默认是-1，表示不启动eviction线程。这里设置为30秒。</p>\n<p><em><font color=\"red\">_minEvictableIdleTimeMillis</font></em>：对象处于idle状态的最长时间，默认是30分钟，这里设置为60秒。</p>\n<p>通过客户端的默认配置看，对象的最大空闲时长是小于服务端的配置的，应该不是配置上的问题了。</p>\n<p>于是，继续看是不是客户端代码使用上的问题。追踪到客户端代码如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/3.png\" alt=\"\"></p>\n<p>可见，客户端首先尝试从本线程的ThreadLocal对象中获取jedis对象，若获取不到，再从masterJedisPool中取得jedis对象并放入ThreadLocal对象以便下次使用，并且jedis对象使用完毕后，没有从ThreadLocal中清除，也没有returnResource给masterJedisPool。</p>\n<p>因此，问题产生的原因就在于此。ThreadLocal中的这个jedis对象被取出后没有return，对于对象池来说是处于非idle状态，因此不会被对象池evict。<font color=\"red\">当业务量大时，这个jedis会被频繁使用，服务端认为这个jedis对应的连接是非空闲的，或者空闲时间达不到120秒，不会主动关闭，所以没什么问题。然而当业务量小时，这个jedis使用频率很低，当两次之间的使用间隔超出120秒时，服务端会主动把这个jedis的连接关闭，第二次调用时，就会出现上面的报错。</font></p>\n<p>从代码开发者的角度来说，这么做的目的是避免频繁从pool中获取jedis对象和return jedis对象以提高性能。</p>\n<p>解决方案有两个：</p>\n<ol>\n<li><p>在redis-cli下在线修改redis 的配置，把timeout改回为0，无需重启redis即可直接生效，但redis若重启，配置会恢复。</p>\n</li>\n<li><p>修改客户端代码，使用完jedis对象后，从ThreadLocal中清除，再返回给连接池。</p>\n</li>\n</ol>\n<p>出于改动成本考虑，先采用了第一种方案，在线修改redis配置后，报错不再出现。</p>\n","excerpt":"","more":"<h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>前段时间，由于线上redis服务器的内存使用率达到了机器总内存的50%以上，导致内存数据的dump持久化一直失败。扩展到多台redis后，应用系统访问redis时，在业务量较少时，时不时会出现以下异常，当业务量较大，redis访问频率很高时，却不会发生这个异常，一时觉得很诡异。</p>\n<blockquote>\n<p>redis.clients.jedis.exceptions.JedisConnectionException: It seems like server has closed the connection.<br>at redis.clients.util.RedisInputStream.readLine(RedisInputStream.java:90) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.processInteger(Protocol.java:110) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.process(Protocol.java:70) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Protocol.read(Protocol.java:131) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Connection.getIntegerReply(Connection.java:188) ~[jedis-2.1.0.jar:na]<br>at redis.clients.jedis.Jedis.sismember(Jedis.java:1266) ~[jedis-2.1.0.jar:na]</p>\n</blockquote>\n<p>看提示，应该是服务端主动关闭了连接。查看了新上线的redis服务器的配置，有这么一项：</p>\n<blockquote>\n<p># Close the connection after a client is idle for N seconds (0 to disable)<br>timeout 120</p>\n</blockquote>\n<p>这项配置指的是客户端连接空闲超过多少秒后，服务端主动关闭连接，默认值0表示服务端永远不主动关闭。而op人员把服务器端的超时时间设置为了120秒。</p>\n<p>这就解释了发生这个异常的原因。客户端使用了一个连接池管理访问redis的所有连接，这些连接是长连接，当业务量较小时，客户端部分连接使用率较低，当两次使用之间的间隔超过120秒时，redis服务端就主动关闭了这个连接，而等客户端下次再使用这个连接对象时，发现服务端已经关闭了连接，进而报错。</p>\n<p>于是，再查看访问redis的系统（客户端）的配置：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/1.png\" alt=\"\"></p>\n<p>客户端使用的是jedis内置的连接池，看其源码本质上是基于apache commons-pool实现的，其中有一个eviction线程，用于回收idle对象，对于redis连接池来说，也就是回收空闲连接。</p>\n<p>JedisPoolConfig类继承自GenericObjectPoolConfig并覆盖了几项关于eviction线程的配置，具体如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/2.png\" alt=\"\"></p>\n<p><em><font color=red>_timeBetweenEvictionRunsMillis</font></em>：eviction线程的运行周期。默认是-1，表示不启动eviction线程。这里设置为30秒。</p>\n<p><em><font color=red>_minEvictableIdleTimeMillis</font></em>：对象处于idle状态的最长时间，默认是30分钟，这里设置为60秒。</p>\n<p>通过客户端的默认配置看，对象的最大空闲时长是小于服务端的配置的，应该不是配置上的问题了。</p>\n<p>于是，继续看是不是客户端代码使用上的问题。追踪到客户端代码如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140601-redis%E8%BF%9E%E6%8E%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/3.png\" alt=\"\"></p>\n<p>可见，客户端首先尝试从本线程的ThreadLocal对象中获取jedis对象，若获取不到，再从masterJedisPool中取得jedis对象并放入ThreadLocal对象以便下次使用，并且jedis对象使用完毕后，没有从ThreadLocal中清除，也没有returnResource给masterJedisPool。</p>\n<p>因此，问题产生的原因就在于此。ThreadLocal中的这个jedis对象被取出后没有return，对于对象池来说是处于非idle状态，因此不会被对象池evict。<font color=red>当业务量大时，这个jedis会被频繁使用，服务端认为这个jedis对应的连接是非空闲的，或者空闲时间达不到120秒，不会主动关闭，所以没什么问题。然而当业务量小时，这个jedis使用频率很低，当两次之间的使用间隔超出120秒时，服务端会主动把这个jedis的连接关闭，第二次调用时，就会出现上面的报错。</font></p>\n<p>从代码开发者的角度来说，这么做的目的是避免频繁从pool中获取jedis对象和return jedis对象以提高性能。</p>\n<p>解决方案有两个：</p>\n<ol>\n<li><p>在redis-cli下在线修改redis 的配置，把timeout改回为0，无需重启redis即可直接生效，但redis若重启，配置会恢复。</p>\n</li>\n<li><p>修改客户端代码，使用完jedis对象后，从ThreadLocal中清除，再返回给连接池。</p>\n</li>\n</ol>\n<p>出于改动成本考虑，先采用了第一种方案，在线修改redis配置后，报错不再出现。</p>\n"},{"title":"使用hexo+gitpage搭建博客","date":"2014-09-02T11:50:40.000Z","_content":"\n环境准备\n--\n系统：mac osx  \n软件：Node.js，npm，git，hexo  \n具体安装以及git与github打通的配置就不详述了，可以google到各种方法。  \n\nhexo命令\n--\nhexo init &lt;folder&gt;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #表示执行init命令初始化hexo到你指定的目录  \n<font color=\"red\">以下命令需要在&lt;folder&gt;目录下执行：</font>  \nhexo generate  &nbsp;&nbsp;&nbsp;#自动根据当前目录下文件,生成静态网页  \nhexo server &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#运行本地服务  \n  \n启动服务后，就可以通过访问 http://localhost:4000 来看看效果了。  \n接下来，可以使用以下命令来创建一篇新博文：  \nhexo new \"test blog 1\"  \n创建一个名为test blog 1的博客页面，对应的md文件路径是&lt;folder&gt;/source/_posts\\test blog 1.md  \n  \n接下来就可以在这个md文件中写文章了，我使用的是MacDown来编辑md文件，支持实时查看页面效果，还是挺好用的。\n\n发布博客\n--\n文章写好后，通过以下方式发布到github上。  \n1.编辑./_config.yml文件，修改以下部分，配置本地内容同步至github：  \n\n>  deploy:  \n>  &nbsp;&nbsp;type: git  \n>  &nbsp;&nbsp;repository: git@github.com:maohong/maohong.github.io.git  \n>  &nbsp;&nbsp;branch: master  \n\n2.执行hexo generate(hexo g)生成html内容  \n3.执行hexo deploy(hexo d)讲更新内容发布至guthub  \n  \n然后就可以访问主页查看效果了，可以使用github帐户名.github.io进行访问, 也可以设置个性域名。\n","source":"_posts/使用hexo-gitpage搭建博客.md","raw":"---\ntitle: 使用hexo+gitpage搭建博客\ndate: 2014-09-02 19:50:40\ntags: \n- hexo\n- gitpage\ncategories: \n- 工具\n---\n\n环境准备\n--\n系统：mac osx  \n软件：Node.js，npm，git，hexo  \n具体安装以及git与github打通的配置就不详述了，可以google到各种方法。  \n\nhexo命令\n--\nhexo init &lt;folder&gt;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #表示执行init命令初始化hexo到你指定的目录  \n<font color=\"red\">以下命令需要在&lt;folder&gt;目录下执行：</font>  \nhexo generate  &nbsp;&nbsp;&nbsp;#自动根据当前目录下文件,生成静态网页  \nhexo server &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#运行本地服务  \n  \n启动服务后，就可以通过访问 http://localhost:4000 来看看效果了。  \n接下来，可以使用以下命令来创建一篇新博文：  \nhexo new \"test blog 1\"  \n创建一个名为test blog 1的博客页面，对应的md文件路径是&lt;folder&gt;/source/_posts\\test blog 1.md  \n  \n接下来就可以在这个md文件中写文章了，我使用的是MacDown来编辑md文件，支持实时查看页面效果，还是挺好用的。\n\n发布博客\n--\n文章写好后，通过以下方式发布到github上。  \n1.编辑./_config.yml文件，修改以下部分，配置本地内容同步至github：  \n\n>  deploy:  \n>  &nbsp;&nbsp;type: git  \n>  &nbsp;&nbsp;repository: git@github.com:maohong/maohong.github.io.git  \n>  &nbsp;&nbsp;branch: master  \n\n2.执行hexo generate(hexo g)生成html内容  \n3.执行hexo deploy(hexo d)讲更新内容发布至guthub  \n  \n然后就可以访问主页查看效果了，可以使用github帐户名.github.io进行访问, 也可以设置个性域名。\n","slug":"使用hexo-gitpage搭建博客","published":1,"updated":"2017-01-18T09:54:11.736Z","_id":"ciy320aj7000lifs6yuyf0aty","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"环境准备\"><a href=\"#环境准备\" class=\"headerlink\" title=\"环境准备\"></a>环境准备</h2><p>系统：mac osx<br>软件：Node.js，npm，git，hexo<br>具体安装以及git与github打通的配置就不详述了，可以google到各种方法。  </p>\n<h2 id=\"hexo命令\"><a href=\"#hexo命令\" class=\"headerlink\" title=\"hexo命令\"></a>hexo命令</h2><p>hexo init &lt;folder&gt;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #表示执行init命令初始化hexo到你指定的目录  </p>\n<p><font color=\"red\">以下命令需要在&lt;folder&gt;目录下执行：</font><br>hexo generate  &nbsp;&nbsp;&nbsp;#自动根据当前目录下文件,生成静态网页<br>hexo server &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#运行本地服务  </p>\n<p>启动服务后，就可以通过访问 <a href=\"http://localhost:4000\" target=\"_blank\" rel=\"external\">http://localhost:4000</a> 来看看效果了。<br>接下来，可以使用以下命令来创建一篇新博文：<br>hexo new “test blog 1”<br>创建一个名为test blog 1的博客页面，对应的md文件路径是&lt;folder&gt;/source/_posts\\test blog 1.md  </p>\n<p>接下来就可以在这个md文件中写文章了，我使用的是MacDown来编辑md文件，支持实时查看页面效果，还是挺好用的。</p>\n<h2 id=\"发布博客\"><a href=\"#发布博客\" class=\"headerlink\" title=\"发布博客\"></a>发布博客</h2><p>文章写好后，通过以下方式发布到github上。<br>1.编辑./_config.yml文件，修改以下部分，配置本地内容同步至github：  </p>\n<blockquote>\n<p> deploy:<br> &nbsp;&nbsp;type: git<br> &nbsp;&nbsp;repository: git@github.com:maohong/maohong.github.io.git<br> &nbsp;&nbsp;branch: master  </p>\n</blockquote>\n<p>2.执行hexo generate(hexo g)生成html内容<br>3.执行hexo deploy(hexo d)讲更新内容发布至guthub  </p>\n<p>然后就可以访问主页查看效果了，可以使用github帐户名.github.io进行访问, 也可以设置个性域名。</p>\n","excerpt":"","more":"<h2 id=\"环境准备\"><a href=\"#环境准备\" class=\"headerlink\" title=\"环境准备\"></a>环境准备</h2><p>系统：mac osx<br>软件：Node.js，npm，git，hexo<br>具体安装以及git与github打通的配置就不详述了，可以google到各种方法。  </p>\n<h2 id=\"hexo命令\"><a href=\"#hexo命令\" class=\"headerlink\" title=\"hexo命令\"></a>hexo命令</h2><p>hexo init &lt;folder&gt;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #表示执行init命令初始化hexo到你指定的目录  </p>\n<p><font color=\"red\">以下命令需要在&lt;folder&gt;目录下执行：</font><br>hexo generate  &nbsp;&nbsp;&nbsp;#自动根据当前目录下文件,生成静态网页<br>hexo server &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#运行本地服务  </p>\n<p>启动服务后，就可以通过访问 <a href=\"http://localhost:4000\">http://localhost:4000</a> 来看看效果了。<br>接下来，可以使用以下命令来创建一篇新博文：<br>hexo new “test blog 1”<br>创建一个名为test blog 1的博客页面，对应的md文件路径是&lt;folder&gt;/source/_posts\\test blog 1.md  </p>\n<p>接下来就可以在这个md文件中写文章了，我使用的是MacDown来编辑md文件，支持实时查看页面效果，还是挺好用的。</p>\n<h2 id=\"发布博客\"><a href=\"#发布博客\" class=\"headerlink\" title=\"发布博客\"></a>发布博客</h2><p>文章写好后，通过以下方式发布到github上。<br>1.编辑./_config.yml文件，修改以下部分，配置本地内容同步至github：  </p>\n<blockquote>\n<p> deploy:<br> &nbsp;&nbsp;type: git<br> &nbsp;&nbsp;repository: git@github.com:maohong/maohong.github.io.git<br> &nbsp;&nbsp;branch: master  </p>\n</blockquote>\n<p>2.执行hexo generate(hexo g)生成html内容<br>3.执行hexo deploy(hexo d)讲更新内容发布至guthub  </p>\n<p>然后就可以访问主页查看效果了，可以使用github帐户名.github.io进行访问, 也可以设置个性域名。</p>\n"},{"title":"使用shell切割文件","date":"2013-03-07T05:49:44.000Z","_content":"\n最近工作中需要使用shell，从远程rsync数据过来预处理后提交到hdfs中，再调用pig脚本在hadoop集群上处理数据，完了fs -get下来结果文件并进行进一步处理，再推送给其他系统使用。其间需要将pig作业的结果文件合并并且均分为10个文件推送给远程服务器上的应用加载。因为结果文件比较大，远程应用拿到结果文件后使用多线程加载，所以需均分为10个小文件。虽然mr作业出来的文件结果也是part-00000、part-00001，但若pig脚本中不指定reduce任务数，产生的结果文件个数是3个，而且下下来之后需要进行重命名。与其这样还不如自己处理。\n\n```java\nrm -rf $TODAY_ALL_INDUSKEY\n\tfor allName in `find $TODAY_ALL_TMP_DIR -name \"part-*\"`\n\t\tdo\n\t\t\tINFO \"Processing result file\" $allName\n\t\t\tcat $allName >> $TODAY_ALL_INDUSKEY   #把结果文件重定向到一个文件\n\tdone\n\n\tALL_INDUSKEY_FILE_NUM=10    #拆分的文件数量\n\tALL_KEY_LINES=0             #结果文件行数\n\tINFO \"Split $TODAY_ALL_INDUSKEY into $ALL_INDUSKEY_FILE_NUM files\"\n\tfor str in `wc -l $TODAY_ALL_INDUSKEY`;\tdo\n\t\tt=`expr match $str \"[1-9][0-9]*$\"`;\n\t\tif [ $t -gt 0 ]; then\n\t\t\tALL_KEY_LINES=$str         #获取结果文件行数\n\t\t\tINFO \"Line of $TODAY_ALL_INDUSKEY is $ALL_KEY_LINES\"\n\t\tfi\n\tdone\n\tif [ $ALL_KEY_LINES -ne 0 ]; then\n\t\ttmpLine=`echo \"scale=2;$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM\"|bc`    #每个小文件的行数，保留两位小数\n\t\tINFO \"$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM=$tmpLine\"\n\t\tsubFileLines=`echo $((${tmpLine//.*/+1}))`        #向上取整\n\t\tINFO \"Per subfile lines:$subFileLines\"\n\t\tsplit -l $subFileLines -a 1 -d $TODAY_ALL_INDUSKEY $TODAY_ALL_INDUSKEY\"_\"      #拆分文件\n\tfi\n\n\tif [ -f $TODAY_ALL_INDUSKEY ]; then\n\t\ttouch $TODAY_ALL_INDUSKEY.done       #创建done文件\n\t\trm -rf $TODAY_ALL_TMP_DIR\n\t\tINFO \"Process result file dir $TODAY_ALL_TMP_DIR done!\"\n\tfi\n```\n\n","source":"_posts/使用shell切割文件.md","raw":"---\ntitle: 使用shell切割文件\ndate: 2013-03-07 13:49:44\ntags:\n- shell\n- linux\n---\n\n最近工作中需要使用shell，从远程rsync数据过来预处理后提交到hdfs中，再调用pig脚本在hadoop集群上处理数据，完了fs -get下来结果文件并进行进一步处理，再推送给其他系统使用。其间需要将pig作业的结果文件合并并且均分为10个文件推送给远程服务器上的应用加载。因为结果文件比较大，远程应用拿到结果文件后使用多线程加载，所以需均分为10个小文件。虽然mr作业出来的文件结果也是part-00000、part-00001，但若pig脚本中不指定reduce任务数，产生的结果文件个数是3个，而且下下来之后需要进行重命名。与其这样还不如自己处理。\n\n```java\nrm -rf $TODAY_ALL_INDUSKEY\n\tfor allName in `find $TODAY_ALL_TMP_DIR -name \"part-*\"`\n\t\tdo\n\t\t\tINFO \"Processing result file\" $allName\n\t\t\tcat $allName >> $TODAY_ALL_INDUSKEY   #把结果文件重定向到一个文件\n\tdone\n\n\tALL_INDUSKEY_FILE_NUM=10    #拆分的文件数量\n\tALL_KEY_LINES=0             #结果文件行数\n\tINFO \"Split $TODAY_ALL_INDUSKEY into $ALL_INDUSKEY_FILE_NUM files\"\n\tfor str in `wc -l $TODAY_ALL_INDUSKEY`;\tdo\n\t\tt=`expr match $str \"[1-9][0-9]*$\"`;\n\t\tif [ $t -gt 0 ]; then\n\t\t\tALL_KEY_LINES=$str         #获取结果文件行数\n\t\t\tINFO \"Line of $TODAY_ALL_INDUSKEY is $ALL_KEY_LINES\"\n\t\tfi\n\tdone\n\tif [ $ALL_KEY_LINES -ne 0 ]; then\n\t\ttmpLine=`echo \"scale=2;$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM\"|bc`    #每个小文件的行数，保留两位小数\n\t\tINFO \"$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM=$tmpLine\"\n\t\tsubFileLines=`echo $((${tmpLine//.*/+1}))`        #向上取整\n\t\tINFO \"Per subfile lines:$subFileLines\"\n\t\tsplit -l $subFileLines -a 1 -d $TODAY_ALL_INDUSKEY $TODAY_ALL_INDUSKEY\"_\"      #拆分文件\n\tfi\n\n\tif [ -f $TODAY_ALL_INDUSKEY ]; then\n\t\ttouch $TODAY_ALL_INDUSKEY.done       #创建done文件\n\t\trm -rf $TODAY_ALL_TMP_DIR\n\t\tINFO \"Process result file dir $TODAY_ALL_TMP_DIR done!\"\n\tfi\n```\n\n","slug":"使用shell切割文件","published":1,"updated":"2017-01-18T09:54:11.737Z","_id":"ciy320aj8000oifs6cxiq9le2","comments":1,"layout":"post","photos":[],"link":"","content":"<p>最近工作中需要使用shell，从远程rsync数据过来预处理后提交到hdfs中，再调用pig脚本在hadoop集群上处理数据，完了fs -get下来结果文件并进行进一步处理，再推送给其他系统使用。其间需要将pig作业的结果文件合并并且均分为10个文件推送给远程服务器上的应用加载。因为结果文件比较大，远程应用拿到结果文件后使用多线程加载，所以需均分为10个小文件。虽然mr作业出来的文件结果也是part-00000、part-00001，但若pig脚本中不指定reduce任务数，产生的结果文件个数是3个，而且下下来之后需要进行重命名。与其这样还不如自己处理。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">rm -rf $TODAY_ALL_INDUSKEY</div><div class=\"line\">\tfor allName in `find $TODAY_ALL_TMP_DIR -name \"part-*\"`</div><div class=\"line\">\t\tdo</div><div class=\"line\">\t\t\tINFO \"Processing result file\" $allName</div><div class=\"line\">\t\t\tcat $allName &gt;&gt; $TODAY_ALL_INDUSKEY   #把结果文件重定向到一个文件</div><div class=\"line\">\tdone</div><div class=\"line\"></div><div class=\"line\">\tALL_INDUSKEY_FILE_NUM=10    #拆分的文件数量</div><div class=\"line\">\tALL_KEY_LINES=0             #结果文件行数</div><div class=\"line\">\tINFO \"Split $TODAY_ALL_INDUSKEY into $ALL_INDUSKEY_FILE_NUM files\"</div><div class=\"line\">\tfor str in `wc -l $TODAY_ALL_INDUSKEY`;\tdo</div><div class=\"line\">\t\tt=`expr match $str \"[1-9][0-9]*$\"`;</div><div class=\"line\">\t\tif [ $t -gt 0 ]; then</div><div class=\"line\">\t\t\tALL_KEY_LINES=$str         #获取结果文件行数</div><div class=\"line\">\t\t\tINFO \"Line of $TODAY_ALL_INDUSKEY is $ALL_KEY_LINES\"</div><div class=\"line\">\t\tfi</div><div class=\"line\">\tdone</div><div class=\"line\">\tif [ $ALL_KEY_LINES -ne 0 ]; then</div><div class=\"line\">\t\ttmpLine=`echo \"scale=2;$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM\"|bc`    #每个小文件的行数，保留两位小数</div><div class=\"line\">\t\tINFO \"$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM=$tmpLine\"</div><div class=\"line\">\t\tsubFileLines=`echo $(($&#123;tmpLine//.*/+1&#125;))`        #向上取整</div><div class=\"line\">\t\tINFO \"Per subfile lines:$subFileLines\"</div><div class=\"line\">\t\tsplit -l $subFileLines -a 1 -d $TODAY_ALL_INDUSKEY $TODAY_ALL_INDUSKEY\"_\"      #拆分文件</div><div class=\"line\">\tfi</div><div class=\"line\"></div><div class=\"line\">\tif [ -f $TODAY_ALL_INDUSKEY ]; then</div><div class=\"line\">\t\ttouch $TODAY_ALL_INDUSKEY.done       #创建done文件</div><div class=\"line\">\t\trm -rf $TODAY_ALL_TMP_DIR</div><div class=\"line\">\t\tINFO \"Process result file dir $TODAY_ALL_TMP_DIR done!\"</div><div class=\"line\">\tfi</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<p>最近工作中需要使用shell，从远程rsync数据过来预处理后提交到hdfs中，再调用pig脚本在hadoop集群上处理数据，完了fs -get下来结果文件并进行进一步处理，再推送给其他系统使用。其间需要将pig作业的结果文件合并并且均分为10个文件推送给远程服务器上的应用加载。因为结果文件比较大，远程应用拿到结果文件后使用多线程加载，所以需均分为10个小文件。虽然mr作业出来的文件结果也是part-00000、part-00001，但若pig脚本中不指定reduce任务数，产生的结果文件个数是3个，而且下下来之后需要进行重命名。与其这样还不如自己处理。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">rm -rf $TODAY_ALL_INDUSKEY</div><div class=\"line\">\tfor allName in `find $TODAY_ALL_TMP_DIR -name \"part-*\"`</div><div class=\"line\">\t\tdo</div><div class=\"line\">\t\t\tINFO \"Processing result file\" $allName</div><div class=\"line\">\t\t\tcat $allName &gt;&gt; $TODAY_ALL_INDUSKEY   #把结果文件重定向到一个文件</div><div class=\"line\">\tdone</div><div class=\"line\"></div><div class=\"line\">\tALL_INDUSKEY_FILE_NUM=10    #拆分的文件数量</div><div class=\"line\">\tALL_KEY_LINES=0             #结果文件行数</div><div class=\"line\">\tINFO \"Split $TODAY_ALL_INDUSKEY into $ALL_INDUSKEY_FILE_NUM files\"</div><div class=\"line\">\tfor str in `wc -l $TODAY_ALL_INDUSKEY`;\tdo</div><div class=\"line\">\t\tt=`expr match $str \"[1-9][0-9]*$\"`;</div><div class=\"line\">\t\tif [ $t -gt 0 ]; then</div><div class=\"line\">\t\t\tALL_KEY_LINES=$str         #获取结果文件行数</div><div class=\"line\">\t\t\tINFO \"Line of $TODAY_ALL_INDUSKEY is $ALL_KEY_LINES\"</div><div class=\"line\">\t\tfi</div><div class=\"line\">\tdone</div><div class=\"line\">\tif [ $ALL_KEY_LINES -ne 0 ]; then</div><div class=\"line\">\t\ttmpLine=`echo \"scale=2;$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM\"|bc`    #每个小文件的行数，保留两位小数</div><div class=\"line\">\t\tINFO \"$ALL_KEY_LINES/$ALL_INDUSKEY_FILE_NUM=$tmpLine\"</div><div class=\"line\">\t\tsubFileLines=`echo $(($&#123;tmpLine//.*/+1&#125;))`        #向上取整</div><div class=\"line\">\t\tINFO \"Per subfile lines:$subFileLines\"</div><div class=\"line\">\t\tsplit -l $subFileLines -a 1 -d $TODAY_ALL_INDUSKEY $TODAY_ALL_INDUSKEY\"_\"      #拆分文件</div><div class=\"line\">\tfi</div><div class=\"line\"></div><div class=\"line\">\tif [ -f $TODAY_ALL_INDUSKEY ]; then</div><div class=\"line\">\t\ttouch $TODAY_ALL_INDUSKEY.done       #创建done文件</div><div class=\"line\">\t\trm -rf $TODAY_ALL_TMP_DIR</div><div class=\"line\">\t\tINFO \"Process result file dir $TODAY_ALL_TMP_DIR done!\"</div><div class=\"line\">\tfi</div></pre></td></tr></table></figure>\n"},{"title":"使用httpclient引起的tcp连接数超高问题","date":"2014-03-28T09:44:58.000Z","_content":"\n组内的一个系统新上线了通过图片url上传图片到图片存储平台的功能。其中使用了httpclient，通过向图片存储平台发送MultipartPostMethod上传图片。当业务量较大时，10个处理线程满负荷运行，上传图片时，发现应用系统服务器的tcp连接数陡然升高，<font color='red'>峰值能达到几万个tcp连接数！</font>\n\n排查系统代码并结合分析httpclient的源码发现，应用系统每次上传图片时，都会做new HttpClient()操作，这个操作内部默认使用的是SimpleHttpConnectionManager来管理http连接，而SimpleHttpConnectionManager有个默认字段alwaysClose=false，表示当外部程序调用了HttpMethod.releaseConnection()时并不会立即释放连接，而是保持这个连接并尝试用于后续的请求，在连接空闲一段时间后（默认3秒）才真正释放。\n\n因此，当业务量较大，<font color='red'>系统高并发发送post请求时，new出来的HttpClient对象会很多，而这个对象使用完毕后，而当中建立的client对象在短时间内并不会立即释放连接</font>，因此，随着时间的积累，tcp连接数保持居高不下。\n\n通过查看官方文档，建议在高并发环境下使用MultiThreadedHttpConnectionManager来管理httpclient，因此，我们将httpclient改为单例后，tcp连接数回复正常水平。\n\n通过管理httpclient的代码如下：\n\n```java\nprivate static HttpClient initHttpClient()\n{\n    HttpConnectionManagerParams params = new HttpConnectionManagerParams();\n    //指定向每个host发起的最大连接数，默认是2，太少了\n    params.setDefaultMaxConnectionsPerHost(1000);\n    //指定总共发起的最大连接数，默认是20，太少了\n    params.setMaxTotalConnections(5000);\n    //连接超时时间-10s\n    params.setConnectionTimeout(60*1000);\n    //读取数据超时时间-60s\n    params.setSoTimeout(60*1000);\n \n    MultiThreadedHttpConnectionManager manager = new MultiThreadedHttpConnectionManager();\n    manager.setParams(params);\n    return new HttpClient(manager);\n}\n```","source":"_posts/使用httpclient引起的tcp连接数超高问题.md","raw":"---\ntitle: 使用httpclient引起的tcp连接数超高问题\ndate: 2014-03-28 17:44:58\ntags:\n- httpclient\n- tcp连接数\ncategories: \n- 问题分析\n---\n\n组内的一个系统新上线了通过图片url上传图片到图片存储平台的功能。其中使用了httpclient，通过向图片存储平台发送MultipartPostMethod上传图片。当业务量较大时，10个处理线程满负荷运行，上传图片时，发现应用系统服务器的tcp连接数陡然升高，<font color='red'>峰值能达到几万个tcp连接数！</font>\n\n排查系统代码并结合分析httpclient的源码发现，应用系统每次上传图片时，都会做new HttpClient()操作，这个操作内部默认使用的是SimpleHttpConnectionManager来管理http连接，而SimpleHttpConnectionManager有个默认字段alwaysClose=false，表示当外部程序调用了HttpMethod.releaseConnection()时并不会立即释放连接，而是保持这个连接并尝试用于后续的请求，在连接空闲一段时间后（默认3秒）才真正释放。\n\n因此，当业务量较大，<font color='red'>系统高并发发送post请求时，new出来的HttpClient对象会很多，而这个对象使用完毕后，而当中建立的client对象在短时间内并不会立即释放连接</font>，因此，随着时间的积累，tcp连接数保持居高不下。\n\n通过查看官方文档，建议在高并发环境下使用MultiThreadedHttpConnectionManager来管理httpclient，因此，我们将httpclient改为单例后，tcp连接数回复正常水平。\n\n通过管理httpclient的代码如下：\n\n```java\nprivate static HttpClient initHttpClient()\n{\n    HttpConnectionManagerParams params = new HttpConnectionManagerParams();\n    //指定向每个host发起的最大连接数，默认是2，太少了\n    params.setDefaultMaxConnectionsPerHost(1000);\n    //指定总共发起的最大连接数，默认是20，太少了\n    params.setMaxTotalConnections(5000);\n    //连接超时时间-10s\n    params.setConnectionTimeout(60*1000);\n    //读取数据超时时间-60s\n    params.setSoTimeout(60*1000);\n \n    MultiThreadedHttpConnectionManager manager = new MultiThreadedHttpConnectionManager();\n    manager.setParams(params);\n    return new HttpClient(manager);\n}\n```","slug":"使用httpclient引起的tcp连接数超高问题","published":1,"updated":"2017-01-18T09:54:11.736Z","_id":"ciy320ajb000tifs6xajsgejs","comments":1,"layout":"post","photos":[],"link":"","content":"<p>组内的一个系统新上线了通过图片url上传图片到图片存储平台的功能。其中使用了httpclient，通过向图片存储平台发送MultipartPostMethod上传图片。当业务量较大时，10个处理线程满负荷运行，上传图片时，发现应用系统服务器的tcp连接数陡然升高，<font color=\"red\">峰值能达到几万个tcp连接数！</font></p>\n<p>排查系统代码并结合分析httpclient的源码发现，应用系统每次上传图片时，都会做new HttpClient()操作，这个操作内部默认使用的是SimpleHttpConnectionManager来管理http连接，而SimpleHttpConnectionManager有个默认字段alwaysClose=false，表示当外部程序调用了HttpMethod.releaseConnection()时并不会立即释放连接，而是保持这个连接并尝试用于后续的请求，在连接空闲一段时间后（默认3秒）才真正释放。</p>\n<p>因此，当业务量较大，<font color=\"red\">系统高并发发送post请求时，new出来的HttpClient对象会很多，而这个对象使用完毕后，而当中建立的client对象在短时间内并不会立即释放连接</font>，因此，随着时间的积累，tcp连接数保持居高不下。</p>\n<p>通过查看官方文档，建议在高并发环境下使用MultiThreadedHttpConnectionManager来管理httpclient，因此，我们将httpclient改为单例后，tcp连接数回复正常水平。</p>\n<p>通过管理httpclient的代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> HttpClient <span class=\"title\">initHttpClient</span><span class=\"params\">()</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    HttpConnectionManagerParams params = <span class=\"keyword\">new</span> HttpConnectionManagerParams();</div><div class=\"line\">    <span class=\"comment\">//指定向每个host发起的最大连接数，默认是2，太少了</span></div><div class=\"line\">    params.setDefaultMaxConnectionsPerHost(<span class=\"number\">1000</span>);</div><div class=\"line\">    <span class=\"comment\">//指定总共发起的最大连接数，默认是20，太少了</span></div><div class=\"line\">    params.setMaxTotalConnections(<span class=\"number\">5000</span>);</div><div class=\"line\">    <span class=\"comment\">//连接超时时间-10s</span></div><div class=\"line\">    params.setConnectionTimeout(<span class=\"number\">60</span>*<span class=\"number\">1000</span>);</div><div class=\"line\">    <span class=\"comment\">//读取数据超时时间-60s</span></div><div class=\"line\">    params.setSoTimeout(<span class=\"number\">60</span>*<span class=\"number\">1000</span>);</div><div class=\"line\"> </div><div class=\"line\">    MultiThreadedHttpConnectionManager manager = <span class=\"keyword\">new</span> MultiThreadedHttpConnectionManager();</div><div class=\"line\">    manager.setParams(params);</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> HttpClient(manager);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>","excerpt":"","more":"<p>组内的一个系统新上线了通过图片url上传图片到图片存储平台的功能。其中使用了httpclient，通过向图片存储平台发送MultipartPostMethod上传图片。当业务量较大时，10个处理线程满负荷运行，上传图片时，发现应用系统服务器的tcp连接数陡然升高，<font color='red'>峰值能达到几万个tcp连接数！</font></p>\n<p>排查系统代码并结合分析httpclient的源码发现，应用系统每次上传图片时，都会做new HttpClient()操作，这个操作内部默认使用的是SimpleHttpConnectionManager来管理http连接，而SimpleHttpConnectionManager有个默认字段alwaysClose=false，表示当外部程序调用了HttpMethod.releaseConnection()时并不会立即释放连接，而是保持这个连接并尝试用于后续的请求，在连接空闲一段时间后（默认3秒）才真正释放。</p>\n<p>因此，当业务量较大，<font color='red'>系统高并发发送post请求时，new出来的HttpClient对象会很多，而这个对象使用完毕后，而当中建立的client对象在短时间内并不会立即释放连接</font>，因此，随着时间的积累，tcp连接数保持居高不下。</p>\n<p>通过查看官方文档，建议在高并发环境下使用MultiThreadedHttpConnectionManager来管理httpclient，因此，我们将httpclient改为单例后，tcp连接数回复正常水平。</p>\n<p>通过管理httpclient的代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> HttpClient <span class=\"title\">initHttpClient</span><span class=\"params\">()</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    HttpConnectionManagerParams params = <span class=\"keyword\">new</span> HttpConnectionManagerParams();</div><div class=\"line\">    <span class=\"comment\">//指定向每个host发起的最大连接数，默认是2，太少了</span></div><div class=\"line\">    params.setDefaultMaxConnectionsPerHost(<span class=\"number\">1000</span>);</div><div class=\"line\">    <span class=\"comment\">//指定总共发起的最大连接数，默认是20，太少了</span></div><div class=\"line\">    params.setMaxTotalConnections(<span class=\"number\">5000</span>);</div><div class=\"line\">    <span class=\"comment\">//连接超时时间-10s</span></div><div class=\"line\">    params.setConnectionTimeout(<span class=\"number\">60</span>*<span class=\"number\">1000</span>);</div><div class=\"line\">    <span class=\"comment\">//读取数据超时时间-60s</span></div><div class=\"line\">    params.setSoTimeout(<span class=\"number\">60</span>*<span class=\"number\">1000</span>);</div><div class=\"line\"> </div><div class=\"line\">    MultiThreadedHttpConnectionManager manager = <span class=\"keyword\">new</span> MultiThreadedHttpConnectionManager();</div><div class=\"line\">    manager.setParams(params);</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> HttpClient(manager);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"交换空间使用率过高问题分析","date":"2015-06-22T02:02:04.000Z","_content":"\n问题现象\n--\n\n线上两台java后台服务每次上线后再过段时间，就出现swap空间使用率较高的现象，而jvm内存使用和垃圾回收情况则很正常。相关图表如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/1.png)\n\n图中，每次上线后过一段时间，swap空间使用量会出现一个陡增，并随时间推移逐渐增加，期间会出现小幅度下降。\n\n首先，从操作系统层面分析，swap空间使用较高，说明是系统物理内存不够用从而发生内存页交换，将部分内存数据搬至虚拟内存空间，也就是swap空间。但究竟是什么原因引起物理内存不足呢？因为Jvm堆大小是固定的，所以推断是因堆外内存占用空间较大引起。\n\n于是，使用jmap -histo:live <pid>把进程中的对象信息dump出来，dump信息如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/2.png)\n\n确实发现存在大量DirectByteBuffer对象，这说明内存中确实有大量引用了堆外内存的对象没有被回收！\n\n同时，内存中也对应存在着大量的sun.misc.Cleaner和java.nio.DirectByteBuffer$Deallocator对象。这两个类是用于回收堆外内存的。Cleaner对象是在DirectByteBuffer的构造函数中创建，其中封装了回收堆外内存的逻辑，Cleaner执行clean资源的操作是通过启动Deallocator线程实现的，这个线程把DirectByteBuffer对象引用的堆外内存做回收。\n\n 那么问题来了：\n\n1. 为什么DirectByteBuffer对象没有被回收？\n\n2. 怎么做才能让DirectByteBuffer对象能被及时回收？\n\n问题分析\n--\n<!--more-->\n先看了下启动jvm参数为-Xmn8192M -Xms13312M -Xmx13312M -XX:PermSize=512m -XX:MaxPermSize=512m，很明显，新生代空间配的太大，同时，也没有指定堆外内存的最大空间（-XX:MaxDirectMemorySize），这个参数没设置则默认等于-Xmx，然而服务器总内存只有16G，所以时间长了很可能会发生堆外内存溢出！\n\n因为此服务是kafka集群的消费者，每天接收的报文量在1亿以上，这个过程中产生了大量的DirectByteBuffer对象，这些对象直接引用堆外内存，而同时，这些临时对象也会被回收，由于新生代空间配的很大，触发minor GC的频率不够高，从而不能及时释放已被占用的堆外内存，随着时间的推移，进程启动过一段时间后，堆外内存占用越来越多，最终被OS交换到swap空间。\n\n解决方案\n--\n\n调整jvm参数，减少新生代大小为jvm堆空间的3/8，并指定堆外内存大小，调整后的jvm参数为-Xmn3840M -Xms10240M -Xmx10240M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m\n\n调整后，swap空间占用情况有所好转，但依然占用2G左右！如下图所示。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/3.png)\n\n4月21日调整参数重启服务后，在相当长的一段时间内，swap空间占用率极低，但在5月2日又出现swap空间使用率上升的情况。继续看了下jvm堆空间使用情况和full gc情况，如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/4.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/5.png)\n\n结合上面两张图，可见young gc较多，jvm堆空间整体使用率稳步上升，在5月2日与5月8日发生了两次full gc，并且每次发生fullgc后，jvm堆空间使用率下降较多，swap空间使用量只有小范围下降。这说明有一部分DirectByteBuffer对象在fullgc阶段做了回收，但依然有很多DirectByteBuffer对象没有被回收，仍然占用着堆外内存。\n\n选择一台机器，继续减小其堆空间，jvm参数为-Xmn2048M -Xms6144M -Xmx6144M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m，经过一段时间观察，交换空间使用率很低，应该没再发生内存页交换了，同时gc频率变高，jvm堆空间的使用率在正常范围，说明DirectByteBuffer对象被更及时的回收了。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/6.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/7.png)\n\n<font color=red>由此可见，swap空间占用率高的原因主要还是JVM堆空间太高导致的堆外内存回收不及时。</font>\n\n遗留问题\n--\n\n看了下kafka-client的源码，接受消息时使用的是ByteBuffer，并没有使用DirectByteBuffer，所以很奇怪，这些大量的DirectByteBuffer对象是从哪生成的？哪里用到的？\n\n运行命令jmap -dump:live,format=b,file=/data/server.dump <pid>，dump出内存快照，并用eclipse mat分析后，发现是zkclient中的一个地方用的，由于dump出的这个快照是问题解决后的内存快照，所以并不能说明问题，如果要找到根本原因，还是需要复现swap空间过高的场景，再做内存快照的dump。\n\n\n","source":"_posts/交换空间使用率过高问题分析.md","raw":"---\ntitle: 交换空间使用率过高问题分析\ndate: 2015-06-22 10:02:04\ntags:\n- swap分区\n- 问题分析\n- Jvm调优\ncategories:\n- 问题分析\n---\n\n问题现象\n--\n\n线上两台java后台服务每次上线后再过段时间，就出现swap空间使用率较高的现象，而jvm内存使用和垃圾回收情况则很正常。相关图表如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/1.png)\n\n图中，每次上线后过一段时间，swap空间使用量会出现一个陡增，并随时间推移逐渐增加，期间会出现小幅度下降。\n\n首先，从操作系统层面分析，swap空间使用较高，说明是系统物理内存不够用从而发生内存页交换，将部分内存数据搬至虚拟内存空间，也就是swap空间。但究竟是什么原因引起物理内存不足呢？因为Jvm堆大小是固定的，所以推断是因堆外内存占用空间较大引起。\n\n于是，使用jmap -histo:live <pid>把进程中的对象信息dump出来，dump信息如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/2.png)\n\n确实发现存在大量DirectByteBuffer对象，这说明内存中确实有大量引用了堆外内存的对象没有被回收！\n\n同时，内存中也对应存在着大量的sun.misc.Cleaner和java.nio.DirectByteBuffer$Deallocator对象。这两个类是用于回收堆外内存的。Cleaner对象是在DirectByteBuffer的构造函数中创建，其中封装了回收堆外内存的逻辑，Cleaner执行clean资源的操作是通过启动Deallocator线程实现的，这个线程把DirectByteBuffer对象引用的堆外内存做回收。\n\n 那么问题来了：\n\n1. 为什么DirectByteBuffer对象没有被回收？\n\n2. 怎么做才能让DirectByteBuffer对象能被及时回收？\n\n问题分析\n--\n<!--more-->\n先看了下启动jvm参数为-Xmn8192M -Xms13312M -Xmx13312M -XX:PermSize=512m -XX:MaxPermSize=512m，很明显，新生代空间配的太大，同时，也没有指定堆外内存的最大空间（-XX:MaxDirectMemorySize），这个参数没设置则默认等于-Xmx，然而服务器总内存只有16G，所以时间长了很可能会发生堆外内存溢出！\n\n因为此服务是kafka集群的消费者，每天接收的报文量在1亿以上，这个过程中产生了大量的DirectByteBuffer对象，这些对象直接引用堆外内存，而同时，这些临时对象也会被回收，由于新生代空间配的很大，触发minor GC的频率不够高，从而不能及时释放已被占用的堆外内存，随着时间的推移，进程启动过一段时间后，堆外内存占用越来越多，最终被OS交换到swap空间。\n\n解决方案\n--\n\n调整jvm参数，减少新生代大小为jvm堆空间的3/8，并指定堆外内存大小，调整后的jvm参数为-Xmn3840M -Xms10240M -Xmx10240M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m\n\n调整后，swap空间占用情况有所好转，但依然占用2G左右！如下图所示。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/3.png)\n\n4月21日调整参数重启服务后，在相当长的一段时间内，swap空间占用率极低，但在5月2日又出现swap空间使用率上升的情况。继续看了下jvm堆空间使用情况和full gc情况，如下：\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/4.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/5.png)\n\n结合上面两张图，可见young gc较多，jvm堆空间整体使用率稳步上升，在5月2日与5月8日发生了两次full gc，并且每次发生fullgc后，jvm堆空间使用率下降较多，swap空间使用量只有小范围下降。这说明有一部分DirectByteBuffer对象在fullgc阶段做了回收，但依然有很多DirectByteBuffer对象没有被回收，仍然占用着堆外内存。\n\n选择一台机器，继续减小其堆空间，jvm参数为-Xmn2048M -Xms6144M -Xmx6144M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m，经过一段时间观察，交换空间使用率很低，应该没再发生内存页交换了，同时gc频率变高，jvm堆空间的使用率在正常范围，说明DirectByteBuffer对象被更及时的回收了。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/6.png)\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/7.png)\n\n<font color=red>由此可见，swap空间占用率高的原因主要还是JVM堆空间太高导致的堆外内存回收不及时。</font>\n\n遗留问题\n--\n\n看了下kafka-client的源码，接受消息时使用的是ByteBuffer，并没有使用DirectByteBuffer，所以很奇怪，这些大量的DirectByteBuffer对象是从哪生成的？哪里用到的？\n\n运行命令jmap -dump:live,format=b,file=/data/server.dump <pid>，dump出内存快照，并用eclipse mat分析后，发现是zkclient中的一个地方用的，由于dump出的这个快照是问题解决后的内存快照，所以并不能说明问题，如果要找到根本原因，还是需要复现swap空间过高的场景，再做内存快照的dump。\n\n\n","slug":"交换空间使用率过高问题分析","published":1,"updated":"2017-01-18T09:54:11.736Z","_id":"ciy320ajd000vifs6kpl1n75r","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>线上两台java后台服务每次上线后再过段时间，就出现swap空间使用率较高的现象，而jvm内存使用和垃圾回收情况则很正常。相关图表如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/1.png\" alt=\"\"></p>\n<p>图中，每次上线后过一段时间，swap空间使用量会出现一个陡增，并随时间推移逐渐增加，期间会出现小幅度下降。</p>\n<p>首先，从操作系统层面分析，swap空间使用较高，说明是系统物理内存不够用从而发生内存页交换，将部分内存数据搬至虚拟内存空间，也就是swap空间。但究竟是什么原因引起物理内存不足呢？因为Jvm堆大小是固定的，所以推断是因堆外内存占用空间较大引起。</p>\n<p>于是，使用jmap -histo:live <pid>把进程中的对象信息dump出来，dump信息如下：</pid></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/2.png\" alt=\"\"></p>\n<p>确实发现存在大量DirectByteBuffer对象，这说明内存中确实有大量引用了堆外内存的对象没有被回收！</p>\n<p>同时，内存中也对应存在着大量的sun.misc.Cleaner和java.nio.DirectByteBuffer$Deallocator对象。这两个类是用于回收堆外内存的。Cleaner对象是在DirectByteBuffer的构造函数中创建，其中封装了回收堆外内存的逻辑，Cleaner执行clean资源的操作是通过启动Deallocator线程实现的，这个线程把DirectByteBuffer对象引用的堆外内存做回收。</p>\n<p> 那么问题来了：</p>\n<ol>\n<li><p>为什么DirectByteBuffer对象没有被回收？</p>\n</li>\n<li><p>怎么做才能让DirectByteBuffer对象能被及时回收？</p>\n</li>\n</ol>\n<h2 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h2><a id=\"more\"></a>\n<p>先看了下启动jvm参数为-Xmn8192M -Xms13312M -Xmx13312M -XX:PermSize=512m -XX:MaxPermSize=512m，很明显，新生代空间配的太大，同时，也没有指定堆外内存的最大空间（-XX:MaxDirectMemorySize），这个参数没设置则默认等于-Xmx，然而服务器总内存只有16G，所以时间长了很可能会发生堆外内存溢出！</p>\n<p>因为此服务是kafka集群的消费者，每天接收的报文量在1亿以上，这个过程中产生了大量的DirectByteBuffer对象，这些对象直接引用堆外内存，而同时，这些临时对象也会被回收，由于新生代空间配的很大，触发minor GC的频率不够高，从而不能及时释放已被占用的堆外内存，随着时间的推移，进程启动过一段时间后，堆外内存占用越来越多，最终被OS交换到swap空间。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>调整jvm参数，减少新生代大小为jvm堆空间的3/8，并指定堆外内存大小，调整后的jvm参数为-Xmn3840M -Xms10240M -Xmx10240M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m</p>\n<p>调整后，swap空间占用情况有所好转，但依然占用2G左右！如下图所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/3.png\" alt=\"\"></p>\n<p>4月21日调整参数重启服务后，在相当长的一段时间内，swap空间占用率极低，但在5月2日又出现swap空间使用率上升的情况。继续看了下jvm堆空间使用情况和full gc情况，如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/4.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/5.png\" alt=\"\"></p>\n<p>结合上面两张图，可见young gc较多，jvm堆空间整体使用率稳步上升，在5月2日与5月8日发生了两次full gc，并且每次发生fullgc后，jvm堆空间使用率下降较多，swap空间使用量只有小范围下降。这说明有一部分DirectByteBuffer对象在fullgc阶段做了回收，但依然有很多DirectByteBuffer对象没有被回收，仍然占用着堆外内存。</p>\n<p>选择一台机器，继续减小其堆空间，jvm参数为-Xmn2048M -Xms6144M -Xmx6144M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m，经过一段时间观察，交换空间使用率很低，应该没再发生内存页交换了，同时gc频率变高，jvm堆空间的使用率在正常范围，说明DirectByteBuffer对象被更及时的回收了。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/6.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/7.png\" alt=\"\"></p>\n<font color=\"red\">由此可见，swap空间占用率高的原因主要还是JVM堆空间太高导致的堆外内存回收不及时。</font>\n\n<h2 id=\"遗留问题\"><a href=\"#遗留问题\" class=\"headerlink\" title=\"遗留问题\"></a>遗留问题</h2><p>看了下kafka-client的源码，接受消息时使用的是ByteBuffer，并没有使用DirectByteBuffer，所以很奇怪，这些大量的DirectByteBuffer对象是从哪生成的？哪里用到的？</p>\n<p>运行命令jmap -dump:live,format=b,file=/data/server.dump <pid>，dump出内存快照，并用eclipse mat分析后，发现是zkclient中的一个地方用的，由于dump出的这个快照是问题解决后的内存快照，所以并不能说明问题，如果要找到根本原因，还是需要复现swap空间过高的场景，再做内存快照的dump。</pid></p>\n","excerpt":"<h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>线上两台java后台服务每次上线后再过段时间，就出现swap空间使用率较高的现象，而jvm内存使用和垃圾回收情况则很正常。相关图表如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/1.png\" alt=\"\"></p>\n<p>图中，每次上线后过一段时间，swap空间使用量会出现一个陡增，并随时间推移逐渐增加，期间会出现小幅度下降。</p>\n<p>首先，从操作系统层面分析，swap空间使用较高，说明是系统物理内存不够用从而发生内存页交换，将部分内存数据搬至虚拟内存空间，也就是swap空间。但究竟是什么原因引起物理内存不足呢？因为Jvm堆大小是固定的，所以推断是因堆外内存占用空间较大引起。</p>\n<p>于是，使用jmap -histo:live <pid>把进程中的对象信息dump出来，dump信息如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/2.png\" alt=\"\"></p>\n<p>确实发现存在大量DirectByteBuffer对象，这说明内存中确实有大量引用了堆外内存的对象没有被回收！</p>\n<p>同时，内存中也对应存在着大量的sun.misc.Cleaner和java.nio.DirectByteBuffer$Deallocator对象。这两个类是用于回收堆外内存的。Cleaner对象是在DirectByteBuffer的构造函数中创建，其中封装了回收堆外内存的逻辑，Cleaner执行clean资源的操作是通过启动Deallocator线程实现的，这个线程把DirectByteBuffer对象引用的堆外内存做回收。</p>\n<p> 那么问题来了：</p>\n<ol>\n<li><p>为什么DirectByteBuffer对象没有被回收？</p>\n</li>\n<li><p>怎么做才能让DirectByteBuffer对象能被及时回收？</p>\n</li>\n</ol>\n<h2 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h2>","more":"<p>先看了下启动jvm参数为-Xmn8192M -Xms13312M -Xmx13312M -XX:PermSize=512m -XX:MaxPermSize=512m，很明显，新生代空间配的太大，同时，也没有指定堆外内存的最大空间（-XX:MaxDirectMemorySize），这个参数没设置则默认等于-Xmx，然而服务器总内存只有16G，所以时间长了很可能会发生堆外内存溢出！</p>\n<p>因为此服务是kafka集群的消费者，每天接收的报文量在1亿以上，这个过程中产生了大量的DirectByteBuffer对象，这些对象直接引用堆外内存，而同时，这些临时对象也会被回收，由于新生代空间配的很大，触发minor GC的频率不够高，从而不能及时释放已被占用的堆外内存，随着时间的推移，进程启动过一段时间后，堆外内存占用越来越多，最终被OS交换到swap空间。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>调整jvm参数，减少新生代大小为jvm堆空间的3/8，并指定堆外内存大小，调整后的jvm参数为-Xmn3840M -Xms10240M -Xmx10240M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m</p>\n<p>调整后，swap空间占用情况有所好转，但依然占用2G左右！如下图所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/3.png\" alt=\"\"></p>\n<p>4月21日调整参数重启服务后，在相当长的一段时间内，swap空间占用率极低，但在5月2日又出现swap空间使用率上升的情况。继续看了下jvm堆空间使用情况和full gc情况，如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/4.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/5.png\" alt=\"\"></p>\n<p>结合上面两张图，可见young gc较多，jvm堆空间整体使用率稳步上升，在5月2日与5月8日发生了两次full gc，并且每次发生fullgc后，jvm堆空间使用率下降较多，swap空间使用量只有小范围下降。这说明有一部分DirectByteBuffer对象在fullgc阶段做了回收，但依然有很多DirectByteBuffer对象没有被回收，仍然占用着堆外内存。</p>\n<p>选择一台机器，继续减小其堆空间，jvm参数为-Xmn2048M -Xms6144M -Xmx6144M -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxDirectMemorySize=4096m，经过一段时间观察，交换空间使用率很低，应该没再发生内存页交换了，同时gc频率变高，jvm堆空间的使用率在正常范围，说明DirectByteBuffer对象被更及时的回收了。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/6.png\" alt=\"\"></p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20150622%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4%E4%BD%BF%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/7.png\" alt=\"\"></p>\n<font color=red>由此可见，swap空间占用率高的原因主要还是JVM堆空间太高导致的堆外内存回收不及时。</font>\n\n<h2 id=\"遗留问题\"><a href=\"#遗留问题\" class=\"headerlink\" title=\"遗留问题\"></a>遗留问题</h2><p>看了下kafka-client的源码，接受消息时使用的是ByteBuffer，并没有使用DirectByteBuffer，所以很奇怪，这些大量的DirectByteBuffer对象是从哪生成的？哪里用到的？</p>\n<p>运行命令jmap -dump:live,format=b,file=/data/server.dump <pid>，dump出内存快照，并用eclipse mat分析后，发现是zkclient中的一个地方用的，由于dump出的这个快照是问题解决后的内存快照，所以并不能说明问题，如果要找到根本原因，还是需要复现swap空间过高的场景，再做内存快照的dump。</p>"},{"title":"基于zookeeper的分布式独占锁实现","date":"2014-05-13T13:05:01.000Z","_content":"\n背景\n--\n\n在分布式系统中，经常遇到这样一种场景：选举一个节点执行某一个任务，当此节点宕机后，其他节点可以接管并继续执行这个任务。由于各个节点运行的代码是一样的，彼此之间也是平等的，各个节点如何可以知道自己是否可以执行这个任务呢？当有节点宕机时，又如何判断自己是否可以接管任务呢？在我们的分布式任务调度系统中，需要选取调度器集群中的一个节点进行轮询任务状态，这里使用了zookeeper来实现一个统一的分布式锁，从而选出轮询节点。\n\n原理\n--\n\n如图所示，每台服务器启动后，都在同一目录下建一个临时顺序节点（EPHEMERAL_SEQUENTIAL），并获取此目录下的所有节点信息，如果自己的序号是最小的，就认为获取到了锁，可以执行任务。若自己的节点不是最小的，就认为自己没有获取到锁，不执行任务，同时，在比自己小1个序号的节点上增加监听。当比自己小1个序号的节点发生变化的时候，再次检查自己是否是最小序号的节点，如果是则获取锁，否则继续监听比自己小1个序号的节点。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140502-%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8B%AC%E5%8D%A0%E9%94%81%E5%AE%9E%E7%8E%B0/1.jpg)\n\n实现\n--\n\n以下是一个demo实现程序：\n\n```java\npublic class DistributedExclusiveLock implements Watcher\n{\n\tprivate ZooKeeper zk;\n\tprivate String lockDir = \"/testlock\";//锁节点所在zk的目录\n\tprivate String lockSymbol = \"_lock_\";//锁节点标志\n\tprivate String lockName;//锁节点前缀，构造锁时由外部传入\n\tprivate String waitNodePath;//等待的前一个锁的节点名称\n\tprivate String myNodePath;//当前锁\n\tprivate CountDownLatch latch;//计数器\n\tprivate String threadId;\n\n\t/**\n\t * 创建分布式锁\n\t * @param lockName 竞争资源标志,lockName中不能包含单词lock\n\t * @throws Exception\n\t */\n\tpublic DistributedExclusiveLock(String zkServers, String lockName) throws Exception\n\t{\n\t\t//简单校验lockDir路径\n\t\tif (!lockDir.startsWith(\"/\"))\n\t\t\tthrow new Exception(\"LockDir Path must start with / character! lockDir=\" + lockDir);\n\t\tif (lockDir.endsWith(\"/\"))\n\t\t\tthrow new Exception(\"LockDir Path must not end with / character! lockDir=\" + lockDir);\n\n\t\tthis.lockName = lockName;\n\t\tthis.threadId = getThreadId();\n\t\t// 创建一个与服务器的连接\n\t\ttry\n\t\t{\n\t\t\tzk = new ZooKeeper(zkServers, 3000, this);\n\t\t\tcreateLockDirIfNecessary(lockDir);\n\t\t} catch (Exception e) {\n\t\t\tthrow new Exception(\"Error while initializing DistributedExclusiveLock!\" + e.getMessage(), e);\n\t\t}\n\t}\n\n\tprivate String getThreadId()\n\t{\n\t\treturn \"Thread-\" + Thread.currentThread().getId();\n\t}\n\n\t/**\n\t * 在zk上建立lock目录，如果目录不存在，逐级创建节点\n\t */\n\tprivate synchronized void createLockDirIfNecessary(String zkDir) throws KeeperException, InterruptedException\n\t{\n\t\t//zkDir是一级节点，如/cloudscheduler\n\t\tif (zkDir.indexOf(\"/\") == zkDir.lastIndexOf(\"/\"))\n\t\t{\n\t\t\tStat stat = zk.exists(zkDir, false);\n\t\t\tif(stat == null){\n\t\t\t\t// 创建一级节点\n\t\t\t\tzk.create(zkDir, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t}\n\t\t}\n\t\telse\t//zkDir非一级节点\n\t\t{\n\t\t\tString parentDir = zkDir.substring(0, zkDir.lastIndexOf(\"/\"));\n\t\t\tif (zk.exists(parentDir, false) != null)\n\t\t\t{\t//如果父节点存在，建当前节点\n\t\t\t\tStat stat = zk.exists(zkDir, false);\n\t\t\t\tif(stat == null){\n\t\t\t\t\t// 创建非一级节点\n\t\t\t\t\tzk.create(zkDir, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\t//否则，先建父节点，再建当前节点\n\t\t\t\tcreateLockDirIfNecessary(parentDir);\n\t\t\t\tcreateLockDirIfNecessary(zkDir);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * zookeeper节点的监视器\n\t */\n\t@Override\n\tpublic void process(WatchedEvent event)\n\t{\n\t\tif (event.getType() == EventType.NodeDeleted)\n\t\t{\n\t\t\tif (this.latch!=null)\n\t\t\t\tthis.latch.countDown();\n\t\t\ttry\n\t\t\t{\n\t\t\t\tList<String> childrenNodes = zk.getChildren(lockDir, false);\n\t\t\t\t// 排序\n\t\t\t\tCollections.sort(childrenNodes);\n\t\t\t\tSystem.out.println(\"Node: \" + event.getPath()\n\t\t\t\t\t\t+ \" change event is deleted! Current locked nodes:\\n\\t\"\n\t\t\t\t\t\t+ StringUtils.join(childrenNodes,\"\\n\\t\"));\n\t\t\t}\n\t\t\tcatch (KeeperException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tcatch (InterruptedException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n    }\n\n\tpublic boolean tryLock()\n\t{\n\t\ttry\n\t\t{\n\t\t\tif(tryLockInner())\n\t\t\t\treturn true;\n\t\t\telse\n\t\t\t\treturn waitForLockInner(waitNodePath);\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate boolean tryLockInner() throws Exception\n\t{\n\t\ttry\n\t\t{\n\t\t\tif(lockName.contains(lockSymbol))\n\t\t\t\tthrow new Exception(\"lockName can not contains \" + lockSymbol);\n\t\t\t//创建临时子节点\n\t\t\tmyNodePath = zk.create(lockDir + \"/\" + lockName + lockSymbol, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);\n\t\t\tSystem.out.println(threadId + \" created \" + myNodePath);\n\t\t\t//取出所有子节点\n\t\t\tList<String> subNodes = zk.getChildren(lockDir, false);\n\t\t\t//取出所有lockName的锁\n\t\t\tList<String> lockedNodes = new ArrayList<String>();\n\t\t\tfor (String node : subNodes) {\n\t\t\t\tString nodePrefix = node.split(lockSymbol)[0];\n\t\t\t\tif(nodePrefix.equals(lockName)){//对锁名做个判断，前缀相同即为同一组锁\n\t\t\t\t\tlockedNodes.add(node);\n\t\t\t\t}\n\t\t\t}\n\t\t\tCollections.sort(lockedNodes);\n\t\t\tSystem.out.println(\"Current locked nodes: \\n\\t\" + StringUtils.join(lockedNodes, \"\\n\\t\"));\n\t\t\tif(myNodePath.equals(lockDir + \"/\" + lockedNodes.get(0))){\n\t\t\t\t//如果是最小的节点,则表示取得锁\n\t            return true;\n\t        }\n\t\t\t//如果不是最小的节点，找到比自己小1的节点，在List中的位置是自己的前一位\n\t\t\tString myZnodeName = myNodePath.substring(myNodePath.lastIndexOf(\"/\") + 1);\n\t\t\twaitNodePath = lockDir + \"/\" + lockedNodes.get(lockedNodes.indexOf(myZnodeName)-1);\n\t\t}\n\t\tcatch (KeeperException e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\tcatch (InterruptedException e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn false;\n\t}\n\n\tprivate boolean waitForLockInner(String waitPath) throws InterruptedException, KeeperException {\n        Stat stat = zk.exists(waitPath, true);\n        //判断比自己小一个数的节点是否存在,如果存在则需等待锁,同时注册监听\n        if (stat != null)\n        {\n        \tSystem.out.println(threadId + \" waiting for \" + waitPath);\n        \tthis.latch = new CountDownLatch(1);\n        \tthis.latch.await(); //不加超时时间，无限等待\n        \t//\n        \t//waiting\n        \t//Zzzzz...\n        \t//still waiting\n        \t//\n        \t// 探测到节点变化，刷新节点信息\n        \tthis.latch = null;\n        \ttry\n\t\t\t{\n\t\t\t\t// 确认myNodePath是否真的是列表中的最小节点\n\t\t\t\tList<String> childrenNodes = zk.getChildren(lockDir, false);\n\t\t\t\t// 排序\n\t\t\t\tCollections.sort(childrenNodes);\n\t\t\t\tif(myNodePath.equals(lockDir + \"/\" + childrenNodes.get(0)))\n\t\t\t\t\treturn true;\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t    // 说明waitNodePath是由于出现异常而挂掉的 , 更新waitNodePath\n\t\t\t\t\tString thisNodeName = myNodePath.substring(myNodePath.lastIndexOf(\"/\") + 1);\n\t\t\t\t\tint index = childrenNodes.indexOf(thisNodeName);\n\t\t\t\t\twaitNodePath = lockDir + \"/\" + childrenNodes.get(index - 1);\n\t\t\t\t\t//重新等待锁\n\t\t\t\t\treturn waitForLockInner(waitNodePath);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (KeeperException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tcatch (InterruptedException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t\treturn false;\n\t\t\t}\n        }\n        return true;\n    }\n\n\tpublic void unlock() throws Exception\n\t{\n\t\ttry\n\t\t{\n\t\t\tSystem.out.println(threadId + \" unlock \" + myNodePath);\n\t\t\tzk.delete(myNodePath,-1);\n\t\t\tmyNodePath = null;\n\t\t\tzk.close();\n\t\t}\n\t\tcatch (InterruptedException e)\n\t\t{\n\t\t\tthrow new Exception(\"Error while releasing lock! \" + e.getMessage(), e);\n\t\t}\n\t\tcatch (KeeperException e)\n\t\t{\n\t\t\tthrow new Exception(\"Error while releasing lock! \" + e.getMessage(), e);\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception\n\t{\n\t        //一个简单的测试\n\t\tList<Thread> workers = new ArrayList<Thread>(10);\n\t\tfor (int i=1; i<10; ++i)\n\t\t{\n\t\t\tThread thread = new Thread(new Runnable()\n\t\t\t{\n\t\t\t\tString zk = \"10.12.10.169:2181,10.12.139.141:2181\";\n\t\t\t\t@Override\n\t\t\t\tpublic void run()\n\t\t\t\t{\n\t\t\t\t\ttry\n\t\t\t\t\t{\n\t\t\t\t\t\tDistributedExclusiveLock lock = new DistributedExclusiveLock(zk, \"zkLock\");\n\t\t\t\t\t\tif (lock.tryLock());\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tString tid = \"Thread-\" + Thread.currentThread().getId();\n\t\t\t\t\t\t\tint time = new Random().nextInt(5000);\n\t\t\t\t\t\t\tSystem.out.println(tid + \" gets lock and is working, sleep for \" + time + \" ms\");\n\t\t\t\t\t\t\tThread.sleep(time);\n\t\t\t\t\t\t\tlock.unlock();\n\t\t\t\t\t\t\tSystem.out.println(tid + \" releases lock\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Exception e)\n\t\t\t\t\t{\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t\tthread.setDaemon(true);\n\t\t\tworkers.add(thread);\n\t\t}\n\n\t\tfor (Thread t : workers)\n\t\t{\n\t\t\tt.start();\n\t\t}\n\t\tThread.sleep(100000);\n\t}\n}\n```\n\n","source":"_posts/基于zookeeper的分布式独占锁实现.md","raw":"---\ntitle: 基于zookeeper的分布式独占锁实现\ndate: 2014-05-13 21:05:01\ntags:\n- zookeeper\n- 分布式应用\n- 分布式协调\ncategories:\n- 分布式应用\n---\n\n背景\n--\n\n在分布式系统中，经常遇到这样一种场景：选举一个节点执行某一个任务，当此节点宕机后，其他节点可以接管并继续执行这个任务。由于各个节点运行的代码是一样的，彼此之间也是平等的，各个节点如何可以知道自己是否可以执行这个任务呢？当有节点宕机时，又如何判断自己是否可以接管任务呢？在我们的分布式任务调度系统中，需要选取调度器集群中的一个节点进行轮询任务状态，这里使用了zookeeper来实现一个统一的分布式锁，从而选出轮询节点。\n\n原理\n--\n\n如图所示，每台服务器启动后，都在同一目录下建一个临时顺序节点（EPHEMERAL_SEQUENTIAL），并获取此目录下的所有节点信息，如果自己的序号是最小的，就认为获取到了锁，可以执行任务。若自己的节点不是最小的，就认为自己没有获取到锁，不执行任务，同时，在比自己小1个序号的节点上增加监听。当比自己小1个序号的节点发生变化的时候，再次检查自己是否是最小序号的节点，如果是则获取锁，否则继续监听比自己小1个序号的节点。\n\n![](https://raw.githubusercontent.com/maohong/picture/master/20140502-%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8B%AC%E5%8D%A0%E9%94%81%E5%AE%9E%E7%8E%B0/1.jpg)\n\n实现\n--\n\n以下是一个demo实现程序：\n\n```java\npublic class DistributedExclusiveLock implements Watcher\n{\n\tprivate ZooKeeper zk;\n\tprivate String lockDir = \"/testlock\";//锁节点所在zk的目录\n\tprivate String lockSymbol = \"_lock_\";//锁节点标志\n\tprivate String lockName;//锁节点前缀，构造锁时由外部传入\n\tprivate String waitNodePath;//等待的前一个锁的节点名称\n\tprivate String myNodePath;//当前锁\n\tprivate CountDownLatch latch;//计数器\n\tprivate String threadId;\n\n\t/**\n\t * 创建分布式锁\n\t * @param lockName 竞争资源标志,lockName中不能包含单词lock\n\t * @throws Exception\n\t */\n\tpublic DistributedExclusiveLock(String zkServers, String lockName) throws Exception\n\t{\n\t\t//简单校验lockDir路径\n\t\tif (!lockDir.startsWith(\"/\"))\n\t\t\tthrow new Exception(\"LockDir Path must start with / character! lockDir=\" + lockDir);\n\t\tif (lockDir.endsWith(\"/\"))\n\t\t\tthrow new Exception(\"LockDir Path must not end with / character! lockDir=\" + lockDir);\n\n\t\tthis.lockName = lockName;\n\t\tthis.threadId = getThreadId();\n\t\t// 创建一个与服务器的连接\n\t\ttry\n\t\t{\n\t\t\tzk = new ZooKeeper(zkServers, 3000, this);\n\t\t\tcreateLockDirIfNecessary(lockDir);\n\t\t} catch (Exception e) {\n\t\t\tthrow new Exception(\"Error while initializing DistributedExclusiveLock!\" + e.getMessage(), e);\n\t\t}\n\t}\n\n\tprivate String getThreadId()\n\t{\n\t\treturn \"Thread-\" + Thread.currentThread().getId();\n\t}\n\n\t/**\n\t * 在zk上建立lock目录，如果目录不存在，逐级创建节点\n\t */\n\tprivate synchronized void createLockDirIfNecessary(String zkDir) throws KeeperException, InterruptedException\n\t{\n\t\t//zkDir是一级节点，如/cloudscheduler\n\t\tif (zkDir.indexOf(\"/\") == zkDir.lastIndexOf(\"/\"))\n\t\t{\n\t\t\tStat stat = zk.exists(zkDir, false);\n\t\t\tif(stat == null){\n\t\t\t\t// 创建一级节点\n\t\t\t\tzk.create(zkDir, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t}\n\t\t}\n\t\telse\t//zkDir非一级节点\n\t\t{\n\t\t\tString parentDir = zkDir.substring(0, zkDir.lastIndexOf(\"/\"));\n\t\t\tif (zk.exists(parentDir, false) != null)\n\t\t\t{\t//如果父节点存在，建当前节点\n\t\t\t\tStat stat = zk.exists(zkDir, false);\n\t\t\t\tif(stat == null){\n\t\t\t\t\t// 创建非一级节点\n\t\t\t\t\tzk.create(zkDir, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\t//否则，先建父节点，再建当前节点\n\t\t\t\tcreateLockDirIfNecessary(parentDir);\n\t\t\t\tcreateLockDirIfNecessary(zkDir);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * zookeeper节点的监视器\n\t */\n\t@Override\n\tpublic void process(WatchedEvent event)\n\t{\n\t\tif (event.getType() == EventType.NodeDeleted)\n\t\t{\n\t\t\tif (this.latch!=null)\n\t\t\t\tthis.latch.countDown();\n\t\t\ttry\n\t\t\t{\n\t\t\t\tList<String> childrenNodes = zk.getChildren(lockDir, false);\n\t\t\t\t// 排序\n\t\t\t\tCollections.sort(childrenNodes);\n\t\t\t\tSystem.out.println(\"Node: \" + event.getPath()\n\t\t\t\t\t\t+ \" change event is deleted! Current locked nodes:\\n\\t\"\n\t\t\t\t\t\t+ StringUtils.join(childrenNodes,\"\\n\\t\"));\n\t\t\t}\n\t\t\tcatch (KeeperException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tcatch (InterruptedException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n    }\n\n\tpublic boolean tryLock()\n\t{\n\t\ttry\n\t\t{\n\t\t\tif(tryLockInner())\n\t\t\t\treturn true;\n\t\t\telse\n\t\t\t\treturn waitForLockInner(waitNodePath);\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate boolean tryLockInner() throws Exception\n\t{\n\t\ttry\n\t\t{\n\t\t\tif(lockName.contains(lockSymbol))\n\t\t\t\tthrow new Exception(\"lockName can not contains \" + lockSymbol);\n\t\t\t//创建临时子节点\n\t\t\tmyNodePath = zk.create(lockDir + \"/\" + lockName + lockSymbol, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);\n\t\t\tSystem.out.println(threadId + \" created \" + myNodePath);\n\t\t\t//取出所有子节点\n\t\t\tList<String> subNodes = zk.getChildren(lockDir, false);\n\t\t\t//取出所有lockName的锁\n\t\t\tList<String> lockedNodes = new ArrayList<String>();\n\t\t\tfor (String node : subNodes) {\n\t\t\t\tString nodePrefix = node.split(lockSymbol)[0];\n\t\t\t\tif(nodePrefix.equals(lockName)){//对锁名做个判断，前缀相同即为同一组锁\n\t\t\t\t\tlockedNodes.add(node);\n\t\t\t\t}\n\t\t\t}\n\t\t\tCollections.sort(lockedNodes);\n\t\t\tSystem.out.println(\"Current locked nodes: \\n\\t\" + StringUtils.join(lockedNodes, \"\\n\\t\"));\n\t\t\tif(myNodePath.equals(lockDir + \"/\" + lockedNodes.get(0))){\n\t\t\t\t//如果是最小的节点,则表示取得锁\n\t            return true;\n\t        }\n\t\t\t//如果不是最小的节点，找到比自己小1的节点，在List中的位置是自己的前一位\n\t\t\tString myZnodeName = myNodePath.substring(myNodePath.lastIndexOf(\"/\") + 1);\n\t\t\twaitNodePath = lockDir + \"/\" + lockedNodes.get(lockedNodes.indexOf(myZnodeName)-1);\n\t\t}\n\t\tcatch (KeeperException e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\tcatch (InterruptedException e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn false;\n\t}\n\n\tprivate boolean waitForLockInner(String waitPath) throws InterruptedException, KeeperException {\n        Stat stat = zk.exists(waitPath, true);\n        //判断比自己小一个数的节点是否存在,如果存在则需等待锁,同时注册监听\n        if (stat != null)\n        {\n        \tSystem.out.println(threadId + \" waiting for \" + waitPath);\n        \tthis.latch = new CountDownLatch(1);\n        \tthis.latch.await(); //不加超时时间，无限等待\n        \t//\n        \t//waiting\n        \t//Zzzzz...\n        \t//still waiting\n        \t//\n        \t// 探测到节点变化，刷新节点信息\n        \tthis.latch = null;\n        \ttry\n\t\t\t{\n\t\t\t\t// 确认myNodePath是否真的是列表中的最小节点\n\t\t\t\tList<String> childrenNodes = zk.getChildren(lockDir, false);\n\t\t\t\t// 排序\n\t\t\t\tCollections.sort(childrenNodes);\n\t\t\t\tif(myNodePath.equals(lockDir + \"/\" + childrenNodes.get(0)))\n\t\t\t\t\treturn true;\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t    // 说明waitNodePath是由于出现异常而挂掉的 , 更新waitNodePath\n\t\t\t\t\tString thisNodeName = myNodePath.substring(myNodePath.lastIndexOf(\"/\") + 1);\n\t\t\t\t\tint index = childrenNodes.indexOf(thisNodeName);\n\t\t\t\t\twaitNodePath = lockDir + \"/\" + childrenNodes.get(index - 1);\n\t\t\t\t\t//重新等待锁\n\t\t\t\t\treturn waitForLockInner(waitNodePath);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (KeeperException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tcatch (InterruptedException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();\n\t\t\t\treturn false;\n\t\t\t}\n        }\n        return true;\n    }\n\n\tpublic void unlock() throws Exception\n\t{\n\t\ttry\n\t\t{\n\t\t\tSystem.out.println(threadId + \" unlock \" + myNodePath);\n\t\t\tzk.delete(myNodePath,-1);\n\t\t\tmyNodePath = null;\n\t\t\tzk.close();\n\t\t}\n\t\tcatch (InterruptedException e)\n\t\t{\n\t\t\tthrow new Exception(\"Error while releasing lock! \" + e.getMessage(), e);\n\t\t}\n\t\tcatch (KeeperException e)\n\t\t{\n\t\t\tthrow new Exception(\"Error while releasing lock! \" + e.getMessage(), e);\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception\n\t{\n\t        //一个简单的测试\n\t\tList<Thread> workers = new ArrayList<Thread>(10);\n\t\tfor (int i=1; i<10; ++i)\n\t\t{\n\t\t\tThread thread = new Thread(new Runnable()\n\t\t\t{\n\t\t\t\tString zk = \"10.12.10.169:2181,10.12.139.141:2181\";\n\t\t\t\t@Override\n\t\t\t\tpublic void run()\n\t\t\t\t{\n\t\t\t\t\ttry\n\t\t\t\t\t{\n\t\t\t\t\t\tDistributedExclusiveLock lock = new DistributedExclusiveLock(zk, \"zkLock\");\n\t\t\t\t\t\tif (lock.tryLock());\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tString tid = \"Thread-\" + Thread.currentThread().getId();\n\t\t\t\t\t\t\tint time = new Random().nextInt(5000);\n\t\t\t\t\t\t\tSystem.out.println(tid + \" gets lock and is working, sleep for \" + time + \" ms\");\n\t\t\t\t\t\t\tThread.sleep(time);\n\t\t\t\t\t\t\tlock.unlock();\n\t\t\t\t\t\t\tSystem.out.println(tid + \" releases lock\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Exception e)\n\t\t\t\t\t{\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t\tthread.setDaemon(true);\n\t\t\tworkers.add(thread);\n\t\t}\n\n\t\tfor (Thread t : workers)\n\t\t{\n\t\t\tt.start();\n\t\t}\n\t\tThread.sleep(100000);\n\t}\n}\n```\n\n","slug":"基于zookeeper的分布式独占锁实现","published":1,"updated":"2017-01-18T09:54:11.738Z","_id":"ciy320aji000zifs6p6j3ri0y","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>在分布式系统中，经常遇到这样一种场景：选举一个节点执行某一个任务，当此节点宕机后，其他节点可以接管并继续执行这个任务。由于各个节点运行的代码是一样的，彼此之间也是平等的，各个节点如何可以知道自己是否可以执行这个任务呢？当有节点宕机时，又如何判断自己是否可以接管任务呢？在我们的分布式任务调度系统中，需要选取调度器集群中的一个节点进行轮询任务状态，这里使用了zookeeper来实现一个统一的分布式锁，从而选出轮询节点。</p>\n<h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>如图所示，每台服务器启动后，都在同一目录下建一个临时顺序节点（EPHEMERAL_SEQUENTIAL），并获取此目录下的所有节点信息，如果自己的序号是最小的，就认为获取到了锁，可以执行任务。若自己的节点不是最小的，就认为自己没有获取到锁，不执行任务，同时，在比自己小1个序号的节点上增加监听。当比自己小1个序号的节点发生变化的时候，再次检查自己是否是最小序号的节点，如果是则获取锁，否则继续监听比自己小1个序号的节点。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140502-%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8B%AC%E5%8D%A0%E9%94%81%E5%AE%9E%E7%8E%B0/1.jpg\" alt=\"\"></p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>以下是一个demo实现程序：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DistributedExclusiveLock</span> <span class=\"keyword\">implements</span> <span class=\"title\">Watcher</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"keyword\">private</span> ZooKeeper zk;</div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockDir = <span class=\"string\">\"/testlock\"</span>;<span class=\"comment\">//锁节点所在zk的目录</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockSymbol = <span class=\"string\">\"_lock_\"</span>;<span class=\"comment\">//锁节点标志</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockName;<span class=\"comment\">//锁节点前缀，构造锁时由外部传入</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String waitNodePath;<span class=\"comment\">//等待的前一个锁的节点名称</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String myNodePath;<span class=\"comment\">//当前锁</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> CountDownLatch latch;<span class=\"comment\">//计数器</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String threadId;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\">\t * 创建分布式锁</div><div class=\"line\">\t * <span class=\"doctag\">@param</span> lockName 竞争资源标志,lockName中不能包含单词lock</div><div class=\"line\">\t * <span class=\"doctag\">@throws</span> Exception</div><div class=\"line\">\t */</div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">DistributedExclusiveLock</span><span class=\"params\">(String zkServers, String lockName)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"comment\">//简单校验lockDir路径</span></div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (!lockDir.startsWith(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"LockDir Path must start with / character! lockDir=\"</span> + lockDir);</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (lockDir.endsWith(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"LockDir Path must not end with / character! lockDir=\"</span> + lockDir);</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">this</span>.lockName = lockName;</div><div class=\"line\">\t\t<span class=\"keyword\">this</span>.threadId = getThreadId();</div><div class=\"line\">\t\t<span class=\"comment\">// 创建一个与服务器的连接</span></div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tzk = <span class=\"keyword\">new</span> ZooKeeper(zkServers, <span class=\"number\">3000</span>, <span class=\"keyword\">this</span>);</div><div class=\"line\">\t\t\tcreateLockDirIfNecessary(lockDir);</div><div class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while initializing DistributedExclusiveLock!\"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> String <span class=\"title\">getThreadId</span><span class=\"params\">()</span></span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"Thread-\"</span> + Thread.currentThread().getId();</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\">\t * 在zk上建立lock目录，如果目录不存在，逐级创建节点</div><div class=\"line\">\t */</div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">createLockDirIfNecessary</span><span class=\"params\">(String zkDir)</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"comment\">//zkDir是一级节点，如/cloudscheduler</span></div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (zkDir.indexOf(<span class=\"string\">\"/\"</span>) == zkDir.lastIndexOf(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tStat stat = zk.exists(zkDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(stat == <span class=\"keyword\">null</span>)&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 创建一级节点</span></div><div class=\"line\">\t\t\t\tzk.create(zkDir, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">else</span>\t<span class=\"comment\">//zkDir非一级节点</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tString parentDir = zkDir.substring(<span class=\"number\">0</span>, zkDir.lastIndexOf(<span class=\"string\">\"/\"</span>));</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span> (zk.exists(parentDir, <span class=\"keyword\">false</span>) != <span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t&#123;\t<span class=\"comment\">//如果父节点存在，建当前节点</span></div><div class=\"line\">\t\t\t\tStat stat = zk.exists(zkDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(stat == <span class=\"keyword\">null</span>)&#123;</div><div class=\"line\">\t\t\t\t\t<span class=\"comment\">// 创建非一级节点</span></div><div class=\"line\">\t\t\t\t\tzk.create(zkDir, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t&#123;\t<span class=\"comment\">//否则，先建父节点，再建当前节点</span></div><div class=\"line\">\t\t\t\tcreateLockDirIfNecessary(parentDir);</div><div class=\"line\">\t\t\t\tcreateLockDirIfNecessary(zkDir);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\">\t * zookeeper节点的监视器</div><div class=\"line\">\t */</div><div class=\"line\">\t<span class=\"meta\">@Override</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(WatchedEvent event)</span></span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (event.getType() == EventType.NodeDeleted)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.latch!=<span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">this</span>.latch.countDown();</div><div class=\"line\">\t\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tList&lt;String&gt; childrenNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 排序</span></div><div class=\"line\">\t\t\t\tCollections.sort(childrenNodes);</div><div class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Node: \"</span> + event.getPath()</div><div class=\"line\">\t\t\t\t\t\t+ <span class=\"string\">\" change event is deleted! Current locked nodes:\\n\\t\"</span></div><div class=\"line\">\t\t\t\t\t\t+ StringUtils.join(childrenNodes,<span class=\"string\">\"\\n\\t\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryLock</span><span class=\"params\">()</span></span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(tryLockInner())</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> waitForLockInner(waitNodePath);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryLockInner</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(lockName.contains(lockSymbol))</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"lockName can not contains \"</span> + lockSymbol);</div><div class=\"line\">\t\t\t<span class=\"comment\">//创建临时子节点</span></div><div class=\"line\">\t\t\tmyNodePath = zk.create(lockDir + <span class=\"string\">\"/\"</span> + lockName + lockSymbol, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</div><div class=\"line\">\t\t\tSystem.out.println(threadId + <span class=\"string\">\" created \"</span> + myNodePath);</div><div class=\"line\">\t\t\t<span class=\"comment\">//取出所有子节点</span></div><div class=\"line\">\t\t\tList&lt;String&gt; subNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t<span class=\"comment\">//取出所有lockName的锁</span></div><div class=\"line\">\t\t\tList&lt;String&gt; lockedNodes = <span class=\"keyword\">new</span> ArrayList&lt;String&gt;();</div><div class=\"line\">\t\t\t<span class=\"keyword\">for</span> (String node : subNodes) &#123;</div><div class=\"line\">\t\t\t\tString nodePrefix = node.split(lockSymbol)[<span class=\"number\">0</span>];</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(nodePrefix.equals(lockName))&#123;<span class=\"comment\">//对锁名做个判断，前缀相同即为同一组锁</span></div><div class=\"line\">\t\t\t\t\tlockedNodes.add(node);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\tCollections.sort(lockedNodes);</div><div class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Current locked nodes: \\n\\t\"</span> + StringUtils.join(lockedNodes, <span class=\"string\">\"\\n\\t\"</span>));</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(myNodePath.equals(lockDir + <span class=\"string\">\"/\"</span> + lockedNodes.get(<span class=\"number\">0</span>)))&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">//如果是最小的节点,则表示取得锁</span></div><div class=\"line\">\t            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t        &#125;</div><div class=\"line\">\t\t\t<span class=\"comment\">//如果不是最小的节点，找到比自己小1的节点，在List中的位置是自己的前一位</span></div><div class=\"line\">\t\t\tString myZnodeName = myNodePath.substring(myNodePath.lastIndexOf(<span class=\"string\">\"/\"</span>) + <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\twaitNodePath = lockDir + <span class=\"string\">\"/\"</span> + lockedNodes.get(lockedNodes.indexOf(myZnodeName)-<span class=\"number\">1</span>);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">waitForLockInner</span><span class=\"params\">(String waitPath)</span> <span class=\"keyword\">throws</span> InterruptedException, KeeperException </span>&#123;</div><div class=\"line\">        Stat stat = zk.exists(waitPath, <span class=\"keyword\">true</span>);</div><div class=\"line\">        <span class=\"comment\">//判断比自己小一个数的节点是否存在,如果存在则需等待锁,同时注册监听</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (stat != <span class=\"keyword\">null</span>)</div><div class=\"line\">        &#123;</div><div class=\"line\">        \tSystem.out.println(threadId + <span class=\"string\">\" waiting for \"</span> + waitPath);</div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch = <span class=\"keyword\">new</span> CountDownLatch(<span class=\"number\">1</span>);</div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch.await(); <span class=\"comment\">//不加超时时间，无限等待</span></div><div class=\"line\">        \t<span class=\"comment\">//</span></div><div class=\"line\">        \t<span class=\"comment\">//waiting</span></div><div class=\"line\">        \t<span class=\"comment\">//Zzzzz...</span></div><div class=\"line\">        \t<span class=\"comment\">//still waiting</span></div><div class=\"line\">        \t<span class=\"comment\">//</span></div><div class=\"line\">        \t<span class=\"comment\">// 探测到节点变化，刷新节点信息</span></div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch = <span class=\"keyword\">null</span>;</div><div class=\"line\">        \t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 确认myNodePath是否真的是列表中的最小节点</span></div><div class=\"line\">\t\t\t\tList&lt;String&gt; childrenNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 排序</span></div><div class=\"line\">\t\t\t\tCollections.sort(childrenNodes);</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(myNodePath.equals(lockDir + <span class=\"string\">\"/\"</span> + childrenNodes.get(<span class=\"number\">0</span>)))</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t    <span class=\"comment\">// 说明waitNodePath是由于出现异常而挂掉的 , 更新waitNodePath</span></div><div class=\"line\">\t\t\t\t\tString thisNodeName = myNodePath.substring(myNodePath.lastIndexOf(<span class=\"string\">\"/\"</span>) + <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">int</span> index = childrenNodes.indexOf(thisNodeName);</div><div class=\"line\">\t\t\t\t\twaitNodePath = lockDir + <span class=\"string\">\"/\"</span> + childrenNodes.get(index - <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\t\t\t<span class=\"comment\">//重新等待锁</span></div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">return</span> waitForLockInner(waitNodePath);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">unlock</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(threadId + <span class=\"string\">\" unlock \"</span> + myNodePath);</div><div class=\"line\">\t\t\tzk.delete(myNodePath,-<span class=\"number\">1</span>);</div><div class=\"line\">\t\t\tmyNodePath = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\tzk.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while releasing lock! \"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while releasing lock! \"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t        <span class=\"comment\">//一个简单的测试</span></div><div class=\"line\">\t\tList&lt;Thread&gt; workers = <span class=\"keyword\">new</span> ArrayList&lt;Thread&gt;(<span class=\"number\">10</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>; i&lt;<span class=\"number\">10</span>; ++i)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tThread thread = <span class=\"keyword\">new</span> Thread(<span class=\"keyword\">new</span> Runnable()</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tString zk = <span class=\"string\">\"10.12.10.169:2181,10.12.139.141:2181\"</span>;</div><div class=\"line\">\t\t\t\t<span class=\"meta\">@Override</span></div><div class=\"line\">\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></span></div><div class=\"line\">\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\tDistributedExclusiveLock lock = <span class=\"keyword\">new</span> DistributedExclusiveLock(zk, <span class=\"string\">\"zkLock\"</span>);</div><div class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (lock.tryLock());</div><div class=\"line\">\t\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\t\tString tid = <span class=\"string\">\"Thread-\"</span> + Thread.currentThread().getId();</div><div class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">int</span> time = <span class=\"keyword\">new</span> Random().nextInt(<span class=\"number\">5000</span>);</div><div class=\"line\">\t\t\t\t\t\t\tSystem.out.println(tid + <span class=\"string\">\" gets lock and is working, sleep for \"</span> + time + <span class=\"string\">\" ms\"</span>);</div><div class=\"line\">\t\t\t\t\t\t\tThread.sleep(time);</div><div class=\"line\">\t\t\t\t\t\t\tlock.unlock();</div><div class=\"line\">\t\t\t\t\t\t\tSystem.out.println(tid + <span class=\"string\">\" releases lock\"</span>);</div><div class=\"line\">\t\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;);</div><div class=\"line\">\t\t\tthread.setDaemon(<span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t\tworkers.add(thread);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">for</span> (Thread t : workers)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tt.start();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\tThread.sleep(<span class=\"number\">100000</span>);</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>在分布式系统中，经常遇到这样一种场景：选举一个节点执行某一个任务，当此节点宕机后，其他节点可以接管并继续执行这个任务。由于各个节点运行的代码是一样的，彼此之间也是平等的，各个节点如何可以知道自己是否可以执行这个任务呢？当有节点宕机时，又如何判断自己是否可以接管任务呢？在我们的分布式任务调度系统中，需要选取调度器集群中的一个节点进行轮询任务状态，这里使用了zookeeper来实现一个统一的分布式锁，从而选出轮询节点。</p>\n<h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>如图所示，每台服务器启动后，都在同一目录下建一个临时顺序节点（EPHEMERAL_SEQUENTIAL），并获取此目录下的所有节点信息，如果自己的序号是最小的，就认为获取到了锁，可以执行任务。若自己的节点不是最小的，就认为自己没有获取到锁，不执行任务，同时，在比自己小1个序号的节点上增加监听。当比自己小1个序号的节点发生变化的时候，再次检查自己是否是最小序号的节点，如果是则获取锁，否则继续监听比自己小1个序号的节点。</p>\n<p><img src=\"https://raw.githubusercontent.com/maohong/picture/master/20140502-%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8B%AC%E5%8D%A0%E9%94%81%E5%AE%9E%E7%8E%B0/1.jpg\" alt=\"\"></p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>以下是一个demo实现程序：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DistributedExclusiveLock</span> <span class=\"keyword\">implements</span> <span class=\"title\">Watcher</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">private</span> ZooKeeper zk;</div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockDir = <span class=\"string\">\"/testlock\"</span>;<span class=\"comment\">//锁节点所在zk的目录</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockSymbol = <span class=\"string\">\"_lock_\"</span>;<span class=\"comment\">//锁节点标志</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String lockName;<span class=\"comment\">//锁节点前缀，构造锁时由外部传入</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String waitNodePath;<span class=\"comment\">//等待的前一个锁的节点名称</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String myNodePath;<span class=\"comment\">//当前锁</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> CountDownLatch latch;<span class=\"comment\">//计数器</span></div><div class=\"line\">\t<span class=\"keyword\">private</span> String threadId;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</div><div class=\"line\">\t * 创建分布式锁</div><div class=\"line\">\t * <span class=\"doctag\">@param</span> lockName 竞争资源标志,lockName中不能包含单词lock</div><div class=\"line\">\t * <span class=\"doctag\">@throws</span> Exception</div><div class=\"line\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">DistributedExclusiveLock</span><span class=\"params\">(String zkServers, String lockName)</span> <span class=\"keyword\">throws</span> Exception</div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"comment\">//简单校验lockDir路径</span></div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (!lockDir.startsWith(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"LockDir Path must start with / character! lockDir=\"</span> + lockDir);</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (lockDir.endsWith(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"LockDir Path must not end with / character! lockDir=\"</span> + lockDir);</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">this</span>.lockName = lockName;</div><div class=\"line\">\t\t<span class=\"keyword\">this</span>.threadId = getThreadId();</div><div class=\"line\">\t\t<span class=\"comment\">// 创建一个与服务器的连接</span></div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tzk = <span class=\"keyword\">new</span> ZooKeeper(zkServers, <span class=\"number\">3000</span>, <span class=\"keyword\">this</span>);</div><div class=\"line\">\t\t\tcreateLockDirIfNecessary(lockDir);</div><div class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while initializing DistributedExclusiveLock!\"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> String <span class=\"title\">getThreadId</span><span class=\"params\">()</span></div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"Thread-\"</span> + Thread.currentThread().getId();</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</div><div class=\"line\">\t * 在zk上建立lock目录，如果目录不存在，逐级创建节点</div><div class=\"line\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">createLockDirIfNecessary</span><span class=\"params\">(String zkDir)</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"comment\">//zkDir是一级节点，如/cloudscheduler</span></div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (zkDir.indexOf(<span class=\"string\">\"/\"</span>) == zkDir.lastIndexOf(<span class=\"string\">\"/\"</span>))</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tStat stat = zk.exists(zkDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(stat == <span class=\"keyword\">null</span>)&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 创建一级节点</span></div><div class=\"line\">\t\t\t\tzk.create(zkDir, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">else</span>\t<span class=\"comment\">//zkDir非一级节点</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tString parentDir = zkDir.substring(<span class=\"number\">0</span>, zkDir.lastIndexOf(<span class=\"string\">\"/\"</span>));</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span> (zk.exists(parentDir, <span class=\"keyword\">false</span>) != <span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t&#123;\t<span class=\"comment\">//如果父节点存在，建当前节点</span></div><div class=\"line\">\t\t\t\tStat stat = zk.exists(zkDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(stat == <span class=\"keyword\">null</span>)&#123;</div><div class=\"line\">\t\t\t\t\t<span class=\"comment\">// 创建非一级节点</span></div><div class=\"line\">\t\t\t\t\tzk.create(zkDir, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t&#123;\t<span class=\"comment\">//否则，先建父节点，再建当前节点</span></div><div class=\"line\">\t\t\t\tcreateLockDirIfNecessary(parentDir);</div><div class=\"line\">\t\t\t\tcreateLockDirIfNecessary(zkDir);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</div><div class=\"line\">\t * zookeeper节点的监视器</div><div class=\"line\">\t */</span></div><div class=\"line\">\t<span class=\"meta\">@Override</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(WatchedEvent event)</span></div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (event.getType() == EventType.NodeDeleted)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.latch!=<span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">this</span>.latch.countDown();</div><div class=\"line\">\t\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tList&lt;String&gt; childrenNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 排序</span></div><div class=\"line\">\t\t\t\tCollections.sort(childrenNodes);</div><div class=\"line\">\t\t\t\tSystem.out.println(<span class=\"string\">\"Node: \"</span> + event.getPath()</div><div class=\"line\">\t\t\t\t\t\t+ <span class=\"string\">\" change event is deleted! Current locked nodes:\\n\\t\"</span></div><div class=\"line\">\t\t\t\t\t\t+ StringUtils.join(childrenNodes,<span class=\"string\">\"\\n\\t\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryLock</span><span class=\"params\">()</span></div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(tryLockInner())</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> waitForLockInner(waitNodePath);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryLockInner</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception</div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(lockName.contains(lockSymbol))</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"lockName can not contains \"</span> + lockSymbol);</div><div class=\"line\">\t\t\t<span class=\"comment\">//创建临时子节点</span></div><div class=\"line\">\t\t\tmyNodePath = zk.create(lockDir + <span class=\"string\">\"/\"</span> + lockName + lockSymbol, <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">0</span>], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</div><div class=\"line\">\t\t\tSystem.out.println(threadId + <span class=\"string\">\" created \"</span> + myNodePath);</div><div class=\"line\">\t\t\t<span class=\"comment\">//取出所有子节点</span></div><div class=\"line\">\t\t\tList&lt;String&gt; subNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t<span class=\"comment\">//取出所有lockName的锁</span></div><div class=\"line\">\t\t\tList&lt;String&gt; lockedNodes = <span class=\"keyword\">new</span> ArrayList&lt;String&gt;();</div><div class=\"line\">\t\t\t<span class=\"keyword\">for</span> (String node : subNodes) &#123;</div><div class=\"line\">\t\t\t\tString nodePrefix = node.split(lockSymbol)[<span class=\"number\">0</span>];</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(nodePrefix.equals(lockName))&#123;<span class=\"comment\">//对锁名做个判断，前缀相同即为同一组锁</span></div><div class=\"line\">\t\t\t\t\tlockedNodes.add(node);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\tCollections.sort(lockedNodes);</div><div class=\"line\">\t\t\tSystem.out.println(<span class=\"string\">\"Current locked nodes: \\n\\t\"</span> + StringUtils.join(lockedNodes, <span class=\"string\">\"\\n\\t\"</span>));</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(myNodePath.equals(lockDir + <span class=\"string\">\"/\"</span> + lockedNodes.get(<span class=\"number\">0</span>)))&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">//如果是最小的节点,则表示取得锁</span></div><div class=\"line\">\t            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t        &#125;</div><div class=\"line\">\t\t\t<span class=\"comment\">//如果不是最小的节点，找到比自己小1的节点，在List中的位置是自己的前一位</span></div><div class=\"line\">\t\t\tString myZnodeName = myNodePath.substring(myNodePath.lastIndexOf(<span class=\"string\">\"/\"</span>) + <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\twaitNodePath = lockDir + <span class=\"string\">\"/\"</span> + lockedNodes.get(lockedNodes.indexOf(myZnodeName)-<span class=\"number\">1</span>);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">waitForLockInner</span><span class=\"params\">(String waitPath)</span> <span class=\"keyword\">throws</span> InterruptedException, KeeperException </span>&#123;</div><div class=\"line\">        Stat stat = zk.exists(waitPath, <span class=\"keyword\">true</span>);</div><div class=\"line\">        <span class=\"comment\">//判断比自己小一个数的节点是否存在,如果存在则需等待锁,同时注册监听</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (stat != <span class=\"keyword\">null</span>)</div><div class=\"line\">        &#123;</div><div class=\"line\">        \tSystem.out.println(threadId + <span class=\"string\">\" waiting for \"</span> + waitPath);</div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch = <span class=\"keyword\">new</span> CountDownLatch(<span class=\"number\">1</span>);</div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch.await(); <span class=\"comment\">//不加超时时间，无限等待</span></div><div class=\"line\">        \t<span class=\"comment\">//</span></div><div class=\"line\">        \t<span class=\"comment\">//waiting</span></div><div class=\"line\">        \t<span class=\"comment\">//Zzzzz...</span></div><div class=\"line\">        \t<span class=\"comment\">//still waiting</span></div><div class=\"line\">        \t<span class=\"comment\">//</span></div><div class=\"line\">        \t<span class=\"comment\">// 探测到节点变化，刷新节点信息</span></div><div class=\"line\">        \t<span class=\"keyword\">this</span>.latch = <span class=\"keyword\">null</span>;</div><div class=\"line\">        \t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 确认myNodePath是否真的是列表中的最小节点</span></div><div class=\"line\">\t\t\t\tList&lt;String&gt; childrenNodes = zk.getChildren(lockDir, <span class=\"keyword\">false</span>);</div><div class=\"line\">\t\t\t\t<span class=\"comment\">// 排序</span></div><div class=\"line\">\t\t\t\tCollections.sort(childrenNodes);</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(myNodePath.equals(lockDir + <span class=\"string\">\"/\"</span> + childrenNodes.get(<span class=\"number\">0</span>)))</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t    <span class=\"comment\">// 说明waitNodePath是由于出现异常而挂掉的 , 更新waitNodePath</span></div><div class=\"line\">\t\t\t\t\tString thisNodeName = myNodePath.substring(myNodePath.lastIndexOf(<span class=\"string\">\"/\"</span>) + <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">int</span> index = childrenNodes.indexOf(thisNodeName);</div><div class=\"line\">\t\t\t\t\twaitNodePath = lockDir + <span class=\"string\">\"/\"</span> + childrenNodes.get(index - <span class=\"number\">1</span>);</div><div class=\"line\">\t\t\t\t\t<span class=\"comment\">//重新等待锁</span></div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">return</span> waitForLockInner(waitNodePath);</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">unlock</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception</div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(threadId + <span class=\"string\">\" unlock \"</span> + myNodePath);</div><div class=\"line\">\t\t\tzk.delete(myNodePath,-<span class=\"number\">1</span>);</div><div class=\"line\">\t\t\tmyNodePath = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\tzk.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while releasing lock! \"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Exception(<span class=\"string\">\"Error while releasing lock! \"</span> + e.getMessage(), e);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception</div><div class=\"line\">\t</span>&#123;</div><div class=\"line\">\t        <span class=\"comment\">//一个简单的测试</span></div><div class=\"line\">\t\tList&lt;Thread&gt; workers = <span class=\"keyword\">new</span> ArrayList&lt;Thread&gt;(<span class=\"number\">10</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>; i&lt;<span class=\"number\">10</span>; ++i)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tThread thread = <span class=\"keyword\">new</span> Thread(<span class=\"keyword\">new</span> Runnable()</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tString zk = <span class=\"string\">\"10.12.10.169:2181,10.12.139.141:2181\"</span>;</div><div class=\"line\">\t\t\t\t<span class=\"meta\">@Override</span></div><div class=\"line\">\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></div><div class=\"line\">\t\t\t\t</span>&#123;</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\tDistributedExclusiveLock lock = <span class=\"keyword\">new</span> DistributedExclusiveLock(zk, <span class=\"string\">\"zkLock\"</span>);</div><div class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (lock.tryLock());</div><div class=\"line\">\t\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\t\tString tid = <span class=\"string\">\"Thread-\"</span> + Thread.currentThread().getId();</div><div class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">int</span> time = <span class=\"keyword\">new</span> Random().nextInt(<span class=\"number\">5000</span>);</div><div class=\"line\">\t\t\t\t\t\t\tSystem.out.println(tid + <span class=\"string\">\" gets lock and is working, sleep for \"</span> + time + <span class=\"string\">\" ms\"</span>);</div><div class=\"line\">\t\t\t\t\t\t\tThread.sleep(time);</div><div class=\"line\">\t\t\t\t\t\t\tlock.unlock();</div><div class=\"line\">\t\t\t\t\t\t\tSystem.out.println(tid + <span class=\"string\">\" releases lock\"</span>);</div><div class=\"line\">\t\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t\t\t\t&#123;</div><div class=\"line\">\t\t\t\t\t\te.printStackTrace();</div><div class=\"line\">\t\t\t\t\t&#125;</div><div class=\"line\">\t\t\t\t&#125;</div><div class=\"line\">\t\t\t&#125;);</div><div class=\"line\">\t\t\tthread.setDaemon(<span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t\tworkers.add(thread);</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">for</span> (Thread t : workers)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tt.start();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\tThread.sleep(<span class=\"number\">100000</span>);</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n"},{"title":"前后端的CharacterEncoding不一致导致提交的表单数据丢失问题","date":"2014-03-20T03:09:03.000Z","_content":"\n最近在开发分布式任务调度系统的web端时，遇到一个坑，记录如下：\n\n在页面上新增和修改任务，提交后，任务的属性在后端怎么都接收不到，但是在另一个协同开发的同学那边本地调试就OK，在我的本地和公共开发环境都不行，这不合理啊。。。。。\n\n排查了很多地方，js、setter等等，一直没发现问题在哪。跟负责前端的同学交流了下，发现前端post的数据确实是修改过的，也就是后端接收有问题。\n\n于是把最新版本和历史版本对比，发现最新版本新增了一个LogFilter，用于记录pagedelay的，仔细一看，logFilter里面是\n\n```java\nrequest.setCharacterEncoding(“UTF-8″);\nresponse.setContentType(“text/html;charset=UTF-8″);\n```\n\n但页面上是GBK编码，所以导致数据在这个filter中编码出错，造成数据丢失，后端接收到的数据为null。\n\n<font color='red'>解决方法：</font>\n\n把logFilter里的UTF-8改为GBK，就一切正常了。\n\n<font color='red'>疑问：</font>\n\n1. 为何历史本没问题呢，因为历史版本中的logFilter配在struts2Filter之后，请求根本走不到logFilter里去。。。。\n\n2. 为何协同开发的同学本地调试没问题呢，那是因为他把web.xml里的LogFilter的filtermapping注掉了。。。。\n\n好一个歪萝卜大烂坑。。。","source":"_posts/前后端的CharacterEncoding不一致导致提交的表单数据丢失问题.md","raw":"---\ntitle: 前后端的CharacterEncoding不一致导致提交的表单数据丢失问题\ndate: 2014-03-20 11:09:03\ntags:\n- web开发\n- java\ncategories: \n- 问题分析\n---\n\n最近在开发分布式任务调度系统的web端时，遇到一个坑，记录如下：\n\n在页面上新增和修改任务，提交后，任务的属性在后端怎么都接收不到，但是在另一个协同开发的同学那边本地调试就OK，在我的本地和公共开发环境都不行，这不合理啊。。。。。\n\n排查了很多地方，js、setter等等，一直没发现问题在哪。跟负责前端的同学交流了下，发现前端post的数据确实是修改过的，也就是后端接收有问题。\n\n于是把最新版本和历史版本对比，发现最新版本新增了一个LogFilter，用于记录pagedelay的，仔细一看，logFilter里面是\n\n```java\nrequest.setCharacterEncoding(“UTF-8″);\nresponse.setContentType(“text/html;charset=UTF-8″);\n```\n\n但页面上是GBK编码，所以导致数据在这个filter中编码出错，造成数据丢失，后端接收到的数据为null。\n\n<font color='red'>解决方法：</font>\n\n把logFilter里的UTF-8改为GBK，就一切正常了。\n\n<font color='red'>疑问：</font>\n\n1. 为何历史本没问题呢，因为历史版本中的logFilter配在struts2Filter之后，请求根本走不到logFilter里去。。。。\n\n2. 为何协同开发的同学本地调试没问题呢，那是因为他把web.xml里的LogFilter的filtermapping注掉了。。。。\n\n好一个歪萝卜大烂坑。。。","slug":"前后端的CharacterEncoding不一致导致提交的表单数据丢失问题","published":1,"updated":"2017-01-18T09:54:11.738Z","_id":"ciy320ajj0011ifs6qph0xggi","comments":1,"layout":"post","photos":[],"link":"","content":"<p>最近在开发分布式任务调度系统的web端时，遇到一个坑，记录如下：</p>\n<p>在页面上新增和修改任务，提交后，任务的属性在后端怎么都接收不到，但是在另一个协同开发的同学那边本地调试就OK，在我的本地和公共开发环境都不行，这不合理啊。。。。。</p>\n<p>排查了很多地方，js、setter等等，一直没发现问题在哪。跟负责前端的同学交流了下，发现前端post的数据确实是修改过的，也就是后端接收有问题。</p>\n<p>于是把最新版本和历史版本对比，发现最新版本新增了一个LogFilter，用于记录pagedelay的，仔细一看，logFilter里面是</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">request.setCharacterEncoding(“UTF-<span class=\"number\">8</span>″);</div><div class=\"line\">response.setContentType(“text/html;charset=UTF-<span class=\"number\">8</span>″);</div></pre></td></tr></table></figure>\n<p>但页面上是GBK编码，所以导致数据在这个filter中编码出错，造成数据丢失，后端接收到的数据为null。</p>\n<font color=\"red\">解决方法：</font>\n\n<p>把logFilter里的UTF-8改为GBK，就一切正常了。</p>\n<font color=\"red\">疑问：</font>\n\n<ol>\n<li><p>为何历史本没问题呢，因为历史版本中的logFilter配在struts2Filter之后，请求根本走不到logFilter里去。。。。</p>\n</li>\n<li><p>为何协同开发的同学本地调试没问题呢，那是因为他把web.xml里的LogFilter的filtermapping注掉了。。。。</p>\n</li>\n</ol>\n<p>好一个歪萝卜大烂坑。。。</p>\n","excerpt":"","more":"<p>最近在开发分布式任务调度系统的web端时，遇到一个坑，记录如下：</p>\n<p>在页面上新增和修改任务，提交后，任务的属性在后端怎么都接收不到，但是在另一个协同开发的同学那边本地调试就OK，在我的本地和公共开发环境都不行，这不合理啊。。。。。</p>\n<p>排查了很多地方，js、setter等等，一直没发现问题在哪。跟负责前端的同学交流了下，发现前端post的数据确实是修改过的，也就是后端接收有问题。</p>\n<p>于是把最新版本和历史版本对比，发现最新版本新增了一个LogFilter，用于记录pagedelay的，仔细一看，logFilter里面是</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">request.setCharacterEncoding(“UTF-<span class=\"number\">8</span>″);</div><div class=\"line\">response.setContentType(“text/html;charset=UTF-<span class=\"number\">8</span>″);</div></pre></td></tr></table></figure>\n<p>但页面上是GBK编码，所以导致数据在这个filter中编码出错，造成数据丢失，后端接收到的数据为null。</p>\n<font color='red'>解决方法：</font>\n\n<p>把logFilter里的UTF-8改为GBK，就一切正常了。</p>\n<font color='red'>疑问：</font>\n\n<ol>\n<li><p>为何历史本没问题呢，因为历史版本中的logFilter配在struts2Filter之后，请求根本走不到logFilter里去。。。。</p>\n</li>\n<li><p>为何协同开发的同学本地调试没问题呢，那是因为他把web.xml里的LogFilter的filtermapping注掉了。。。。</p>\n</li>\n</ol>\n<p>好一个歪萝卜大烂坑。。。</p>\n"},{"title":"将Hadoop RPC框架应用于多节点任务调度","date":"2013-01-21T12:53:38.000Z","_content":"背景\n--\n在hadoop中，主从节点之间保持着心跳通信，用于传输节点状态信息、任务调度信息以及节点动作信息等等。 hdfs的namenode与datanode，mapreduce的jobtracker与tasktracker，hbase的hmaster与 regionserver之间的通信，都是基于hadoop RPC。Hadoop RPC是hadoop里非常基础的通信框架。hadoop 2.0以前hadoop RPC的数据序列化是通过实现自己定义的Writable接口实现，而从hadoop 2.0开始，数据的序列化工作交给了ProtocolBuffer去做。关于Hadoop RPC的实现原理已经有很多文章进行了详细的介绍（[源码级强力分析hadoop的RPC机制](http://weixiaolu.iteye.com/blog/1504898)，[Hadoop基于Protocol Buffer的RPC实现代码分析-Server端](http://yanbohappy.sinaapp.com/?p=110)，[带有HA功能的Hadoop Client端RPC实现原理与代码分析](http://yanbohappy.sinaapp.com/?p=115)），这里就不在赘述了。下面就直接引入问题和方案吧。  \n\n问题\n--\n工作中经常需要在定时任务系统上写一些定时任务，随着业务规模的增长和扩大，需要定时处理的任务越来越多，任务之间的执行间隔越来越小，某一时间段内（比如0点、整点或半点）执行的任务会越来越密集，只在一台机器上执行这些任务的话，会出现较大的风险：  \n* 任务并发度较高时，单机的系统资源将成为瓶颈  \n* 如果一个任务的运行占用了整个机器的大部分资源，比如sql查询耗费巨大内存和CPU资源，将直接影响其他任务的运行  \n* 任务失败后，如果仍然在同一台节点自动重新执行，失败率较高  \n* 机器宕机后，必须第一时间重启机器或重新部署定时任务系统，所有任务都不能按时执行  \n* 等等  \n\n方案\n--\n可想而知的是，可以通过将定时任务系统进行分布式改造，使用多个节点执行任务，将任务分发到不同节点上进行处理，并且完善失败重试机制，从而提高系统稳定性，实现任务系统的高可靠。  \n既然是在多个节点之间分发任务，肯定得有个任务的管理者(主节点)，在我们现有的系统中，也就是一套可以部署定时任务的web系统，任务代码更新后，部署好这套web系统，即可通过web页面设置定时任务并且进行调度(在单个节点上执行)。执行任务的节点(子节点)有多个以后，如何分发任务到子节点呢，我们可以把任务的信息封装成一个bean，通过RPC发布给子节点，子节点通过这个任务bean获得任务信息，并在指定的时刻执行任务。同时，子节点可以通过与主节点的心跳通信将节点状态和执行任务的情况告诉主节点。  \n这样其实就与hadoop mapreduce分发任务有点相似了，呵呵，这里主节点与子节点之间的通信，我们就可以通过Hadoop RPC框架来实现了，不同的是，我们分发的任务是定时任务，发布任务时需要将任务的定时信息一并发给子节点。  \n\n实现\n--\n单点的定时任务系统是基于Quartz的，在分布式环境下，可以继续基于Quartz进行改造，任务的定时信息可以通过Quartz中的JobDetail和Trigger对象来描述并封装，加上任务执行的入口类信息，再通过RPC由主节点发给子节点。子节点收到封装好的任务信息对象后，再构造JobDetail和Trigger，设置好启动时间后，通过入口类启动任务。下面是一个简单的demo。  \n<!--more-->\n以下是一个简单的定时任务信息描述对象CronJobInfo，包括JobDetailInfo和TriggerInfo两个属性：  \n```java\n/**\n* 定时任务信息，包括任务信息和触发器信息\n*/\npublic class CronJobInfo implements Writable\n{\n    private JobDetailInfo jobDetailInfo = new JobDetailInfo();\n    private TriggerInfo triggerInfo = new TriggerInfo();\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n        jobDetailInfo.readFields(in);\n        triggerInfo.readFields(in);\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n        jobDetailInfo.write(out);\n        triggerInfo.write(out);\n    }\n    // getters and setters...\n}\n```  \n  \n  \n任务信息JobDetailInfo，由主节点构造，子节点解析构造JobDetail对象：  \n```java\npublic class JobDetailInfo implements Writable\n{\n    private String name; // 任务名称\n    private String group = Scheduler.DEFAULT_GROUP; // 任务组\n    private String description; // 任务描述\n    private Class jobClass; // 任务的启动类\n    private JobDataMap jobDataMap; // 任务所需的参数，用来给作业提供数据支持的数据结构\n    private boolean volatility = false; // <span>重启应用之后是否删除任务的相关信息,</span>\n    private boolean durability = false; // 任务完成之后是否依然保留到数据库\n    private boolean shouldRecover = false; // 应用重启之后时候忽略过期任务\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n        name = WritableUtils.readString(in);\n        group = WritableUtils.readString(in);\n        description = WritableUtils.readString(in);\n        String className = WritableUtils.readString(in);\n        if (className != null)\n        {\n          try\n          {\n             jobClass = Class.forName(new String(className));\n          }\n          catch (ClassNotFoundException e)\n          {\n             e.printStackTrace();\n          }\n        }\n        int dataMapSize = WritableUtils.readVInt(in);\n        while (dataMapSize-- > 0)\n        {\n           String key = WritableUtils.readString(in);\n           String value = WritableUtils.readString(in);\n           jobDataMap.put(key, value);\n        }\n        volatility = in.readBoolean();\n        durability = in.readBoolean();\n        shouldRecover = in.readBoolean();\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n        WritableUtils.writeString(out, name);\n        WritableUtils.writeString(out, group);\n        WritableUtils.writeString(out, description);\n        WritableUtils.writeString(out, jobClass.getName());\n        if (jobDataMap == null)\n            WritableUtils.writeVInt(out, 0);\n        else\n        {\n            WritableUtils.writeVInt(out, jobDataMap.size());\n            for (Object k : jobDataMap.keySet())\n            {\n                WritableUtils.writeString(out, k.toString());\n                WritableUtils.writeString(out, jobDataMap.get(k).toString());\n            }\n        }\n        out.writeBoolean(volatility);\n        out.writeBoolean(durability);\n        out.writeBoolean(shouldRecover);\n   }\n   //getters and setters\n   //.....\n}\n```  \n  \n  \n任务触发器信息TriggerInfo ，由主节点构造，子节点解析构造Trigger对象：  \n```java\npublic class TriggerInfo implements Writable\n{\n    private String name; // trigger名称\n    private String group = Scheduler.DEFAULT_GROUP; // triger组名称\n    private String description; // trigger描述\n    private Date startTime; // 启动时间\n    private Date endTime; // 结束时间\n    private long repeatInterval; // 重试时间间隔\n    private int repeatCount; //重试次数\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n       name = WritableUtils.readString(in);\n       group = WritableUtils.readString(in);\n       description = WritableUtils.readString(in);\n       long start = in.readLong();\n       startTime = start==0 ? null : new Date(start);\n       long end = in.readLong();\n       endTime = end==0 ? null : new Date(end);\n       repeatInterval = in.readLong();\n       repeatCount = in.readInt();\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n       WritableUtils.writeString(out, name);\n       WritableUtils.writeString(out, group);\n       WritableUtils.writeString(out, description);\n       out.writeLong(startTime == null ? 0 : startTime.getTime());\n       out.writeLong(endTime == null ? 0 : endTime.getTime());\n       out.writeLong(repeatInterval);\n       out.writeInt(repeatCount);\n    }\n    //getters and setters\n    //.....\n}\n```  \n  \n  \n主从节点通信的协议：  \n```java\npublic interface TaskProtocol extends VersionedProtocol\n{\n    public CronJobInfo hearbeat();\n}\n```  \n  \n在这个demo中，主节点启动后，启动RPC server线程，等待客户端（子节点）的连接，当客户端调用heartbeat方法时，主节点将会生成一个任务信息返回给客户端：  \n```java\npublic class TaskScheduler implements TaskProtocol\n{\n    private Logger logger = Logger.getLogger(getClass());\n    private Server server;\n \n    public TaskScheduler()\n    {\n        try\n        {\n            server = RPC.getServer(this, \"192.168.1.101\", 8888, new Configuration());\n            server.start();\n            server.join();\n        }\n        catch (UnknownHostException e)\n        {\n            e.printStackTrace();\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n        catch (InterruptedException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    @Override\n    public long getProtocolVersion(String arg0, long arg1) throws IOException\n    {\n        return 1;\n    }\n \n    @Override\n    public CronJobInfo generateCronJob()\n    {\n        // 1、创建JobDetial对象\n        JobDetailInfo detail = new JobDetailInfo();\n        // 设置工作项\n        detail.setJobClass(DemoTask.class);\n        detail.setName(\"MyJob_1\");\n        detail.setGroup(\"JobGroup_1\");\n \n        // 2、创建Trigger对象\n        TriggerInfo trigger = new TriggerInfo();\n        trigger.setName(\"Trigger_1\");\n        trigger.setGroup(\"Trigger_Group_1\");\n        trigger.setStartTime(new Date());\n        // 设置重复停止时间，并销毁该Trigger对象\n        Calendar c = Calendar.getInstance();\n        c.setTimeInMillis(System.currentTimeMillis() + 1000 * 1L);\n        trigger.setEndTime(c.getTime());\n        // 设置重复间隔时间\n        trigger.setRepeatInterval(1000 * 1L);\n        // 设置重复执行次数\n        trigger.setRepeatCount(3);\n \n        CronJobInfo info = new CronJobInfo();\n        info.setJobDetailInfo(detail);\n        info.setTriggerInfo(trigger);\n \n        return info;\n    }\n \n    public static void main(String[] args)\n    {\n        TaskScheduler ts = new TaskScheduler();\n    }\n \n}\n```  \n  \ndemo任务类，打印信息：  \n```java\npublic class DemoTask implements Job\n{\n    public void execute(JobExecutionContext context)\n            throws JobExecutionException\n    {\n        System.out.println(this + \": executing task @\" + new Date());\n    }\n} \n```  \n  \n子节点demo，启动后连接主节点，远程调用generateCronJob方法，获得一个任务描述信息，并启动定时任务。  \n```java\npublic class TaskRunner\n{\n    private Logger logger = Logger.getLogger(getClass());\n    private TaskProtocol proxy;\n \n    public TaskRunner()\n    {\n        InetSocketAddress addr = new InetSocketAddress(\"localhost\", 8888);\n        try\n        {\n            proxy = (TaskProtocol) RPC.waitForProxy(TaskProtocol.class, 1, addr,\n                    new Configuration());\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    public void close()\n    {\n        RPC.stopProxy(proxy);\n    }\n \n    /**\n     * 从server获取一个定时任务\n     */\n    public void getCronJob()\n    {\n        CronJobInfo info = proxy.generateCronJob();\n        JobDetail jobDetail = getJobDetail(info.getJobDetailInfo());\n        SimpleTrigger trigger = getTrigger(info.getTriggerInfo());\n \n        // 创建Scheduler对象，并配置JobDetail和Trigger对象\n        SchedulerFactory sf = new StdSchedulerFactory();\n        Scheduler scheduler = null;\n        try\n        {\n            scheduler = sf.getScheduler();\n            scheduler.scheduleJob(jobDetail, trigger);\n            // 执行启动操作\n            scheduler.start();\n \n        }\n        catch (SchedulerException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    /**\n     * @param jobDetailInfo\n     * @return\n     */\n    private JobDetail getJobDetail(JobDetailInfo info)\n    {\n        JobDetail detail = new JobDetail();\n        detail.setName(info.getName());\n        detail.setGroup(info.getGroup());\n        detail.setDescription(info.getDescription());\n        detail.setJobClass(info.getJobClass());\n        detail.setJobDataMap(info.getJobDataMap());\n        detail.setRequestsRecovery(info.isShouldRecover());\n        detail.setDurability(info.isDurability());\n        detail.setVolatility(info.isVolatility());\n        logger.info(\"client get jobdetail:\" + detail);\n        return detail;\n    }\n \n    /**\n     * @param triggerInfo\n     * @return\n     */\n    private SimpleTrigger getTrigger(TriggerInfo info)\n    {\n        SimpleTrigger trigger = new SimpleTrigger();\n        trigger.setName(info.getName());\n        trigger.setGroup(info.getGroup());\n        trigger.setDescription(info.getDescription());\n        trigger.setStartTime(info.getStartTime());\n        trigger.setEndTime(info.getEndTime());\n        trigger.setRepeatInterval(info.getRepeatInterval());\n        trigger.setRepeatCount(info.getRepeatCount());\n        logger.info(\"client get trigger:\" + trigger);\n        return trigger;\n    }\n \n    public static void main(String[] args)\n    {\n        TaskRunner t = new TaskRunner();\n        t.getCronJob();\n        t.close();\n    }\n}\n```  \n  \n先启动TaskScheduler，再启动TaskRunner，结果如下：  \n\n> TaskScheduler日志:\n> 2013-01-20 15:42:21,661 [Socket Reader #1 for port 8888] INFO  [org.apache.hadoop.ipc.Server] – Starting Socket Reader #1 for port 8888\n> 2013-01-20 15:42:21,662 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888\n> 2013-01-20 15:42:21,706 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcDetailedMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888\n> 2013-01-20 15:42:21,710 [IPC Server listener on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server listener on 8888: starting\n> 2013-01-20 15:42:21,711 [IPC Server Responder] INFO  [org.apache.hadoop.ipc.Server] – IPC Server Responder: starting\n> 2013-01-20 15:42:21,711 [IPC Server handler 0 on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server handler 0 on 8888: starting\n> 2013-01-20 15:42:24,084 [IPC Server handler 0 on 8888] INFO  [org.mh.rpc.task.TaskScheduler] – generate a task: org.mh.rpc.task.JobDetailInfo@1f26605\n> \n> TaskRunner:\n> 2013-01-20 15:42:26,323 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get jobdetail:JobDetail ‘JobGroup_1.MyJob_1′:  jobClass: ‘org.mh.rpc.quartz.GetSumTask isStateful: false isVolatile: false isDurable: false requestsRecovers: false\n> 2013-01-20 15:42:26,329 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get trigger:Trigger ‘Trigger_Group_1.Trigger_1′:  triggerClass: ‘org.quartz.SimpleTrigger isVolatile: false calendar: ‘null’ misfireInstruction: 0 nextFireTime: null\n> 2013-01-20 15:42:26,382 [main] INFO  [org.quartz.simpl.SimpleThreadPool] – Job execution threads will use class loader of thread: main\n> 2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.SchedulerSignalerImpl] – Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl\n> 2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.QuartzScheduler] – Quartz Scheduler v.1.6.5 created.\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.simpl.RAMJobStore] – RAMJobStore initialized.\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler ‘DefaultQuartzScheduler’ initialized from default resource file in Quartz package: ‘quartz.properties’\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler version: 1.6.5\n> 2013-01-20 15:42:26,415 [main] INFO  [org.quartz.core.QuartzScheduler] – Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.\n> org.mh.rpc.quartz.DemoTask@1b66b06: executing task @Sun Jan 20 15:42:26 CST 2013\n\n上面是一个简单的demo，演示了如何通过RPC将任务调度给节点去执行，对于Quartz来说，任务的形式可以千变万化，关键就看怎么去使用了，分发到多个节点上执行的话，就还需要对任务的信息做更多的封装了。\n\n","source":"_posts/将Hadoop-RPC框架应用于多节点任务调度.md","raw":"---\ntitle: 将Hadoop RPC框架应用于多节点任务调度\ndate: 2013-01-21 20:53:38\ntags: \n- hadoop\n- RPC\n- 任务调度\n- 分布式应用\ncategories: \n- Hadoop\n---\n背景\n--\n在hadoop中，主从节点之间保持着心跳通信，用于传输节点状态信息、任务调度信息以及节点动作信息等等。 hdfs的namenode与datanode，mapreduce的jobtracker与tasktracker，hbase的hmaster与 regionserver之间的通信，都是基于hadoop RPC。Hadoop RPC是hadoop里非常基础的通信框架。hadoop 2.0以前hadoop RPC的数据序列化是通过实现自己定义的Writable接口实现，而从hadoop 2.0开始，数据的序列化工作交给了ProtocolBuffer去做。关于Hadoop RPC的实现原理已经有很多文章进行了详细的介绍（[源码级强力分析hadoop的RPC机制](http://weixiaolu.iteye.com/blog/1504898)，[Hadoop基于Protocol Buffer的RPC实现代码分析-Server端](http://yanbohappy.sinaapp.com/?p=110)，[带有HA功能的Hadoop Client端RPC实现原理与代码分析](http://yanbohappy.sinaapp.com/?p=115)），这里就不在赘述了。下面就直接引入问题和方案吧。  \n\n问题\n--\n工作中经常需要在定时任务系统上写一些定时任务，随着业务规模的增长和扩大，需要定时处理的任务越来越多，任务之间的执行间隔越来越小，某一时间段内（比如0点、整点或半点）执行的任务会越来越密集，只在一台机器上执行这些任务的话，会出现较大的风险：  \n* 任务并发度较高时，单机的系统资源将成为瓶颈  \n* 如果一个任务的运行占用了整个机器的大部分资源，比如sql查询耗费巨大内存和CPU资源，将直接影响其他任务的运行  \n* 任务失败后，如果仍然在同一台节点自动重新执行，失败率较高  \n* 机器宕机后，必须第一时间重启机器或重新部署定时任务系统，所有任务都不能按时执行  \n* 等等  \n\n方案\n--\n可想而知的是，可以通过将定时任务系统进行分布式改造，使用多个节点执行任务，将任务分发到不同节点上进行处理，并且完善失败重试机制，从而提高系统稳定性，实现任务系统的高可靠。  \n既然是在多个节点之间分发任务，肯定得有个任务的管理者(主节点)，在我们现有的系统中，也就是一套可以部署定时任务的web系统，任务代码更新后，部署好这套web系统，即可通过web页面设置定时任务并且进行调度(在单个节点上执行)。执行任务的节点(子节点)有多个以后，如何分发任务到子节点呢，我们可以把任务的信息封装成一个bean，通过RPC发布给子节点，子节点通过这个任务bean获得任务信息，并在指定的时刻执行任务。同时，子节点可以通过与主节点的心跳通信将节点状态和执行任务的情况告诉主节点。  \n这样其实就与hadoop mapreduce分发任务有点相似了，呵呵，这里主节点与子节点之间的通信，我们就可以通过Hadoop RPC框架来实现了，不同的是，我们分发的任务是定时任务，发布任务时需要将任务的定时信息一并发给子节点。  \n\n实现\n--\n单点的定时任务系统是基于Quartz的，在分布式环境下，可以继续基于Quartz进行改造，任务的定时信息可以通过Quartz中的JobDetail和Trigger对象来描述并封装，加上任务执行的入口类信息，再通过RPC由主节点发给子节点。子节点收到封装好的任务信息对象后，再构造JobDetail和Trigger，设置好启动时间后，通过入口类启动任务。下面是一个简单的demo。  \n<!--more-->\n以下是一个简单的定时任务信息描述对象CronJobInfo，包括JobDetailInfo和TriggerInfo两个属性：  \n```java\n/**\n* 定时任务信息，包括任务信息和触发器信息\n*/\npublic class CronJobInfo implements Writable\n{\n    private JobDetailInfo jobDetailInfo = new JobDetailInfo();\n    private TriggerInfo triggerInfo = new TriggerInfo();\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n        jobDetailInfo.readFields(in);\n        triggerInfo.readFields(in);\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n        jobDetailInfo.write(out);\n        triggerInfo.write(out);\n    }\n    // getters and setters...\n}\n```  \n  \n  \n任务信息JobDetailInfo，由主节点构造，子节点解析构造JobDetail对象：  \n```java\npublic class JobDetailInfo implements Writable\n{\n    private String name; // 任务名称\n    private String group = Scheduler.DEFAULT_GROUP; // 任务组\n    private String description; // 任务描述\n    private Class jobClass; // 任务的启动类\n    private JobDataMap jobDataMap; // 任务所需的参数，用来给作业提供数据支持的数据结构\n    private boolean volatility = false; // <span>重启应用之后是否删除任务的相关信息,</span>\n    private boolean durability = false; // 任务完成之后是否依然保留到数据库\n    private boolean shouldRecover = false; // 应用重启之后时候忽略过期任务\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n        name = WritableUtils.readString(in);\n        group = WritableUtils.readString(in);\n        description = WritableUtils.readString(in);\n        String className = WritableUtils.readString(in);\n        if (className != null)\n        {\n          try\n          {\n             jobClass = Class.forName(new String(className));\n          }\n          catch (ClassNotFoundException e)\n          {\n             e.printStackTrace();\n          }\n        }\n        int dataMapSize = WritableUtils.readVInt(in);\n        while (dataMapSize-- > 0)\n        {\n           String key = WritableUtils.readString(in);\n           String value = WritableUtils.readString(in);\n           jobDataMap.put(key, value);\n        }\n        volatility = in.readBoolean();\n        durability = in.readBoolean();\n        shouldRecover = in.readBoolean();\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n        WritableUtils.writeString(out, name);\n        WritableUtils.writeString(out, group);\n        WritableUtils.writeString(out, description);\n        WritableUtils.writeString(out, jobClass.getName());\n        if (jobDataMap == null)\n            WritableUtils.writeVInt(out, 0);\n        else\n        {\n            WritableUtils.writeVInt(out, jobDataMap.size());\n            for (Object k : jobDataMap.keySet())\n            {\n                WritableUtils.writeString(out, k.toString());\n                WritableUtils.writeString(out, jobDataMap.get(k).toString());\n            }\n        }\n        out.writeBoolean(volatility);\n        out.writeBoolean(durability);\n        out.writeBoolean(shouldRecover);\n   }\n   //getters and setters\n   //.....\n}\n```  \n  \n  \n任务触发器信息TriggerInfo ，由主节点构造，子节点解析构造Trigger对象：  \n```java\npublic class TriggerInfo implements Writable\n{\n    private String name; // trigger名称\n    private String group = Scheduler.DEFAULT_GROUP; // triger组名称\n    private String description; // trigger描述\n    private Date startTime; // 启动时间\n    private Date endTime; // 结束时间\n    private long repeatInterval; // 重试时间间隔\n    private int repeatCount; //重试次数\n \n    @Override\n    public void readFields(DataInput in) throws IOException\n    {\n       name = WritableUtils.readString(in);\n       group = WritableUtils.readString(in);\n       description = WritableUtils.readString(in);\n       long start = in.readLong();\n       startTime = start==0 ? null : new Date(start);\n       long end = in.readLong();\n       endTime = end==0 ? null : new Date(end);\n       repeatInterval = in.readLong();\n       repeatCount = in.readInt();\n    }\n \n    @Override\n    public void write(DataOutput out) throws IOException\n    {\n       WritableUtils.writeString(out, name);\n       WritableUtils.writeString(out, group);\n       WritableUtils.writeString(out, description);\n       out.writeLong(startTime == null ? 0 : startTime.getTime());\n       out.writeLong(endTime == null ? 0 : endTime.getTime());\n       out.writeLong(repeatInterval);\n       out.writeInt(repeatCount);\n    }\n    //getters and setters\n    //.....\n}\n```  \n  \n  \n主从节点通信的协议：  \n```java\npublic interface TaskProtocol extends VersionedProtocol\n{\n    public CronJobInfo hearbeat();\n}\n```  \n  \n在这个demo中，主节点启动后，启动RPC server线程，等待客户端（子节点）的连接，当客户端调用heartbeat方法时，主节点将会生成一个任务信息返回给客户端：  \n```java\npublic class TaskScheduler implements TaskProtocol\n{\n    private Logger logger = Logger.getLogger(getClass());\n    private Server server;\n \n    public TaskScheduler()\n    {\n        try\n        {\n            server = RPC.getServer(this, \"192.168.1.101\", 8888, new Configuration());\n            server.start();\n            server.join();\n        }\n        catch (UnknownHostException e)\n        {\n            e.printStackTrace();\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n        catch (InterruptedException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    @Override\n    public long getProtocolVersion(String arg0, long arg1) throws IOException\n    {\n        return 1;\n    }\n \n    @Override\n    public CronJobInfo generateCronJob()\n    {\n        // 1、创建JobDetial对象\n        JobDetailInfo detail = new JobDetailInfo();\n        // 设置工作项\n        detail.setJobClass(DemoTask.class);\n        detail.setName(\"MyJob_1\");\n        detail.setGroup(\"JobGroup_1\");\n \n        // 2、创建Trigger对象\n        TriggerInfo trigger = new TriggerInfo();\n        trigger.setName(\"Trigger_1\");\n        trigger.setGroup(\"Trigger_Group_1\");\n        trigger.setStartTime(new Date());\n        // 设置重复停止时间，并销毁该Trigger对象\n        Calendar c = Calendar.getInstance();\n        c.setTimeInMillis(System.currentTimeMillis() + 1000 * 1L);\n        trigger.setEndTime(c.getTime());\n        // 设置重复间隔时间\n        trigger.setRepeatInterval(1000 * 1L);\n        // 设置重复执行次数\n        trigger.setRepeatCount(3);\n \n        CronJobInfo info = new CronJobInfo();\n        info.setJobDetailInfo(detail);\n        info.setTriggerInfo(trigger);\n \n        return info;\n    }\n \n    public static void main(String[] args)\n    {\n        TaskScheduler ts = new TaskScheduler();\n    }\n \n}\n```  \n  \ndemo任务类，打印信息：  \n```java\npublic class DemoTask implements Job\n{\n    public void execute(JobExecutionContext context)\n            throws JobExecutionException\n    {\n        System.out.println(this + \": executing task @\" + new Date());\n    }\n} \n```  \n  \n子节点demo，启动后连接主节点，远程调用generateCronJob方法，获得一个任务描述信息，并启动定时任务。  \n```java\npublic class TaskRunner\n{\n    private Logger logger = Logger.getLogger(getClass());\n    private TaskProtocol proxy;\n \n    public TaskRunner()\n    {\n        InetSocketAddress addr = new InetSocketAddress(\"localhost\", 8888);\n        try\n        {\n            proxy = (TaskProtocol) RPC.waitForProxy(TaskProtocol.class, 1, addr,\n                    new Configuration());\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    public void close()\n    {\n        RPC.stopProxy(proxy);\n    }\n \n    /**\n     * 从server获取一个定时任务\n     */\n    public void getCronJob()\n    {\n        CronJobInfo info = proxy.generateCronJob();\n        JobDetail jobDetail = getJobDetail(info.getJobDetailInfo());\n        SimpleTrigger trigger = getTrigger(info.getTriggerInfo());\n \n        // 创建Scheduler对象，并配置JobDetail和Trigger对象\n        SchedulerFactory sf = new StdSchedulerFactory();\n        Scheduler scheduler = null;\n        try\n        {\n            scheduler = sf.getScheduler();\n            scheduler.scheduleJob(jobDetail, trigger);\n            // 执行启动操作\n            scheduler.start();\n \n        }\n        catch (SchedulerException e)\n        {\n            e.printStackTrace();\n        }\n    }\n \n    /**\n     * @param jobDetailInfo\n     * @return\n     */\n    private JobDetail getJobDetail(JobDetailInfo info)\n    {\n        JobDetail detail = new JobDetail();\n        detail.setName(info.getName());\n        detail.setGroup(info.getGroup());\n        detail.setDescription(info.getDescription());\n        detail.setJobClass(info.getJobClass());\n        detail.setJobDataMap(info.getJobDataMap());\n        detail.setRequestsRecovery(info.isShouldRecover());\n        detail.setDurability(info.isDurability());\n        detail.setVolatility(info.isVolatility());\n        logger.info(\"client get jobdetail:\" + detail);\n        return detail;\n    }\n \n    /**\n     * @param triggerInfo\n     * @return\n     */\n    private SimpleTrigger getTrigger(TriggerInfo info)\n    {\n        SimpleTrigger trigger = new SimpleTrigger();\n        trigger.setName(info.getName());\n        trigger.setGroup(info.getGroup());\n        trigger.setDescription(info.getDescription());\n        trigger.setStartTime(info.getStartTime());\n        trigger.setEndTime(info.getEndTime());\n        trigger.setRepeatInterval(info.getRepeatInterval());\n        trigger.setRepeatCount(info.getRepeatCount());\n        logger.info(\"client get trigger:\" + trigger);\n        return trigger;\n    }\n \n    public static void main(String[] args)\n    {\n        TaskRunner t = new TaskRunner();\n        t.getCronJob();\n        t.close();\n    }\n}\n```  \n  \n先启动TaskScheduler，再启动TaskRunner，结果如下：  \n\n> TaskScheduler日志:\n> 2013-01-20 15:42:21,661 [Socket Reader #1 for port 8888] INFO  [org.apache.hadoop.ipc.Server] – Starting Socket Reader #1 for port 8888\n> 2013-01-20 15:42:21,662 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888\n> 2013-01-20 15:42:21,706 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcDetailedMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888\n> 2013-01-20 15:42:21,710 [IPC Server listener on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server listener on 8888: starting\n> 2013-01-20 15:42:21,711 [IPC Server Responder] INFO  [org.apache.hadoop.ipc.Server] – IPC Server Responder: starting\n> 2013-01-20 15:42:21,711 [IPC Server handler 0 on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server handler 0 on 8888: starting\n> 2013-01-20 15:42:24,084 [IPC Server handler 0 on 8888] INFO  [org.mh.rpc.task.TaskScheduler] – generate a task: org.mh.rpc.task.JobDetailInfo@1f26605\n> \n> TaskRunner:\n> 2013-01-20 15:42:26,323 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get jobdetail:JobDetail ‘JobGroup_1.MyJob_1′:  jobClass: ‘org.mh.rpc.quartz.GetSumTask isStateful: false isVolatile: false isDurable: false requestsRecovers: false\n> 2013-01-20 15:42:26,329 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get trigger:Trigger ‘Trigger_Group_1.Trigger_1′:  triggerClass: ‘org.quartz.SimpleTrigger isVolatile: false calendar: ‘null’ misfireInstruction: 0 nextFireTime: null\n> 2013-01-20 15:42:26,382 [main] INFO  [org.quartz.simpl.SimpleThreadPool] – Job execution threads will use class loader of thread: main\n> 2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.SchedulerSignalerImpl] – Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl\n> 2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.QuartzScheduler] – Quartz Scheduler v.1.6.5 created.\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.simpl.RAMJobStore] – RAMJobStore initialized.\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler ‘DefaultQuartzScheduler’ initialized from default resource file in Quartz package: ‘quartz.properties’\n> 2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler version: 1.6.5\n> 2013-01-20 15:42:26,415 [main] INFO  [org.quartz.core.QuartzScheduler] – Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.\n> org.mh.rpc.quartz.DemoTask@1b66b06: executing task @Sun Jan 20 15:42:26 CST 2013\n\n上面是一个简单的demo，演示了如何通过RPC将任务调度给节点去执行，对于Quartz来说，任务的形式可以千变万化，关键就看怎么去使用了，分发到多个节点上执行的话，就还需要对任务的信息做更多的封装了。\n\n","slug":"将Hadoop-RPC框架应用于多节点任务调度","published":1,"updated":"2017-01-18T09:54:11.739Z","_id":"ciy320ajm0014ifs6j9g6f22o","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>在hadoop中，主从节点之间保持着心跳通信，用于传输节点状态信息、任务调度信息以及节点动作信息等等。 hdfs的namenode与datanode，mapreduce的jobtracker与tasktracker，hbase的hmaster与 regionserver之间的通信，都是基于hadoop RPC。Hadoop RPC是hadoop里非常基础的通信框架。hadoop 2.0以前hadoop RPC的数据序列化是通过实现自己定义的Writable接口实现，而从hadoop 2.0开始，数据的序列化工作交给了ProtocolBuffer去做。关于Hadoop RPC的实现原理已经有很多文章进行了详细的介绍（<a href=\"http://weixiaolu.iteye.com/blog/1504898\" target=\"_blank\" rel=\"external\">源码级强力分析hadoop的RPC机制</a>，<a href=\"http://yanbohappy.sinaapp.com/?p=110\" target=\"_blank\" rel=\"external\">Hadoop基于Protocol Buffer的RPC实现代码分析-Server端</a>，<a href=\"http://yanbohappy.sinaapp.com/?p=115\" target=\"_blank\" rel=\"external\">带有HA功能的Hadoop Client端RPC实现原理与代码分析</a>），这里就不在赘述了。下面就直接引入问题和方案吧。  </p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>工作中经常需要在定时任务系统上写一些定时任务，随着业务规模的增长和扩大，需要定时处理的任务越来越多，任务之间的执行间隔越来越小，某一时间段内（比如0点、整点或半点）执行的任务会越来越密集，只在一台机器上执行这些任务的话，会出现较大的风险：  </p>\n<ul>\n<li>任务并发度较高时，单机的系统资源将成为瓶颈  </li>\n<li>如果一个任务的运行占用了整个机器的大部分资源，比如sql查询耗费巨大内存和CPU资源，将直接影响其他任务的运行  </li>\n<li>任务失败后，如果仍然在同一台节点自动重新执行，失败率较高  </li>\n<li>机器宕机后，必须第一时间重启机器或重新部署定时任务系统，所有任务都不能按时执行  </li>\n<li>等等  </li>\n</ul>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><p>可想而知的是，可以通过将定时任务系统进行分布式改造，使用多个节点执行任务，将任务分发到不同节点上进行处理，并且完善失败重试机制，从而提高系统稳定性，实现任务系统的高可靠。<br>既然是在多个节点之间分发任务，肯定得有个任务的管理者(主节点)，在我们现有的系统中，也就是一套可以部署定时任务的web系统，任务代码更新后，部署好这套web系统，即可通过web页面设置定时任务并且进行调度(在单个节点上执行)。执行任务的节点(子节点)有多个以后，如何分发任务到子节点呢，我们可以把任务的信息封装成一个bean，通过RPC发布给子节点，子节点通过这个任务bean获得任务信息，并在指定的时刻执行任务。同时，子节点可以通过与主节点的心跳通信将节点状态和执行任务的情况告诉主节点。<br>这样其实就与hadoop mapreduce分发任务有点相似了，呵呵，这里主节点与子节点之间的通信，我们就可以通过Hadoop RPC框架来实现了，不同的是，我们分发的任务是定时任务，发布任务时需要将任务的定时信息一并发给子节点。  </p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>单点的定时任务系统是基于Quartz的，在分布式环境下，可以继续基于Quartz进行改造，任务的定时信息可以通过Quartz中的JobDetail和Trigger对象来描述并封装，加上任务执行的入口类信息，再通过RPC由主节点发给子节点。子节点收到封装好的任务信息对象后，再构造JobDetail和Trigger，设置好启动时间后，通过入口类启动任务。下面是一个简单的demo。<br><a id=\"more\"></a><br>以下是一个简单的定时任务信息描述对象CronJobInfo，包括JobDetailInfo和TriggerInfo两个属性：  </p>\n<pre><code class=\"java\"><span class=\"comment\">/**\n* 定时任务信息，包括任务信息和触发器信息\n*/</span>\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CronJobInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> JobDetailInfo jobDetailInfo = <span class=\"keyword\">new</span> JobDetailInfo();\n    <span class=\"keyword\">private</span> TriggerInfo triggerInfo = <span class=\"keyword\">new</span> TriggerInfo();\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        jobDetailInfo.readFields(in);\n        triggerInfo.readFields(in);\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        jobDetailInfo.write(out);\n        triggerInfo.write(out);\n    }\n    <span class=\"comment\">// getters and setters...</span>\n}\n</code></pre>\n<p>任务信息JobDetailInfo，由主节点构造，子节点解析构造JobDetail对象：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobDetailInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> String name; <span class=\"comment\">// 任务名称</span>\n    <span class=\"keyword\">private</span> String group = Scheduler.DEFAULT_GROUP; <span class=\"comment\">// 任务组</span>\n    <span class=\"keyword\">private</span> String description; <span class=\"comment\">// 任务描述</span>\n    <span class=\"keyword\">private</span> Class jobClass; <span class=\"comment\">// 任务的启动类</span>\n    <span class=\"keyword\">private</span> JobDataMap jobDataMap; <span class=\"comment\">// 任务所需的参数，用来给作业提供数据支持的数据结构</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> volatility = <span class=\"keyword\">false</span>; <span class=\"comment\">// &lt;span&gt;重启应用之后是否删除任务的相关信息,&lt;/span&gt;</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> durability = <span class=\"keyword\">false</span>; <span class=\"comment\">// 任务完成之后是否依然保留到数据库</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> shouldRecover = <span class=\"keyword\">false</span>; <span class=\"comment\">// 应用重启之后时候忽略过期任务</span>\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        name = WritableUtils.readString(in);\n        group = WritableUtils.readString(in);\n        description = WritableUtils.readString(in);\n        String className = WritableUtils.readString(in);\n        <span class=\"keyword\">if</span> (className != <span class=\"keyword\">null</span>)\n        {\n          <span class=\"keyword\">try</span>\n          {\n             jobClass = Class.forName(<span class=\"keyword\">new</span> String(className));\n          }\n          <span class=\"keyword\">catch</span> (ClassNotFoundException e)\n          {\n             e.printStackTrace();\n          }\n        }\n        <span class=\"keyword\">int</span> dataMapSize = WritableUtils.readVInt(in);\n        <span class=\"keyword\">while</span> (dataMapSize-- &gt; <span class=\"number\">0</span>)\n        {\n           String key = WritableUtils.readString(in);\n           String value = WritableUtils.readString(in);\n           jobDataMap.put(key, value);\n        }\n        volatility = in.readBoolean();\n        durability = in.readBoolean();\n        shouldRecover = in.readBoolean();\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        WritableUtils.writeString(out, name);\n        WritableUtils.writeString(out, group);\n        WritableUtils.writeString(out, description);\n        WritableUtils.writeString(out, jobClass.getName());\n        <span class=\"keyword\">if</span> (jobDataMap == <span class=\"keyword\">null</span>)\n            WritableUtils.writeVInt(out, <span class=\"number\">0</span>);\n        <span class=\"keyword\">else</span>\n        {\n            WritableUtils.writeVInt(out, jobDataMap.size());\n            <span class=\"keyword\">for</span> (Object k : jobDataMap.keySet())\n            {\n                WritableUtils.writeString(out, k.toString());\n                WritableUtils.writeString(out, jobDataMap.get(k).toString());\n            }\n        }\n        out.writeBoolean(volatility);\n        out.writeBoolean(durability);\n        out.writeBoolean(shouldRecover);\n   }\n   <span class=\"comment\">//getters and setters</span>\n   <span class=\"comment\">//.....</span>\n}\n</code></pre>\n<p>任务触发器信息TriggerInfo ，由主节点构造，子节点解析构造Trigger对象：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TriggerInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> String name; <span class=\"comment\">// trigger名称</span>\n    <span class=\"keyword\">private</span> String group = Scheduler.DEFAULT_GROUP; <span class=\"comment\">// triger组名称</span>\n    <span class=\"keyword\">private</span> String description; <span class=\"comment\">// trigger描述</span>\n    <span class=\"keyword\">private</span> Date startTime; <span class=\"comment\">// 启动时间</span>\n    <span class=\"keyword\">private</span> Date endTime; <span class=\"comment\">// 结束时间</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> repeatInterval; <span class=\"comment\">// 重试时间间隔</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> repeatCount; <span class=\"comment\">//重试次数</span>\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n       name = WritableUtils.readString(in);\n       group = WritableUtils.readString(in);\n       description = WritableUtils.readString(in);\n       <span class=\"keyword\">long</span> start = in.readLong();\n       startTime = start==<span class=\"number\">0</span> ? <span class=\"keyword\">null</span> : <span class=\"keyword\">new</span> Date(start);\n       <span class=\"keyword\">long</span> end = in.readLong();\n       endTime = end==<span class=\"number\">0</span> ? <span class=\"keyword\">null</span> : <span class=\"keyword\">new</span> Date(end);\n       repeatInterval = in.readLong();\n       repeatCount = in.readInt();\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n       WritableUtils.writeString(out, name);\n       WritableUtils.writeString(out, group);\n       WritableUtils.writeString(out, description);\n       out.writeLong(startTime == <span class=\"keyword\">null</span> ? <span class=\"number\">0</span> : startTime.getTime());\n       out.writeLong(endTime == <span class=\"keyword\">null</span> ? <span class=\"number\">0</span> : endTime.getTime());\n       out.writeLong(repeatInterval);\n       out.writeInt(repeatCount);\n    }\n    <span class=\"comment\">//getters and setters</span>\n    <span class=\"comment\">//.....</span>\n}\n</code></pre>\n<p>主从节点通信的协议：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">TaskProtocol</span> <span class=\"keyword\">extends</span> <span class=\"title\">VersionedProtocol</span>\n</span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> CronJobInfo <span class=\"title\">hearbeat</span><span class=\"params\">()</span></span>;\n}\n</code></pre>\n<p>在这个demo中，主节点启动后，启动RPC server线程，等待客户端（子节点）的连接，当客户端调用heartbeat方法时，主节点将会生成一个任务信息返回给客户端：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TaskScheduler</span> <span class=\"keyword\">implements</span> <span class=\"title\">TaskProtocol</span>\n</span>{\n    <span class=\"keyword\">private</span> Logger logger = Logger.getLogger(getClass());\n    <span class=\"keyword\">private</span> Server server;\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TaskScheduler</span><span class=\"params\">()</span>\n    </span>{\n        <span class=\"keyword\">try</span>\n        {\n            server = RPC.getServer(<span class=\"keyword\">this</span>, <span class=\"string\">\"192.168.1.101\"</span>, <span class=\"number\">8888</span>, <span class=\"keyword\">new</span> Configuration());\n            server.start();\n            server.join();\n        }\n        <span class=\"keyword\">catch</span> (UnknownHostException e)\n        {\n            e.printStackTrace();\n        }\n        <span class=\"keyword\">catch</span> (IOException e)\n        {\n            e.printStackTrace();\n        }\n        <span class=\"keyword\">catch</span> (InterruptedException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">getProtocolVersion</span><span class=\"params\">(String arg0, <span class=\"keyword\">long</span> arg1)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> CronJobInfo <span class=\"title\">generateCronJob</span><span class=\"params\">()</span>\n    </span>{\n        <span class=\"comment\">// 1、创建JobDetial对象</span>\n        JobDetailInfo detail = <span class=\"keyword\">new</span> JobDetailInfo();\n        <span class=\"comment\">// 设置工作项</span>\n        detail.setJobClass(DemoTask.class);\n        detail.setName(<span class=\"string\">\"MyJob_1\"</span>);\n        detail.setGroup(<span class=\"string\">\"JobGroup_1\"</span>);\n\n        <span class=\"comment\">// 2、创建Trigger对象</span>\n        TriggerInfo trigger = <span class=\"keyword\">new</span> TriggerInfo();\n        trigger.setName(<span class=\"string\">\"Trigger_1\"</span>);\n        trigger.setGroup(<span class=\"string\">\"Trigger_Group_1\"</span>);\n        trigger.setStartTime(<span class=\"keyword\">new</span> Date());\n        <span class=\"comment\">// 设置重复停止时间，并销毁该Trigger对象</span>\n        Calendar c = Calendar.getInstance();\n        c.setTimeInMillis(System.currentTimeMillis() + <span class=\"number\">1000</span> * <span class=\"number\">1L</span>);\n        trigger.setEndTime(c.getTime());\n        <span class=\"comment\">// 设置重复间隔时间</span>\n        trigger.setRepeatInterval(<span class=\"number\">1000</span> * <span class=\"number\">1L</span>);\n        <span class=\"comment\">// 设置重复执行次数</span>\n        trigger.setRepeatCount(<span class=\"number\">3</span>);\n\n        CronJobInfo info = <span class=\"keyword\">new</span> CronJobInfo();\n        info.setJobDetailInfo(detail);\n        info.setTriggerInfo(trigger);\n\n        <span class=\"keyword\">return</span> info;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span>\n    </span>{\n        TaskScheduler ts = <span class=\"keyword\">new</span> TaskScheduler();\n    }\n\n}\n</code></pre>\n<p>demo任务类，打印信息：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DemoTask</span> <span class=\"keyword\">implements</span> <span class=\"title\">Job</span>\n</span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(JobExecutionContext context)</span>\n            <span class=\"keyword\">throws</span> JobExecutionException\n    </span>{\n        System.out.println(<span class=\"keyword\">this</span> + <span class=\"string\">\": executing task @\"</span> + <span class=\"keyword\">new</span> Date());\n    }\n}\n</code></pre>\n<p>子节点demo，启动后连接主节点，远程调用generateCronJob方法，获得一个任务描述信息，并启动定时任务。  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TaskRunner</span>\n</span>{\n    <span class=\"keyword\">private</span> Logger logger = Logger.getLogger(getClass());\n    <span class=\"keyword\">private</span> TaskProtocol proxy;\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TaskRunner</span><span class=\"params\">()</span>\n    </span>{\n        InetSocketAddress addr = <span class=\"keyword\">new</span> InetSocketAddress(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">8888</span>);\n        <span class=\"keyword\">try</span>\n        {\n            proxy = (TaskProtocol) RPC.waitForProxy(TaskProtocol.class, <span class=\"number\">1</span>, addr,\n                    <span class=\"keyword\">new</span> Configuration());\n        }\n        <span class=\"keyword\">catch</span> (IOException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span>\n    </span>{\n        RPC.stopProxy(proxy);\n    }\n\n    <span class=\"comment\">/**\n     * 从server获取一个定时任务\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getCronJob</span><span class=\"params\">()</span>\n    </span>{\n        CronJobInfo info = proxy.generateCronJob();\n        JobDetail jobDetail = getJobDetail(info.getJobDetailInfo());\n        SimpleTrigger trigger = getTrigger(info.getTriggerInfo());\n\n        <span class=\"comment\">// 创建Scheduler对象，并配置JobDetail和Trigger对象</span>\n        SchedulerFactory sf = <span class=\"keyword\">new</span> StdSchedulerFactory();\n        Scheduler scheduler = <span class=\"keyword\">null</span>;\n        <span class=\"keyword\">try</span>\n        {\n            scheduler = sf.getScheduler();\n            scheduler.scheduleJob(jobDetail, trigger);\n            <span class=\"comment\">// 执行启动操作</span>\n            scheduler.start();\n\n        }\n        <span class=\"keyword\">catch</span> (SchedulerException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"comment\">/**\n     * <span class=\"doctag\">@param</span> jobDetailInfo\n     * <span class=\"doctag\">@return</span>\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">private</span> JobDetail <span class=\"title\">getJobDetail</span><span class=\"params\">(JobDetailInfo info)</span>\n    </span>{\n        JobDetail detail = <span class=\"keyword\">new</span> JobDetail();\n        detail.setName(info.getName());\n        detail.setGroup(info.getGroup());\n        detail.setDescription(info.getDescription());\n        detail.setJobClass(info.getJobClass());\n        detail.setJobDataMap(info.getJobDataMap());\n        detail.setRequestsRecovery(info.isShouldRecover());\n        detail.setDurability(info.isDurability());\n        detail.setVolatility(info.isVolatility());\n        logger.info(<span class=\"string\">\"client get jobdetail:\"</span> + detail);\n        <span class=\"keyword\">return</span> detail;\n    }\n\n    <span class=\"comment\">/**\n     * <span class=\"doctag\">@param</span> triggerInfo\n     * <span class=\"doctag\">@return</span>\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">private</span> SimpleTrigger <span class=\"title\">getTrigger</span><span class=\"params\">(TriggerInfo info)</span>\n    </span>{\n        SimpleTrigger trigger = <span class=\"keyword\">new</span> SimpleTrigger();\n        trigger.setName(info.getName());\n        trigger.setGroup(info.getGroup());\n        trigger.setDescription(info.getDescription());\n        trigger.setStartTime(info.getStartTime());\n        trigger.setEndTime(info.getEndTime());\n        trigger.setRepeatInterval(info.getRepeatInterval());\n        trigger.setRepeatCount(info.getRepeatCount());\n        logger.info(<span class=\"string\">\"client get trigger:\"</span> + trigger);\n        <span class=\"keyword\">return</span> trigger;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span>\n    </span>{\n        TaskRunner t = <span class=\"keyword\">new</span> TaskRunner();\n        t.getCronJob();\n        t.close();\n    }\n}\n</code></pre>\n<p>先启动TaskScheduler，再启动TaskRunner，结果如下：  </p>\n<blockquote>\n<p>TaskScheduler日志:<br>2013-01-20 15:42:21,661 [Socket Reader #1 for port 8888] INFO  [org.apache.hadoop.ipc.Server] – Starting Socket Reader #1 for port 8888<br>2013-01-20 15:42:21,662 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888<br>2013-01-20 15:42:21,706 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcDetailedMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888<br>2013-01-20 15:42:21,710 [IPC Server listener on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server listener on 8888: starting<br>2013-01-20 15:42:21,711 [IPC Server Responder] INFO  [org.apache.hadoop.ipc.Server] – IPC Server Responder: starting<br>2013-01-20 15:42:21,711 [IPC Server handler 0 on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server handler 0 on 8888: starting<br>2013-01-20 15:42:24,084 [IPC Server handler 0 on 8888] INFO  [org.mh.rpc.task.TaskScheduler] – generate a task: org.mh.rpc.task.JobDetailInfo@1f26605</p>\n<p>TaskRunner:<br>2013-01-20 15:42:26,323 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get jobdetail:JobDetail ‘JobGroup_1.MyJob_1′:  jobClass: ‘org.mh.rpc.quartz.GetSumTask isStateful: false isVolatile: false isDurable: false requestsRecovers: false<br>2013-01-20 15:42:26,329 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get trigger:Trigger ‘Trigger_Group_1.Trigger<em>1′:  triggerClass: ‘org.quartz.SimpleTrigger isVolatile: false calendar: ‘null’ misfireInstruction: 0 nextFireTime: null<br>2013-01-20 15:42:26,382 [main] INFO  [org.quartz.simpl.SimpleThreadPool] – Job execution threads will use class loader of thread: main<br>2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.SchedulerSignalerImpl] – Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl<br>2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.QuartzScheduler] – Quartz Scheduler v.1.6.5 created.<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.simpl.RAMJobStore] – RAMJobStore initialized.<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler ‘DefaultQuartzScheduler’ initialized from default resource file in Quartz package: ‘quartz.properties’<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler version: 1.6.5<br>2013-01-20 15:42:26,415 [main] INFO  [org.quartz.core.QuartzScheduler] – Scheduler DefaultQuartzScheduler</em>$_NON_CLUSTERED started.<br>org.mh.rpc.quartz.DemoTask@1b66b06: executing task @Sun Jan 20 15:42:26 CST 2013</p>\n</blockquote>\n<p>上面是一个简单的demo，演示了如何通过RPC将任务调度给节点去执行，对于Quartz来说，任务的形式可以千变万化，关键就看怎么去使用了，分发到多个节点上执行的话，就还需要对任务的信息做更多的封装了。</p>\n","excerpt":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>在hadoop中，主从节点之间保持着心跳通信，用于传输节点状态信息、任务调度信息以及节点动作信息等等。 hdfs的namenode与datanode，mapreduce的jobtracker与tasktracker，hbase的hmaster与 regionserver之间的通信，都是基于hadoop RPC。Hadoop RPC是hadoop里非常基础的通信框架。hadoop 2.0以前hadoop RPC的数据序列化是通过实现自己定义的Writable接口实现，而从hadoop 2.0开始，数据的序列化工作交给了ProtocolBuffer去做。关于Hadoop RPC的实现原理已经有很多文章进行了详细的介绍（<a href=\"http://weixiaolu.iteye.com/blog/1504898\">源码级强力分析hadoop的RPC机制</a>，<a href=\"http://yanbohappy.sinaapp.com/?p=110\">Hadoop基于Protocol Buffer的RPC实现代码分析-Server端</a>，<a href=\"http://yanbohappy.sinaapp.com/?p=115\">带有HA功能的Hadoop Client端RPC实现原理与代码分析</a>），这里就不在赘述了。下面就直接引入问题和方案吧。  </p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>工作中经常需要在定时任务系统上写一些定时任务，随着业务规模的增长和扩大，需要定时处理的任务越来越多，任务之间的执行间隔越来越小，某一时间段内（比如0点、整点或半点）执行的任务会越来越密集，只在一台机器上执行这些任务的话，会出现较大的风险：  </p>\n<ul>\n<li>任务并发度较高时，单机的系统资源将成为瓶颈  </li>\n<li>如果一个任务的运行占用了整个机器的大部分资源，比如sql查询耗费巨大内存和CPU资源，将直接影响其他任务的运行  </li>\n<li>任务失败后，如果仍然在同一台节点自动重新执行，失败率较高  </li>\n<li>机器宕机后，必须第一时间重启机器或重新部署定时任务系统，所有任务都不能按时执行  </li>\n<li>等等  </li>\n</ul>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><p>可想而知的是，可以通过将定时任务系统进行分布式改造，使用多个节点执行任务，将任务分发到不同节点上进行处理，并且完善失败重试机制，从而提高系统稳定性，实现任务系统的高可靠。<br>既然是在多个节点之间分发任务，肯定得有个任务的管理者(主节点)，在我们现有的系统中，也就是一套可以部署定时任务的web系统，任务代码更新后，部署好这套web系统，即可通过web页面设置定时任务并且进行调度(在单个节点上执行)。执行任务的节点(子节点)有多个以后，如何分发任务到子节点呢，我们可以把任务的信息封装成一个bean，通过RPC发布给子节点，子节点通过这个任务bean获得任务信息，并在指定的时刻执行任务。同时，子节点可以通过与主节点的心跳通信将节点状态和执行任务的情况告诉主节点。<br>这样其实就与hadoop mapreduce分发任务有点相似了，呵呵，这里主节点与子节点之间的通信，我们就可以通过Hadoop RPC框架来实现了，不同的是，我们分发的任务是定时任务，发布任务时需要将任务的定时信息一并发给子节点。  </p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>单点的定时任务系统是基于Quartz的，在分布式环境下，可以继续基于Quartz进行改造，任务的定时信息可以通过Quartz中的JobDetail和Trigger对象来描述并封装，加上任务执行的入口类信息，再通过RPC由主节点发给子节点。子节点收到封装好的任务信息对象后，再构造JobDetail和Trigger，设置好启动时间后，通过入口类启动任务。下面是一个简单的demo。<br>","more":"<br>以下是一个简单的定时任务信息描述对象CronJobInfo，包括JobDetailInfo和TriggerInfo两个属性：  </p>\n<pre><code class=\"java\"><span class=\"comment\">/**\n* 定时任务信息，包括任务信息和触发器信息\n*/</span>\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CronJobInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> JobDetailInfo jobDetailInfo = <span class=\"keyword\">new</span> JobDetailInfo();\n    <span class=\"keyword\">private</span> TriggerInfo triggerInfo = <span class=\"keyword\">new</span> TriggerInfo();\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        jobDetailInfo.readFields(in);\n        triggerInfo.readFields(in);\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        jobDetailInfo.write(out);\n        triggerInfo.write(out);\n    }\n    <span class=\"comment\">// getters and setters...</span>\n}\n</code></pre>\n<p>任务信息JobDetailInfo，由主节点构造，子节点解析构造JobDetail对象：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobDetailInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> String name; <span class=\"comment\">// 任务名称</span>\n    <span class=\"keyword\">private</span> String group = Scheduler.DEFAULT_GROUP; <span class=\"comment\">// 任务组</span>\n    <span class=\"keyword\">private</span> String description; <span class=\"comment\">// 任务描述</span>\n    <span class=\"keyword\">private</span> Class jobClass; <span class=\"comment\">// 任务的启动类</span>\n    <span class=\"keyword\">private</span> JobDataMap jobDataMap; <span class=\"comment\">// 任务所需的参数，用来给作业提供数据支持的数据结构</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> volatility = <span class=\"keyword\">false</span>; <span class=\"comment\">// &lt;span&gt;重启应用之后是否删除任务的相关信息,&lt;/span&gt;</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> durability = <span class=\"keyword\">false</span>; <span class=\"comment\">// 任务完成之后是否依然保留到数据库</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> shouldRecover = <span class=\"keyword\">false</span>; <span class=\"comment\">// 应用重启之后时候忽略过期任务</span>\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        name = WritableUtils.readString(in);\n        group = WritableUtils.readString(in);\n        description = WritableUtils.readString(in);\n        String className = WritableUtils.readString(in);\n        <span class=\"keyword\">if</span> (className != <span class=\"keyword\">null</span>)\n        {\n          <span class=\"keyword\">try</span>\n          {\n             jobClass = Class.forName(<span class=\"keyword\">new</span> String(className));\n          }\n          <span class=\"keyword\">catch</span> (ClassNotFoundException e)\n          {\n             e.printStackTrace();\n          }\n        }\n        <span class=\"keyword\">int</span> dataMapSize = WritableUtils.readVInt(in);\n        <span class=\"keyword\">while</span> (dataMapSize-- &gt; <span class=\"number\">0</span>)\n        {\n           String key = WritableUtils.readString(in);\n           String value = WritableUtils.readString(in);\n           jobDataMap.put(key, value);\n        }\n        volatility = in.readBoolean();\n        durability = in.readBoolean();\n        shouldRecover = in.readBoolean();\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        WritableUtils.writeString(out, name);\n        WritableUtils.writeString(out, group);\n        WritableUtils.writeString(out, description);\n        WritableUtils.writeString(out, jobClass.getName());\n        <span class=\"keyword\">if</span> (jobDataMap == <span class=\"keyword\">null</span>)\n            WritableUtils.writeVInt(out, <span class=\"number\">0</span>);\n        <span class=\"keyword\">else</span>\n        {\n            WritableUtils.writeVInt(out, jobDataMap.size());\n            <span class=\"keyword\">for</span> (Object k : jobDataMap.keySet())\n            {\n                WritableUtils.writeString(out, k.toString());\n                WritableUtils.writeString(out, jobDataMap.get(k).toString());\n            }\n        }\n        out.writeBoolean(volatility);\n        out.writeBoolean(durability);\n        out.writeBoolean(shouldRecover);\n   }\n   <span class=\"comment\">//getters and setters</span>\n   <span class=\"comment\">//.....</span>\n}\n</code></pre>\n<p>任务触发器信息TriggerInfo ，由主节点构造，子节点解析构造Trigger对象：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TriggerInfo</span> <span class=\"keyword\">implements</span> <span class=\"title\">Writable</span>\n</span>{\n    <span class=\"keyword\">private</span> String name; <span class=\"comment\">// trigger名称</span>\n    <span class=\"keyword\">private</span> String group = Scheduler.DEFAULT_GROUP; <span class=\"comment\">// triger组名称</span>\n    <span class=\"keyword\">private</span> String description; <span class=\"comment\">// trigger描述</span>\n    <span class=\"keyword\">private</span> Date startTime; <span class=\"comment\">// 启动时间</span>\n    <span class=\"keyword\">private</span> Date endTime; <span class=\"comment\">// 结束时间</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> repeatInterval; <span class=\"comment\">// 重试时间间隔</span>\n    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> repeatCount; <span class=\"comment\">//重试次数</span>\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n       name = WritableUtils.readString(in);\n       group = WritableUtils.readString(in);\n       description = WritableUtils.readString(in);\n       <span class=\"keyword\">long</span> start = in.readLong();\n       startTime = start==<span class=\"number\">0</span> ? <span class=\"keyword\">null</span> : <span class=\"keyword\">new</span> Date(start);\n       <span class=\"keyword\">long</span> end = in.readLong();\n       endTime = end==<span class=\"number\">0</span> ? <span class=\"keyword\">null</span> : <span class=\"keyword\">new</span> Date(end);\n       repeatInterval = in.readLong();\n       repeatCount = in.readInt();\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n       WritableUtils.writeString(out, name);\n       WritableUtils.writeString(out, group);\n       WritableUtils.writeString(out, description);\n       out.writeLong(startTime == <span class=\"keyword\">null</span> ? <span class=\"number\">0</span> : startTime.getTime());\n       out.writeLong(endTime == <span class=\"keyword\">null</span> ? <span class=\"number\">0</span> : endTime.getTime());\n       out.writeLong(repeatInterval);\n       out.writeInt(repeatCount);\n    }\n    <span class=\"comment\">//getters and setters</span>\n    <span class=\"comment\">//.....</span>\n}\n</code></pre>\n<p>主从节点通信的协议：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">TaskProtocol</span> <span class=\"keyword\">extends</span> <span class=\"title\">VersionedProtocol</span>\n</span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> CronJobInfo <span class=\"title\">hearbeat</span><span class=\"params\">()</span></span>;\n}\n</code></pre>\n<p>在这个demo中，主节点启动后，启动RPC server线程，等待客户端（子节点）的连接，当客户端调用heartbeat方法时，主节点将会生成一个任务信息返回给客户端：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TaskScheduler</span> <span class=\"keyword\">implements</span> <span class=\"title\">TaskProtocol</span>\n</span>{\n    <span class=\"keyword\">private</span> Logger logger = Logger.getLogger(getClass());\n    <span class=\"keyword\">private</span> Server server;\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TaskScheduler</span><span class=\"params\">()</span>\n    </span>{\n        <span class=\"keyword\">try</span>\n        {\n            server = RPC.getServer(<span class=\"keyword\">this</span>, <span class=\"string\">\"192.168.1.101\"</span>, <span class=\"number\">8888</span>, <span class=\"keyword\">new</span> Configuration());\n            server.start();\n            server.join();\n        }\n        <span class=\"keyword\">catch</span> (UnknownHostException e)\n        {\n            e.printStackTrace();\n        }\n        <span class=\"keyword\">catch</span> (IOException e)\n        {\n            e.printStackTrace();\n        }\n        <span class=\"keyword\">catch</span> (InterruptedException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">getProtocolVersion</span><span class=\"params\">(String arg0, <span class=\"keyword\">long</span> arg1)</span> <span class=\"keyword\">throws</span> IOException\n    </span>{\n        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> CronJobInfo <span class=\"title\">generateCronJob</span><span class=\"params\">()</span>\n    </span>{\n        <span class=\"comment\">// 1、创建JobDetial对象</span>\n        JobDetailInfo detail = <span class=\"keyword\">new</span> JobDetailInfo();\n        <span class=\"comment\">// 设置工作项</span>\n        detail.setJobClass(DemoTask.class);\n        detail.setName(<span class=\"string\">\"MyJob_1\"</span>);\n        detail.setGroup(<span class=\"string\">\"JobGroup_1\"</span>);\n\n        <span class=\"comment\">// 2、创建Trigger对象</span>\n        TriggerInfo trigger = <span class=\"keyword\">new</span> TriggerInfo();\n        trigger.setName(<span class=\"string\">\"Trigger_1\"</span>);\n        trigger.setGroup(<span class=\"string\">\"Trigger_Group_1\"</span>);\n        trigger.setStartTime(<span class=\"keyword\">new</span> Date());\n        <span class=\"comment\">// 设置重复停止时间，并销毁该Trigger对象</span>\n        Calendar c = Calendar.getInstance();\n        c.setTimeInMillis(System.currentTimeMillis() + <span class=\"number\">1000</span> * <span class=\"number\">1L</span>);\n        trigger.setEndTime(c.getTime());\n        <span class=\"comment\">// 设置重复间隔时间</span>\n        trigger.setRepeatInterval(<span class=\"number\">1000</span> * <span class=\"number\">1L</span>);\n        <span class=\"comment\">// 设置重复执行次数</span>\n        trigger.setRepeatCount(<span class=\"number\">3</span>);\n\n        CronJobInfo info = <span class=\"keyword\">new</span> CronJobInfo();\n        info.setJobDetailInfo(detail);\n        info.setTriggerInfo(trigger);\n\n        <span class=\"keyword\">return</span> info;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span>\n    </span>{\n        TaskScheduler ts = <span class=\"keyword\">new</span> TaskScheduler();\n    }\n\n}\n</code></pre>\n<p>demo任务类，打印信息：  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DemoTask</span> <span class=\"keyword\">implements</span> <span class=\"title\">Job</span>\n</span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(JobExecutionContext context)</span>\n            <span class=\"keyword\">throws</span> JobExecutionException\n    </span>{\n        System.out.println(<span class=\"keyword\">this</span> + <span class=\"string\">\": executing task @\"</span> + <span class=\"keyword\">new</span> Date());\n    }\n}\n</code></pre>\n<p>子节点demo，启动后连接主节点，远程调用generateCronJob方法，获得一个任务描述信息，并启动定时任务。  </p>\n<pre><code class=\"java\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TaskRunner</span>\n</span>{\n    <span class=\"keyword\">private</span> Logger logger = Logger.getLogger(getClass());\n    <span class=\"keyword\">private</span> TaskProtocol proxy;\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TaskRunner</span><span class=\"params\">()</span>\n    </span>{\n        InetSocketAddress addr = <span class=\"keyword\">new</span> InetSocketAddress(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">8888</span>);\n        <span class=\"keyword\">try</span>\n        {\n            proxy = (TaskProtocol) RPC.waitForProxy(TaskProtocol.class, <span class=\"number\">1</span>, addr,\n                    <span class=\"keyword\">new</span> Configuration());\n        }\n        <span class=\"keyword\">catch</span> (IOException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span>\n    </span>{\n        RPC.stopProxy(proxy);\n    }\n\n    <span class=\"comment\">/**\n     * 从server获取一个定时任务\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getCronJob</span><span class=\"params\">()</span>\n    </span>{\n        CronJobInfo info = proxy.generateCronJob();\n        JobDetail jobDetail = getJobDetail(info.getJobDetailInfo());\n        SimpleTrigger trigger = getTrigger(info.getTriggerInfo());\n\n        <span class=\"comment\">// 创建Scheduler对象，并配置JobDetail和Trigger对象</span>\n        SchedulerFactory sf = <span class=\"keyword\">new</span> StdSchedulerFactory();\n        Scheduler scheduler = <span class=\"keyword\">null</span>;\n        <span class=\"keyword\">try</span>\n        {\n            scheduler = sf.getScheduler();\n            scheduler.scheduleJob(jobDetail, trigger);\n            <span class=\"comment\">// 执行启动操作</span>\n            scheduler.start();\n\n        }\n        <span class=\"keyword\">catch</span> (SchedulerException e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    <span class=\"comment\">/**\n     * <span class=\"doctag\">@param</span> jobDetailInfo\n     * <span class=\"doctag\">@return</span>\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">private</span> JobDetail <span class=\"title\">getJobDetail</span><span class=\"params\">(JobDetailInfo info)</span>\n    </span>{\n        JobDetail detail = <span class=\"keyword\">new</span> JobDetail();\n        detail.setName(info.getName());\n        detail.setGroup(info.getGroup());\n        detail.setDescription(info.getDescription());\n        detail.setJobClass(info.getJobClass());\n        detail.setJobDataMap(info.getJobDataMap());\n        detail.setRequestsRecovery(info.isShouldRecover());\n        detail.setDurability(info.isDurability());\n        detail.setVolatility(info.isVolatility());\n        logger.info(<span class=\"string\">\"client get jobdetail:\"</span> + detail);\n        <span class=\"keyword\">return</span> detail;\n    }\n\n    <span class=\"comment\">/**\n     * <span class=\"doctag\">@param</span> triggerInfo\n     * <span class=\"doctag\">@return</span>\n     */</span>\n    <span class=\"function\"><span class=\"keyword\">private</span> SimpleTrigger <span class=\"title\">getTrigger</span><span class=\"params\">(TriggerInfo info)</span>\n    </span>{\n        SimpleTrigger trigger = <span class=\"keyword\">new</span> SimpleTrigger();\n        trigger.setName(info.getName());\n        trigger.setGroup(info.getGroup());\n        trigger.setDescription(info.getDescription());\n        trigger.setStartTime(info.getStartTime());\n        trigger.setEndTime(info.getEndTime());\n        trigger.setRepeatInterval(info.getRepeatInterval());\n        trigger.setRepeatCount(info.getRepeatCount());\n        logger.info(<span class=\"string\">\"client get trigger:\"</span> + trigger);\n        <span class=\"keyword\">return</span> trigger;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span>\n    </span>{\n        TaskRunner t = <span class=\"keyword\">new</span> TaskRunner();\n        t.getCronJob();\n        t.close();\n    }\n}\n</code></pre>\n<p>先启动TaskScheduler，再启动TaskRunner，结果如下：  </p>\n<blockquote>\n<p>TaskScheduler日志:<br>2013-01-20 15:42:21,661 [Socket Reader #1 for port 8888] INFO  [org.apache.hadoop.ipc.Server] – Starting Socket Reader #1 for port 8888<br>2013-01-20 15:42:21,662 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888<br>2013-01-20 15:42:21,706 [main] INFO  [org.apache.hadoop.ipc.metrics.RpcDetailedMetrics] – Initializing RPC Metrics with hostName=TaskScheduler, port=8888<br>2013-01-20 15:42:21,710 [IPC Server listener on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server listener on 8888: starting<br>2013-01-20 15:42:21,711 [IPC Server Responder] INFO  [org.apache.hadoop.ipc.Server] – IPC Server Responder: starting<br>2013-01-20 15:42:21,711 [IPC Server handler 0 on 8888] INFO  [org.apache.hadoop.ipc.Server] – IPC Server handler 0 on 8888: starting<br>2013-01-20 15:42:24,084 [IPC Server handler 0 on 8888] INFO  [org.mh.rpc.task.TaskScheduler] – generate a task: org.mh.rpc.task.JobDetailInfo@1f26605</p>\n<p>TaskRunner:<br>2013-01-20 15:42:26,323 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get jobdetail:JobDetail ‘JobGroup_1.MyJob_1′:  jobClass: ‘org.mh.rpc.quartz.GetSumTask isStateful: false isVolatile: false isDurable: false requestsRecovers: false<br>2013-01-20 15:42:26,329 [main] INFO  [org.mh.rpc.task.TaskRunner] – client get trigger:Trigger ‘Trigger_Group_1.Trigger<em>1′:  triggerClass: ‘org.quartz.SimpleTrigger isVolatile: false calendar: ‘null’ misfireInstruction: 0 nextFireTime: null<br>2013-01-20 15:42:26,382 [main] INFO  [org.quartz.simpl.SimpleThreadPool] – Job execution threads will use class loader of thread: main<br>2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.SchedulerSignalerImpl] – Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl<br>2013-01-20 15:42:26,411 [main] INFO  [org.quartz.core.QuartzScheduler] – Quartz Scheduler v.1.6.5 created.<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.simpl.RAMJobStore] – RAMJobStore initialized.<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler ‘DefaultQuartzScheduler’ initialized from default resource file in Quartz package: ‘quartz.properties’<br>2013-01-20 15:42:26,413 [main] INFO  [org.quartz.impl.StdSchedulerFactory] – Quartz scheduler version: 1.6.5<br>2013-01-20 15:42:26,415 [main] INFO  [org.quartz.core.QuartzScheduler] – Scheduler DefaultQuartzScheduler</em>$_NON_CLUSTERED started.<br>org.mh.rpc.quartz.DemoTask@1b66b06: executing task @Sun Jan 20 15:42:26 CST 2013</p>\n</blockquote>\n<p>上面是一个简单的demo，演示了如何通过RPC将任务调度给节点去执行，对于Quartz来说，任务的形式可以千变万化，关键就看怎么去使用了，分发到多个节点上执行的话，就还需要对任务的信息做更多的封装了。</p>"},{"title":"使用zookeeper协调多服务器的任务处理","date":"2012-11-13T08:16:43.000Z","_content":"\n背景\n--\n**Zookeeper**是hadoop的子项目，是google的chubby的开源实现，是一个针对大规模分布式系统的可靠的分布式协调系统。Zookeeper一般部署在一个集群上，通过在集群间维护一个数据树，使得连接到集群的client能够获得统一的数据信息，比如系统公共配置信息、节点存活状态等等。因此，在互联网公司中，zookeeper被广泛运用于统一配置管理、名字服务、分布式同步等。\n问题\n--\n我们看下这样一种场景：  \n前台系统每时每刻都生成大量数据，这些原生数据由后台系统处理完毕后再作他用，我们暂且不谈这些数据的存储形式，只关注如何能够尽可能高效的处理。举个例子，前台系统可能是微博的前端发布系统、搜索引擎上的广告投放系统，或者是任务发布系统，后台系统则可能是对微博和广告信息的审查系统，比如用户发的微博如果包含近期敏感信息则不予显示，若是任务，后台系统则负责处理任务具体的执行。  \n若数据量和任务量较小，单节点的后台系统或许可以处理得过来，但是如果数据量和任务量很大（比如新浪微博，龙年正月初一0点0分0秒，共有32312条微博同时发布），单节点的后台系统肯定吃不消，这时候，可想而知的是多节点同时处理前台过来的数据。  \n最简单的方法是，按消息id对后台节点数取模（msgid%server_num=mod），每个后台节点取自己那份数据进行处理，这就需要每个节点都知晓当前有多少个后台节点以及本节点所应取的mod数。但是，当某个节点宕机时，这个节点所应处理的数据无法被继续处理了，势必会造成阻塞，除非重新配置各节点上的参数，将节点数server_num减1，并修改各节点取数据的mod数。  \n毋庸置疑，这样非常麻烦！如果能够将这种配置信息（实际上是数据在节点间分配的控制信息）统一管理起来，在配置信息发生变化时，各个后台节点能够及时知晓其变化，就可以避免上述情况的发生。  \n因此，采用多节点处理数据时，有两个问题：  \n1.避免多个节点重复处理同一条数据，否则造成资源浪费。  \n2.不能有数据被遗漏处理，尤其是在有后台节点down掉的时候。  \n也就是说，采用多节点同时处理数据时，需要将数据隔离开，分别给不同的节点处理，而且在有节点宕机的情况下，所有数据也必须可以无误的被其他可用节点处理。如何做到这一点呢，使用zookeeper吧！  \n<!--more-->\n解决方案\n--\n我们通过zookeeper维护一个目录（比如/app/config），服务器启动时连接zookeeper集群并在该目录下创建表示自己的临时节点（CreateMode.EPHEMERAL），相当于注册一个节点，节点名可以是本服务器的ip，节点的值为该服务器的mod值，按注册顺序从0递增，即第一个注册的节点值为0，第二个为1，依次下去，因此/app/config的子节点数就是注册到zookeeper的服务器数。同时，各服务器监听/app/config目录，当其发生变化（新加入子节点、子节点失效等）时，每个服务器都将获取到这个事件并进行相应的处理。  \ndemo\n--\n下面针对以上场景给出一个示例demo。  \n**Server类**：服务器  \n**ClientThread类**：服务器上的单个线程  \n**NodeStateWatcher类**：服务器监听zookeeper集群的监听器  \n**ZkOperationImpl类**：zookeeper的操作封装（实现ZkOperation接口）  \nServer.java\n```java\npublic class Server extends Thread\n{\n private ClientThread[] clients = new ClientThread[Constant.THREAD_COUNT]; // 数据处理线程\n private ZkOperation operationCient = null; // 与zookeeper的连接\n private Watcher nodeWatcher = null;  // 向zookeeper注册的监听器\n private String name; // 服务器名\n private String ip; // 服务器ip\n \n public Server(String name, String ip) throws IOException, KeeperException, InterruptedException\n {\n  this.name = name;\n  this.ip = ip;\n  this.operationCient = new ZkOperationImpl();\n  this.nodeWatcher = new NodeStateWatcher(this);\n  this.operationCient.init(Constant.ZK_ADDRESS, nodeWatcher);\n \n  for (int i=0; i<Constant.THREAD_COUNT; ++i)\n  {\n   ClientThread c = new ClientThread(i, ip, name);\n   this.clients[i]= c;\n  }\n \n  initialize();\n }\n \n /**\n  * 向zookeeper集群注册\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n private void registerServer() throws KeeperException, InterruptedException\n {\n  List<String> children = operationCient.getChilds(Constant.ROOT_PATH);\n  int max = -1;\n  for (String childName : children)\n  {\n   String childPath = Constant.ROOT_PATH + \"/\" + childName;\n   int mod = Integer.parseInt(operationCient.getData(childPath));\n   if (mod > max)\n    max = mod;\n  }\n  String path = Constant.ROOT_PATH + \"/\" + ip;\n  operationCient.apendTempNode(path, String.valueOf(max<0 ? 0 : ++max));\n }\n \n /**\n  * 启动数据处理线程\n  * @throws Exception\n  */\n public void run()\n {\n  for (ClientThread c : clients)\n  {\n   CommonUtil.log(\"Start thread-\" + c);\n   c.start();\n  }\n }\n \n /**\n  * 服务器初始化\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n private void initialize() throws KeeperException, InterruptedException\n {\n  CommonUtil.log(\"================\");\n  CommonUtil.log(this + \" initializing...\");\n \n  // 配置信息的上级目录不存在\n  if (!operationCient.exist(Constant.ROOT_PATH))\n  {\n   System.err.println(\"Root path \" + Constant.ROOT_PATH + \"does not exist!!! Create root path...\");\n   operationCient.apendPresistentNode(Constant.ROOT_PATH, \"1\");\n   CommonUtil.log(\"Create root path \" + Constant.ROOT_PATH + \" successfully!\");\n  }\n \n  registerServer();\n \n  refreshConfig();\n \n  CommonUtil.log(this + \" finish initializing...\");\n  CommonUtil.log(\"================\");\n }\n \n /**\n  * watch到节点变化后，刷新节点数和模数\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n public void refresh() throws KeeperException, InterruptedException\n {\n  CommonUtil.log(\"================\");\n  CommonUtil.log(this + \":freshing...\");\n \n  refreshConfig();\n \n  CommonUtil.log(this + \":end freshing...\");\n  CommonUtil.log(\"================\");\n }\n \n private void refreshConfig() throws KeeperException, InterruptedException\n {\n  String version = operationCient.getData(Constant.ROOT_PATH);\n  CommonUtil.log(\"SYSTEM VERSION: \" + version);\n  List<String> children = operationCient.getChilds(Constant.ROOT_PATH);\n \n  // 1. 服务器数量为子节点的个数\n  int nodeCount = children.size();\n  CommonUtil.log(\"Server count:\" + nodeCount);\n  synchronized (CommonUtil.BASE)\n  {\n   CommonUtil.BASE = nodeCount * Constant.THREAD_COUNT;\n  }\n \n  if (CommonUtil.BASE.intValue() == 0)\n   return;\n \n  Integer mod = null;\n \n  for (String childName : children)\n  {\n   // 2. 获取本服务器的模数\n   if (childName.equals(ip))\n   {\n    String childPath = Constant.ROOT_PATH + \"/\" + childName;\n    mod = Integer.parseInt(operationCient.getData(childPath));\n    break;\n   }\n  }\n  // 3. 刷新数据处理线程的取模数\n  if (mod == null)\n  {\n   System.err.println(\"Did not get the mod number for \" + this);\n  }\n  else\n  {\n   CommonUtil.log(this + \", mod=\" + mod + \",base=\" + CommonUtil.BASE);\n   for (ClientThread c : clients)\n   {\n    c.refresh(mod);\n   }\n  }\n }\n \n public String toString()\n {\n  return this.name + \"@\" + this.ip + \"\";\n }\n \n public ClientThread[] getClients()\n {\n  return clients;\n }\n \n public ZkOperation getOperationCient()\n {\n  return operationCient;\n }\n \n public Watcher getNodeWatcher()\n {\n  return nodeWatcher;\n }\n \n public String getIp()\n {\n  return ip;\n }\n}\n```\nClientThread.java\n```java\npublic class ClientThread extends Thread\n{\n \n private Integer modNum = -1;\n private Integer threadId;\n private String ip;\n private String clientName;\n \n public ClientThread(Integer threadId, String ip, String clientName) throws IOException, KeeperException, InterruptedException\n {\n  this.threadId = threadId;\n  this.ip = ip;\n  this.clientName = clientName;\n }\n \n /**\n  * watch到节点变化后，调用刷新节点数和模数\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n public void refresh(int mod) throws KeeperException, InterruptedException\n {\n//  CommonUtil.log(\"================\");\n//  CommonUtil.log(this + \":freshing...\");\n \n  synchronized (this.modNum)\n  {\n   this.modNum = threadId + mod * Constant.THREAD_COUNT;\n  }\n \n  CommonUtil.log(this + \":\" + modNum + \"/\" + CommonUtil.BASE);\n \n//  CommonUtil.log(this + \":end freshing...\");\n//  CommonUtil.log(\"================\");\n }\n \n @Override\n public void run()\n {\n  long start = System.currentTimeMillis();\n  while (System.currentTimeMillis() - start < Constant.DURATION)\n  {\n   // 处理数据\n   processData();\n   try\n   {\n    Thread.sleep(5000); //等待2秒\n   }\n   catch (InterruptedException e)\n   {\n    e.printStackTrace();\n   }\n  }\n \n }\n \n /**\n  * 模拟处理数据逻辑：打印属于本线程的数据\n  */\n private void processData()\n {\n  if (CommonUtil.BASE.equals(0) || modNum.equals(-1))\n  {\n   CommonUtil.err(this + \": did not get server_count and modNum!!!\");\n   return;\n  }\n \n  StringBuilder sb = new StringBuilder(this + \"-\" + modNum + \"/\" + CommonUtil.BASE + \":\");\n  for (int i=0; i<Constant.NUMBERS.length; ++i)\n  {\n   int n = Constant.NUMBERS[i];\n   if (n % CommonUtil.BASE == modNum)\n   {\n    sb.append(n).append(\" \");\n   }\n  }\n  CommonUtil.log(sb.toString());\n }\n \n @Override\n public String toString()\n {\n  return \"ClientThread_\" + this.clientName + \"@\" + this.ip + \"-thread_\" + this.threadId;\n }\n \n public Integer getModNum()\n {\n  return modNum;\n }\n \n public synchronized void setModNum(Integer modNum)\n {\n  this.modNum = modNum;\n }\n \n public String getClientName()\n {\n  return clientName;\n }\n}\n```\nNodeStateWatcher.java\n```java\npublic class NodeStateWatcher implements Watcher\n{\n private Server server;\n \n public NodeStateWatcher(Server server)\n {\n  this.server = server;\n }\n \n @Override\n public void process(WatchedEvent event)\n {\n  StringBuilder outputStr = new StringBuilder();\n  if (server.getName() != null)\n  {\n   outputStr.append(server.getName() + \" get an event.\");\n  }\n  outputStr.append(\"Path:\" + event.getPath());\n  outputStr.append(\",state:\" + event.getState());\n  outputStr.append(\",type:\" + event.getType());\n  CommonUtil.log(outputStr.toString());\n \n  // 发现子节点有变化\n  if (event.getType() == EventType.NodeChildrenChanged\n    || event.getType() == EventType.NodeDataChanged\n    || event.getType() == EventType.NodeDeleted)\n  {\n   CommonUtil.log(\"In event: \" + event.getType());\n   try\n   {\n    server.refresh();\n   }\n   catch (KeeperException e)\n   {\n    e.printStackTrace();\n   }\n   catch (InterruptedException e)\n   {\n    e.printStackTrace();\n   }\n   CommonUtil.log(\"End event: \" + event.getType());\n  }\n }\n}\n```\nZkOperationImpl.java 部分zk操作代码\n```java\n@Override\n public void apendPresistentNode(String path, String data)\n   throws KeeperException, InterruptedException\n {\n  if (zk != null)\n  {\n   zk.create(path, data.getBytes(), Ids.OPEN_ACL_UNSAFE,\n     CreateMode.PERSISTENT);\n  }\n }\n \n @Override\n public void delNode(String path) throws KeeperException,\n   InterruptedException\n {\n  if (zk != null)\n  {\n   zk.delete(path, -1);\n  }\n }\n \n @Override\n public boolean exist(String path) throws KeeperException,\n   InterruptedException\n {\n  if (zk != null)\n  {\n   return zk.exists(path, true) != null;\n  }\n  return false;\n }\n}\n```\n\nMain.java：主类，启动demo\n\n```java\npublic class Main\n{\n public static void main(String[] args) throws Exception\n {\n  Server c1 = new Server(\"ServerA\", \"1.1.1.1\");\n  Server c2 = new Server(\"ServerB\", \"1.1.1.2\");\n  Server c3 = new Server(\"ServerC\", \"1.1.1.3\");\n \n  c1.start();\n  c2.start();\n  c3.start();\n }\n}\n```\n\n\n验证\n--\n由于Server的3个实例在同一台机器上运行，连接到zookeeper时，用的是一个session，所以demo中没有通过程序断开server与zookeeper的连接，如果serverA断开，那么serverB和serverC与zookeeper的session连接也会失效，达不到演示效果，所以我们只能暂时在zookeeper客户端手工更改zookeeper上的配置信息，用于模拟server与zookeeper集群断开连接和增加server的情形。server启动后，会先向zookeeper注册节点，因此我们先手工删除节点，再手工添加节点。  \n手工执行的命令如下：  \n> [zk: localhost:2181(CONNECTED) 141] delete /demo/1.1.1.3\n> [zk: localhost:2181(CONNECTED) 142] delete /demo/1.1.1.2\n> [zk: localhost:2181(CONNECTED) 143] delete /demo/1.1.1.1\n> [zk: localhost:2181(CONNECTED) 144] create -e /demo/1.1.1.1 0\n> [zk: localhost:2181(CONNECTED) 145] create -e /demo/1.1.1.2 1\n> [zk: localhost:2181(CONNECTED) 146] create -e /demo/1.1.1.3 2  \n\n可以通过程序打印信息发现，在节点配置信息每个服务器(Server)上的线程会动态的获取属于自己的数据并打印。当然，这里对数据的处理逻辑很简单，仅仅是打印出来，处理的数据也只是内存中的一个数组，对于类似这样的但是更复杂的应用场景，zookeeper同样适用，但是需要更多的考虑服务器与zookeeper集群连接的可靠性（比如session超时重连）、权限机制等等。  \n上面的demo程序打印信息如下：  \n\n> [2012-11-14 15:18:42] New zk connection session: 0\n> [2012-11-14 15:18:42] ================\n> [2012-11-14 15:18:42] ServerA@1.1.1.1 initializing…\n> [2012-11-14 15:18:47] Thread-0 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:47] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:47] In event: NodeChildrenChanged\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:47] SYSTEM VERSION: 1\n> [2012-11-14 15:18:47] SYSTEM VERSION: 1\n> [2012-11-14 15:18:47] Server count:1\n> [2012-11-14 15:18:47] Server count:1\n> [2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:18:47] ServerA@1.1.1.1 finish initializing…\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:18:47] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] End event: NodeChildrenChanged\n> [2012-11-14 15:18:47] New zk connection session: 0\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerB@1.1.1.2 initializing…\n> [2012-11-14 15:18:51] Thread-6 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:51] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:51] In event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:51] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:51] In event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] ServerA@1.1.1.1, mod=0,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_0:0/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_1:1/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_2:2/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_3:3/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_4:4/10\n> [2012-11-14 15:18:51] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] End event: NodeChildrenChanged\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:18:51] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] End event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:18:51] ServerB@1.1.1.2 finish initializing…\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] New zk connection session: 0\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerC@1.1.1.3 initializing…\n> [2012-11-14 15:18:56] Thread-12 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:56] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:56] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:18:56] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerA@1.1.1.1, mod=0,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0:0/15\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1:1/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2:2/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3:3/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4:4/15\n> [2012-11-14 15:18:56] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerB@1.1.1.2, mod=1,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0:5/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1:6/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2:7/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3:8/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4:9/15\n> [2012-11-14 15:18:56] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:18:56] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:18:56] ServerC@1.1.1.3 finish initializing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_1\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_1\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_1\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:02] Thread-0 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:02] Thread-12 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:02] Thread-6 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] Server count:2\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:07] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:07] Thread-12 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:07] In event: NodeDeleted\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:07] Thread-6 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:07] In event: NodeDeleted\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeDeleted\n> [2012-11-14 15:19:07] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeDeleted\n> [2012-11-14 15:19:07] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_1-6/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_0-10/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_0-5/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_1-11/5:\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_3-8/5:\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_2-7/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_4-9/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_2-12/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_4-14/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_3-13/5:\n> [2012-11-14 15:19:12] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:12] Thread-12 get an event.Path:/demo/1.1.1.3,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:12] In event: NodeDeleted\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:12] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeDeleted\n> [2012-11-14 15:19:12] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:20] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:20] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:20] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] Server count:1\n> Did not get the mod number for ServerC@1.1.1.3\n> [2012-11-14 15:19:20] Server count:1\n> [2012-11-14 15:19:20] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:19:20] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:20] Server count:1\n> [2012-11-14 15:19:20] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_1-6/5:\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_0-10/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_0-5/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_1-11/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_3-8/5:\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_2-7/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_2-12/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_4-9/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_4-14/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_3-13/5:\n> [2012-11-14 15:19:25] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:25] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:25] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:25] ================\n> Did not get the mod number for ServerC@1.1.1.3\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:25] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ServerA@1.1.1.1, mod=0,base=10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_0:0/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_1:1/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_2:2/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_3:3/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_4:4/10\n> [2012-11-14 15:19:25] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:31] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:31] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:31] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] ServerA@1.1.1.1, mod=0,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0:0/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1:1/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2:2/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3:3/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4:4/15\n> [2012-11-14 15:19:31] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:19:31] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ServerB@1.1.1.2, mod=1,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0:5/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1:6/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2:7/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3:8/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4:9/15\n> [2012-11-14 15:19:31] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n","source":"_posts/使用zookeeper协调多服务器的任务处理.md","raw":"---\ntitle: 使用zookeeper协调多服务器的任务处理\ndate: 2012-11-13 16:16:43\ntags: \n- zookeeper\n- 横向扩展\n- 分布式协调\ncategories: \n- 分布式应用\n---\n\n背景\n--\n**Zookeeper**是hadoop的子项目，是google的chubby的开源实现，是一个针对大规模分布式系统的可靠的分布式协调系统。Zookeeper一般部署在一个集群上，通过在集群间维护一个数据树，使得连接到集群的client能够获得统一的数据信息，比如系统公共配置信息、节点存活状态等等。因此，在互联网公司中，zookeeper被广泛运用于统一配置管理、名字服务、分布式同步等。\n问题\n--\n我们看下这样一种场景：  \n前台系统每时每刻都生成大量数据，这些原生数据由后台系统处理完毕后再作他用，我们暂且不谈这些数据的存储形式，只关注如何能够尽可能高效的处理。举个例子，前台系统可能是微博的前端发布系统、搜索引擎上的广告投放系统，或者是任务发布系统，后台系统则可能是对微博和广告信息的审查系统，比如用户发的微博如果包含近期敏感信息则不予显示，若是任务，后台系统则负责处理任务具体的执行。  \n若数据量和任务量较小，单节点的后台系统或许可以处理得过来，但是如果数据量和任务量很大（比如新浪微博，龙年正月初一0点0分0秒，共有32312条微博同时发布），单节点的后台系统肯定吃不消，这时候，可想而知的是多节点同时处理前台过来的数据。  \n最简单的方法是，按消息id对后台节点数取模（msgid%server_num=mod），每个后台节点取自己那份数据进行处理，这就需要每个节点都知晓当前有多少个后台节点以及本节点所应取的mod数。但是，当某个节点宕机时，这个节点所应处理的数据无法被继续处理了，势必会造成阻塞，除非重新配置各节点上的参数，将节点数server_num减1，并修改各节点取数据的mod数。  \n毋庸置疑，这样非常麻烦！如果能够将这种配置信息（实际上是数据在节点间分配的控制信息）统一管理起来，在配置信息发生变化时，各个后台节点能够及时知晓其变化，就可以避免上述情况的发生。  \n因此，采用多节点处理数据时，有两个问题：  \n1.避免多个节点重复处理同一条数据，否则造成资源浪费。  \n2.不能有数据被遗漏处理，尤其是在有后台节点down掉的时候。  \n也就是说，采用多节点同时处理数据时，需要将数据隔离开，分别给不同的节点处理，而且在有节点宕机的情况下，所有数据也必须可以无误的被其他可用节点处理。如何做到这一点呢，使用zookeeper吧！  \n<!--more-->\n解决方案\n--\n我们通过zookeeper维护一个目录（比如/app/config），服务器启动时连接zookeeper集群并在该目录下创建表示自己的临时节点（CreateMode.EPHEMERAL），相当于注册一个节点，节点名可以是本服务器的ip，节点的值为该服务器的mod值，按注册顺序从0递增，即第一个注册的节点值为0，第二个为1，依次下去，因此/app/config的子节点数就是注册到zookeeper的服务器数。同时，各服务器监听/app/config目录，当其发生变化（新加入子节点、子节点失效等）时，每个服务器都将获取到这个事件并进行相应的处理。  \ndemo\n--\n下面针对以上场景给出一个示例demo。  \n**Server类**：服务器  \n**ClientThread类**：服务器上的单个线程  \n**NodeStateWatcher类**：服务器监听zookeeper集群的监听器  \n**ZkOperationImpl类**：zookeeper的操作封装（实现ZkOperation接口）  \nServer.java\n```java\npublic class Server extends Thread\n{\n private ClientThread[] clients = new ClientThread[Constant.THREAD_COUNT]; // 数据处理线程\n private ZkOperation operationCient = null; // 与zookeeper的连接\n private Watcher nodeWatcher = null;  // 向zookeeper注册的监听器\n private String name; // 服务器名\n private String ip; // 服务器ip\n \n public Server(String name, String ip) throws IOException, KeeperException, InterruptedException\n {\n  this.name = name;\n  this.ip = ip;\n  this.operationCient = new ZkOperationImpl();\n  this.nodeWatcher = new NodeStateWatcher(this);\n  this.operationCient.init(Constant.ZK_ADDRESS, nodeWatcher);\n \n  for (int i=0; i<Constant.THREAD_COUNT; ++i)\n  {\n   ClientThread c = new ClientThread(i, ip, name);\n   this.clients[i]= c;\n  }\n \n  initialize();\n }\n \n /**\n  * 向zookeeper集群注册\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n private void registerServer() throws KeeperException, InterruptedException\n {\n  List<String> children = operationCient.getChilds(Constant.ROOT_PATH);\n  int max = -1;\n  for (String childName : children)\n  {\n   String childPath = Constant.ROOT_PATH + \"/\" + childName;\n   int mod = Integer.parseInt(operationCient.getData(childPath));\n   if (mod > max)\n    max = mod;\n  }\n  String path = Constant.ROOT_PATH + \"/\" + ip;\n  operationCient.apendTempNode(path, String.valueOf(max<0 ? 0 : ++max));\n }\n \n /**\n  * 启动数据处理线程\n  * @throws Exception\n  */\n public void run()\n {\n  for (ClientThread c : clients)\n  {\n   CommonUtil.log(\"Start thread-\" + c);\n   c.start();\n  }\n }\n \n /**\n  * 服务器初始化\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n private void initialize() throws KeeperException, InterruptedException\n {\n  CommonUtil.log(\"================\");\n  CommonUtil.log(this + \" initializing...\");\n \n  // 配置信息的上级目录不存在\n  if (!operationCient.exist(Constant.ROOT_PATH))\n  {\n   System.err.println(\"Root path \" + Constant.ROOT_PATH + \"does not exist!!! Create root path...\");\n   operationCient.apendPresistentNode(Constant.ROOT_PATH, \"1\");\n   CommonUtil.log(\"Create root path \" + Constant.ROOT_PATH + \" successfully!\");\n  }\n \n  registerServer();\n \n  refreshConfig();\n \n  CommonUtil.log(this + \" finish initializing...\");\n  CommonUtil.log(\"================\");\n }\n \n /**\n  * watch到节点变化后，刷新节点数和模数\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n public void refresh() throws KeeperException, InterruptedException\n {\n  CommonUtil.log(\"================\");\n  CommonUtil.log(this + \":freshing...\");\n \n  refreshConfig();\n \n  CommonUtil.log(this + \":end freshing...\");\n  CommonUtil.log(\"================\");\n }\n \n private void refreshConfig() throws KeeperException, InterruptedException\n {\n  String version = operationCient.getData(Constant.ROOT_PATH);\n  CommonUtil.log(\"SYSTEM VERSION: \" + version);\n  List<String> children = operationCient.getChilds(Constant.ROOT_PATH);\n \n  // 1. 服务器数量为子节点的个数\n  int nodeCount = children.size();\n  CommonUtil.log(\"Server count:\" + nodeCount);\n  synchronized (CommonUtil.BASE)\n  {\n   CommonUtil.BASE = nodeCount * Constant.THREAD_COUNT;\n  }\n \n  if (CommonUtil.BASE.intValue() == 0)\n   return;\n \n  Integer mod = null;\n \n  for (String childName : children)\n  {\n   // 2. 获取本服务器的模数\n   if (childName.equals(ip))\n   {\n    String childPath = Constant.ROOT_PATH + \"/\" + childName;\n    mod = Integer.parseInt(operationCient.getData(childPath));\n    break;\n   }\n  }\n  // 3. 刷新数据处理线程的取模数\n  if (mod == null)\n  {\n   System.err.println(\"Did not get the mod number for \" + this);\n  }\n  else\n  {\n   CommonUtil.log(this + \", mod=\" + mod + \",base=\" + CommonUtil.BASE);\n   for (ClientThread c : clients)\n   {\n    c.refresh(mod);\n   }\n  }\n }\n \n public String toString()\n {\n  return this.name + \"@\" + this.ip + \"\";\n }\n \n public ClientThread[] getClients()\n {\n  return clients;\n }\n \n public ZkOperation getOperationCient()\n {\n  return operationCient;\n }\n \n public Watcher getNodeWatcher()\n {\n  return nodeWatcher;\n }\n \n public String getIp()\n {\n  return ip;\n }\n}\n```\nClientThread.java\n```java\npublic class ClientThread extends Thread\n{\n \n private Integer modNum = -1;\n private Integer threadId;\n private String ip;\n private String clientName;\n \n public ClientThread(Integer threadId, String ip, String clientName) throws IOException, KeeperException, InterruptedException\n {\n  this.threadId = threadId;\n  this.ip = ip;\n  this.clientName = clientName;\n }\n \n /**\n  * watch到节点变化后，调用刷新节点数和模数\n  * @throws InterruptedException\n  * @throws KeeperException\n  */\n public void refresh(int mod) throws KeeperException, InterruptedException\n {\n//  CommonUtil.log(\"================\");\n//  CommonUtil.log(this + \":freshing...\");\n \n  synchronized (this.modNum)\n  {\n   this.modNum = threadId + mod * Constant.THREAD_COUNT;\n  }\n \n  CommonUtil.log(this + \":\" + modNum + \"/\" + CommonUtil.BASE);\n \n//  CommonUtil.log(this + \":end freshing...\");\n//  CommonUtil.log(\"================\");\n }\n \n @Override\n public void run()\n {\n  long start = System.currentTimeMillis();\n  while (System.currentTimeMillis() - start < Constant.DURATION)\n  {\n   // 处理数据\n   processData();\n   try\n   {\n    Thread.sleep(5000); //等待2秒\n   }\n   catch (InterruptedException e)\n   {\n    e.printStackTrace();\n   }\n  }\n \n }\n \n /**\n  * 模拟处理数据逻辑：打印属于本线程的数据\n  */\n private void processData()\n {\n  if (CommonUtil.BASE.equals(0) || modNum.equals(-1))\n  {\n   CommonUtil.err(this + \": did not get server_count and modNum!!!\");\n   return;\n  }\n \n  StringBuilder sb = new StringBuilder(this + \"-\" + modNum + \"/\" + CommonUtil.BASE + \":\");\n  for (int i=0; i<Constant.NUMBERS.length; ++i)\n  {\n   int n = Constant.NUMBERS[i];\n   if (n % CommonUtil.BASE == modNum)\n   {\n    sb.append(n).append(\" \");\n   }\n  }\n  CommonUtil.log(sb.toString());\n }\n \n @Override\n public String toString()\n {\n  return \"ClientThread_\" + this.clientName + \"@\" + this.ip + \"-thread_\" + this.threadId;\n }\n \n public Integer getModNum()\n {\n  return modNum;\n }\n \n public synchronized void setModNum(Integer modNum)\n {\n  this.modNum = modNum;\n }\n \n public String getClientName()\n {\n  return clientName;\n }\n}\n```\nNodeStateWatcher.java\n```java\npublic class NodeStateWatcher implements Watcher\n{\n private Server server;\n \n public NodeStateWatcher(Server server)\n {\n  this.server = server;\n }\n \n @Override\n public void process(WatchedEvent event)\n {\n  StringBuilder outputStr = new StringBuilder();\n  if (server.getName() != null)\n  {\n   outputStr.append(server.getName() + \" get an event.\");\n  }\n  outputStr.append(\"Path:\" + event.getPath());\n  outputStr.append(\",state:\" + event.getState());\n  outputStr.append(\",type:\" + event.getType());\n  CommonUtil.log(outputStr.toString());\n \n  // 发现子节点有变化\n  if (event.getType() == EventType.NodeChildrenChanged\n    || event.getType() == EventType.NodeDataChanged\n    || event.getType() == EventType.NodeDeleted)\n  {\n   CommonUtil.log(\"In event: \" + event.getType());\n   try\n   {\n    server.refresh();\n   }\n   catch (KeeperException e)\n   {\n    e.printStackTrace();\n   }\n   catch (InterruptedException e)\n   {\n    e.printStackTrace();\n   }\n   CommonUtil.log(\"End event: \" + event.getType());\n  }\n }\n}\n```\nZkOperationImpl.java 部分zk操作代码\n```java\n@Override\n public void apendPresistentNode(String path, String data)\n   throws KeeperException, InterruptedException\n {\n  if (zk != null)\n  {\n   zk.create(path, data.getBytes(), Ids.OPEN_ACL_UNSAFE,\n     CreateMode.PERSISTENT);\n  }\n }\n \n @Override\n public void delNode(String path) throws KeeperException,\n   InterruptedException\n {\n  if (zk != null)\n  {\n   zk.delete(path, -1);\n  }\n }\n \n @Override\n public boolean exist(String path) throws KeeperException,\n   InterruptedException\n {\n  if (zk != null)\n  {\n   return zk.exists(path, true) != null;\n  }\n  return false;\n }\n}\n```\n\nMain.java：主类，启动demo\n\n```java\npublic class Main\n{\n public static void main(String[] args) throws Exception\n {\n  Server c1 = new Server(\"ServerA\", \"1.1.1.1\");\n  Server c2 = new Server(\"ServerB\", \"1.1.1.2\");\n  Server c3 = new Server(\"ServerC\", \"1.1.1.3\");\n \n  c1.start();\n  c2.start();\n  c3.start();\n }\n}\n```\n\n\n验证\n--\n由于Server的3个实例在同一台机器上运行，连接到zookeeper时，用的是一个session，所以demo中没有通过程序断开server与zookeeper的连接，如果serverA断开，那么serverB和serverC与zookeeper的session连接也会失效，达不到演示效果，所以我们只能暂时在zookeeper客户端手工更改zookeeper上的配置信息，用于模拟server与zookeeper集群断开连接和增加server的情形。server启动后，会先向zookeeper注册节点，因此我们先手工删除节点，再手工添加节点。  \n手工执行的命令如下：  \n> [zk: localhost:2181(CONNECTED) 141] delete /demo/1.1.1.3\n> [zk: localhost:2181(CONNECTED) 142] delete /demo/1.1.1.2\n> [zk: localhost:2181(CONNECTED) 143] delete /demo/1.1.1.1\n> [zk: localhost:2181(CONNECTED) 144] create -e /demo/1.1.1.1 0\n> [zk: localhost:2181(CONNECTED) 145] create -e /demo/1.1.1.2 1\n> [zk: localhost:2181(CONNECTED) 146] create -e /demo/1.1.1.3 2  \n\n可以通过程序打印信息发现，在节点配置信息每个服务器(Server)上的线程会动态的获取属于自己的数据并打印。当然，这里对数据的处理逻辑很简单，仅仅是打印出来，处理的数据也只是内存中的一个数组，对于类似这样的但是更复杂的应用场景，zookeeper同样适用，但是需要更多的考虑服务器与zookeeper集群连接的可靠性（比如session超时重连）、权限机制等等。  \n上面的demo程序打印信息如下：  \n\n> [2012-11-14 15:18:42] New zk connection session: 0\n> [2012-11-14 15:18:42] ================\n> [2012-11-14 15:18:42] ServerA@1.1.1.1 initializing…\n> [2012-11-14 15:18:47] Thread-0 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:47] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:47] In event: NodeChildrenChanged\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:47] SYSTEM VERSION: 1\n> [2012-11-14 15:18:47] SYSTEM VERSION: 1\n> [2012-11-14 15:18:47] Server count:1\n> [2012-11-14 15:18:47] Server count:1\n> [2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:18:47] ServerA@1.1.1.1 finish initializing…\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:18:47] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] End event: NodeChildrenChanged\n> [2012-11-14 15:18:47] New zk connection session: 0\n> [2012-11-14 15:18:47] ================\n> [2012-11-14 15:18:47] ServerB@1.1.1.2 initializing…\n> [2012-11-14 15:18:51] Thread-6 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:51] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:51] In event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:51] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:51] In event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] SYSTEM VERSION: 1\n> [2012-11-14 15:18:51] ServerA@1.1.1.1, mod=0,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_0:0/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_1:1/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_2:2/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_3:3/10\n> [2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_4:4/10\n> [2012-11-14 15:18:51] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] End event: NodeChildrenChanged\n> [2012-11-14 15:18:51] Server count:2\n> [2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:18:51] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] End event: NodeChildrenChanged\n> [2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:18:51] ServerB@1.1.1.2 finish initializing…\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] New zk connection session: 0\n> [2012-11-14 15:18:51] ================\n> [2012-11-14 15:18:51] ServerC@1.1.1.3 initializing…\n> [2012-11-14 15:18:56] Thread-12 get an event.Path:null,state:SyncConnected,type:None\n> [2012-11-14 15:18:56] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:18:56] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:18:56] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:18:56] In event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerA@1.1.1.1, mod=0,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0:0/15\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1:1/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2:2/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3:3/15\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4:4/15\n> [2012-11-14 15:18:56] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] SYSTEM VERSION: 1\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerB@1.1.1.2, mod=1,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0:5/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1:6/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2:7/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3:8/15\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4:9/15\n> [2012-11-14 15:18:56] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] Server count:3\n> [2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:18:56] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] End event: NodeChildrenChanged\n> [2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:18:56] ServerC@1.1.1.3 finish initializing…\n> [2012-11-14 15:18:56] ================\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_0\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_1\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_1\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_1\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_2\n> [2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_3\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_4\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:02] Thread-0 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:02] Thread-12 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:02] Thread-6 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:02] In event: NodeDeleted\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] Server count:2\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeDeleted\n> [2012-11-14 15:19:02] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:02] In event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:02] SYSTEM VERSION: 1\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] Server count:2\n> [2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:02] ================\n> [2012-11-14 15:19:02] End event: NodeChildrenChanged\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:07] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:07] Thread-12 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:07] In event: NodeDeleted\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:07] Thread-6 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:07] In event: NodeDeleted\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> Did not get the mod number for ServerA@1.1.1.1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeDeleted\n> [2012-11-14 15:19:07] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeDeleted\n> [2012-11-14 15:19:07] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:07] In event: NodeChildrenChanged\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:07] SYSTEM VERSION: 1\n> [2012-11-14 15:19:07] Server count:1\n> [2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5\n> [2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5\n> [2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:07] ================\n> [2012-11-14 15:19:07] End event: NodeChildrenChanged\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_1-6/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_0-10/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_0-5/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_1-11/5:\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_3-8/5:\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_2-7/5:\n> [2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29\n> [2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_4-9/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_2-12/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_4-14/5:\n> [2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_3-13/5:\n> [2012-11-14 15:19:12] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:12] Thread-12 get an event.Path:/demo/1.1.1.3,state:SyncConnected,type:NodeDeleted\n> [2012-11-14 15:19:12] In event: NodeDeleted\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:12] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeDeleted\n> [2012-11-14 15:19:12] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:12] In event: NodeChildrenChanged\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:12] SYSTEM VERSION: 1\n> [2012-11-14 15:19:12] Server count:0\n> [2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:12] ================\n> [2012-11-14 15:19:12] End event: NodeChildrenChanged\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_0: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_1: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_2: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_3: did not get server_count and modNum!!!\n> [2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_4: did not get server_count and modNum!!!\n> [2012-11-14 15:19:20] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:20] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:20] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:20] In event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] SYSTEM VERSION: 1\n> [2012-11-14 15:19:20] Server count:1\n> Did not get the mod number for ServerC@1.1.1.3\n> [2012-11-14 15:19:20] Server count:1\n> [2012-11-14 15:19:20] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> [2012-11-14 15:19:20] ServerA@1.1.1.1, mod=0,base=5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_0:0/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_1:1/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_2:2/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_3:3/5\n> [2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_4:4/5\n> [2012-11-14 15:19:20] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> Did not get the mod number for ServerB@1.1.1.2\n> [2012-11-14 15:19:20] Server count:1\n> [2012-11-14 15:19:20] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:20] ================\n> [2012-11-14 15:19:20] End event: NodeChildrenChanged\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_1-6/5:\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_0-10/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_0-5/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_1-11/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_3-8/5:\n> [2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_2-7/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_2-12/5:\n> [2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_4-9/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_4-14/5:\n> [2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_3-13/5:\n> [2012-11-14 15:19:25] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:25] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:25] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:25] In event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] SYSTEM VERSION: 1\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:25] ================\n> Did not get the mod number for ServerC@1.1.1.3\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] Server count:2\n> [2012-11-14 15:19:25] ServerB@1.1.1.2, mod=1,base=10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_0:5/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_1:6/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_2:7/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_3:8/10\n> [2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_4:9/10\n> [2012-11-14 15:19:25] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:25] ServerA@1.1.1.1, mod=0,base=10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_0:0/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_1:1/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_2:2/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_3:3/10\n> [2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_4:4/10\n> [2012-11-14 15:19:25] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:25] ================\n> [2012-11-14 15:19:25] End event: NodeChildrenChanged\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0-10/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1-11/10:\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2-12/10:\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3-13/10:\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4-14/10:\n> [2012-11-14 15:19:31] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerA@1.1.1.1:freshing…\n> [2012-11-14 15:19:31] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerC@1.1.1.3:freshing…\n> [2012-11-14 15:19:31] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged\n> [2012-11-14 15:19:31] In event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] ServerB@1.1.1.2:freshing…\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] SYSTEM VERSION: 1\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] ServerA@1.1.1.1, mod=0,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0:0/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1:1/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2:2/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3:3/15\n> [2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4:4/15\n> [2012-11-14 15:19:31] ServerA@1.1.1.1:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:31] Server count:3\n> [2012-11-14 15:19:31] ServerC@1.1.1.3, mod=2,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0:10/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1:11/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2:12/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3:13/15\n> [2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4:14/15\n> [2012-11-14 15:19:31] ServerC@1.1.1.3:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:31] ServerB@1.1.1.2, mod=1,base=15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0:5/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1:6/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2:7/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3:8/15\n> [2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4:9/15\n> [2012-11-14 15:19:31] ServerB@1.1.1.2:end freshing…\n> [2012-11-14 15:19:31] ================\n> [2012-11-14 15:19:31] End event: NodeChildrenChanged\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22\n> [2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27\n> [2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29\n> [2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28\n","slug":"使用zookeeper协调多服务器的任务处理","published":1,"updated":"2017-01-18T09:54:11.737Z","_id":"ciy320ajp0017ifs68kn2x92u","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p><strong>Zookeeper</strong>是hadoop的子项目，是google的chubby的开源实现，是一个针对大规模分布式系统的可靠的分布式协调系统。Zookeeper一般部署在一个集群上，通过在集群间维护一个数据树，使得连接到集群的client能够获得统一的数据信息，比如系统公共配置信息、节点存活状态等等。因此，在互联网公司中，zookeeper被广泛运用于统一配置管理、名字服务、分布式同步等。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>我们看下这样一种场景：<br>前台系统每时每刻都生成大量数据，这些原生数据由后台系统处理完毕后再作他用，我们暂且不谈这些数据的存储形式，只关注如何能够尽可能高效的处理。举个例子，前台系统可能是微博的前端发布系统、搜索引擎上的广告投放系统，或者是任务发布系统，后台系统则可能是对微博和广告信息的审查系统，比如用户发的微博如果包含近期敏感信息则不予显示，若是任务，后台系统则负责处理任务具体的执行。<br>若数据量和任务量较小，单节点的后台系统或许可以处理得过来，但是如果数据量和任务量很大（比如新浪微博，龙年正月初一0点0分0秒，共有32312条微博同时发布），单节点的后台系统肯定吃不消，这时候，可想而知的是多节点同时处理前台过来的数据。<br>最简单的方法是，按消息id对后台节点数取模（msgid%server_num=mod），每个后台节点取自己那份数据进行处理，这就需要每个节点都知晓当前有多少个后台节点以及本节点所应取的mod数。但是，当某个节点宕机时，这个节点所应处理的数据无法被继续处理了，势必会造成阻塞，除非重新配置各节点上的参数，将节点数server_num减1，并修改各节点取数据的mod数。<br>毋庸置疑，这样非常麻烦！如果能够将这种配置信息（实际上是数据在节点间分配的控制信息）统一管理起来，在配置信息发生变化时，各个后台节点能够及时知晓其变化，就可以避免上述情况的发生。<br>因此，采用多节点处理数据时，有两个问题：<br>1.避免多个节点重复处理同一条数据，否则造成资源浪费。<br>2.不能有数据被遗漏处理，尤其是在有后台节点down掉的时候。<br>也就是说，采用多节点同时处理数据时，需要将数据隔离开，分别给不同的节点处理，而且在有节点宕机的情况下，所有数据也必须可以无误的被其他可用节点处理。如何做到这一点呢，使用zookeeper吧！<br><a id=\"more\"></a></p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>我们通过zookeeper维护一个目录（比如/app/config），服务器启动时连接zookeeper集群并在该目录下创建表示自己的临时节点（CreateMode.EPHEMERAL），相当于注册一个节点，节点名可以是本服务器的ip，节点的值为该服务器的mod值，按注册顺序从0递增，即第一个注册的节点值为0，第二个为1，依次下去，因此/app/config的子节点数就是注册到zookeeper的服务器数。同时，各服务器监听/app/config目录，当其发生变化（新加入子节点、子节点失效等）时，每个服务器都将获取到这个事件并进行相应的处理。  </p>\n<h2 id=\"demo\"><a href=\"#demo\" class=\"headerlink\" title=\"demo\"></a>demo</h2><p>下面针对以上场景给出一个示例demo。<br><strong>Server类</strong>：服务器<br><strong>ClientThread类</strong>：服务器上的单个线程<br><strong>NodeStateWatcher类</strong>：服务器监听zookeeper集群的监听器<br><strong>ZkOperationImpl类</strong>：zookeeper的操作封装（实现ZkOperation接口）<br>Server.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Server</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span></span></div><div class=\"line\">&#123;</div><div class=\"line\"> <span class=\"keyword\">private</span> ClientThread[] clients = <span class=\"keyword\">new</span> ClientThread[Constant.THREAD_COUNT]; <span class=\"comment\">// 数据处理线程</span></div><div class=\"line\"> <span class=\"keyword\">private</span> ZkOperation operationCient = <span class=\"keyword\">null</span>; <span class=\"comment\">// 与zookeeper的连接</span></div><div class=\"line\"> <span class=\"keyword\">private</span> Watcher nodeWatcher = <span class=\"keyword\">null</span>;  <span class=\"comment\">// 向zookeeper注册的监听器</span></div><div class=\"line\"> <span class=\"keyword\">private</span> String name; <span class=\"comment\">// 服务器名</span></div><div class=\"line\"> <span class=\"keyword\">private</span> String ip; <span class=\"comment\">// 服务器ip</span></div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Server</span><span class=\"params\">(String name, String ip)</span> <span class=\"keyword\">throws</span> IOException, KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.name = name;</div><div class=\"line\">  <span class=\"keyword\">this</span>.ip = ip;</div><div class=\"line\">  <span class=\"keyword\">this</span>.operationCient = <span class=\"keyword\">new</span> ZkOperationImpl();</div><div class=\"line\">  <span class=\"keyword\">this</span>.nodeWatcher = <span class=\"keyword\">new</span> NodeStateWatcher(<span class=\"keyword\">this</span>);</div><div class=\"line\">  <span class=\"keyword\">this</span>.operationCient.init(Constant.ZK_ADDRESS, nodeWatcher);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;Constant.THREAD_COUNT; ++i)</div><div class=\"line\">  &#123;</div><div class=\"line\">   ClientThread c = <span class=\"keyword\">new</span> ClientThread(i, ip, name);</div><div class=\"line\">   <span class=\"keyword\">this</span>.clients[i]= c;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  initialize();</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * 向zookeeper集群注册</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">registerServer</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  List&lt;String&gt; children = operationCient.getChilds(Constant.ROOT_PATH);</div><div class=\"line\">  <span class=\"keyword\">int</span> max = -<span class=\"number\">1</span>;</div><div class=\"line\">  <span class=\"keyword\">for</span> (String childName : children)</div><div class=\"line\">  &#123;</div><div class=\"line\">   String childPath = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + childName;</div><div class=\"line\">   <span class=\"keyword\">int</span> mod = Integer.parseInt(operationCient.getData(childPath));</div><div class=\"line\">   <span class=\"keyword\">if</span> (mod &gt; max)</div><div class=\"line\">    max = mod;</div><div class=\"line\">  &#125;</div><div class=\"line\">  String path = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + ip;</div><div class=\"line\">  operationCient.apendTempNode(path, String.valueOf(max&lt;<span class=\"number\">0</span> ? <span class=\"number\">0</span> : ++max));</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * 启动数据处理线程</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> Exception</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">for</span> (ClientThread c : clients)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"Start thread-\"</span> + c);</div><div class=\"line\">   c.start();</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * 服务器初始化</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">initialize</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\" initializing...\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 配置信息的上级目录不存在</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (!operationCient.exist(Constant.ROOT_PATH))</div><div class=\"line\">  &#123;</div><div class=\"line\">   System.err.println(<span class=\"string\">\"Root path \"</span> + Constant.ROOT_PATH + <span class=\"string\">\"does not exist!!! Create root path...\"</span>);</div><div class=\"line\">   operationCient.apendPresistentNode(Constant.ROOT_PATH, <span class=\"string\">\"1\"</span>);</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"Create root path \"</span> + Constant.ROOT_PATH + <span class=\"string\">\" successfully!\"</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  registerServer();</div><div class=\"line\"> </div><div class=\"line\">  refreshConfig();</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\" finish initializing...\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * watch到节点变化后，刷新节点数和模数</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">refresh</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":freshing...\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  refreshConfig();</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":end freshing...\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">refreshConfig</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  String version = operationCient.getData(Constant.ROOT_PATH);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"SYSTEM VERSION: \"</span> + version);</div><div class=\"line\">  List&lt;String&gt; children = operationCient.getChilds(Constant.ROOT_PATH);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 1. 服务器数量为子节点的个数</span></div><div class=\"line\">  <span class=\"keyword\">int</span> nodeCount = children.size();</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"Server count:\"</span> + nodeCount);</div><div class=\"line\">  <span class=\"keyword\">synchronized</span> (CommonUtil.BASE)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.BASE = nodeCount * Constant.THREAD_COUNT;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">if</span> (CommonUtil.BASE.intValue() == <span class=\"number\">0</span>)</div><div class=\"line\">   <span class=\"keyword\">return</span>;</div><div class=\"line\"> </div><div class=\"line\">  Integer mod = <span class=\"keyword\">null</span>;</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">for</span> (String childName : children)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"comment\">// 2. 获取本服务器的模数</span></div><div class=\"line\">   <span class=\"keyword\">if</span> (childName.equals(ip))</div><div class=\"line\">   &#123;</div><div class=\"line\">    String childPath = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + childName;</div><div class=\"line\">    mod = Integer.parseInt(operationCient.getData(childPath));</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// 3. 刷新数据处理线程的取模数</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (mod == <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   System.err.println(<span class=\"string\">\"Did not get the mod number for \"</span> + <span class=\"keyword\">this</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\", mod=\"</span> + mod + <span class=\"string\">\",base=\"</span> + CommonUtil.BASE);</div><div class=\"line\">   <span class=\"keyword\">for</span> (ClientThread c : clients)</div><div class=\"line\">   &#123;</div><div class=\"line\">    c.refresh(mod);</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.name + <span class=\"string\">\"@\"</span> + <span class=\"keyword\">this</span>.ip + <span class=\"string\">\"\"</span>;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"keyword\">public</span> ClientThread[] getClients()</div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> clients;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> ZkOperation <span class=\"title\">getOperationCient</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> operationCient;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> Watcher <span class=\"title\">getNodeWatcher</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> nodeWatcher;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getIp</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> ip;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>ClientThread.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ClientThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span></span></div><div class=\"line\">&#123;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"keyword\">private</span> Integer modNum = -<span class=\"number\">1</span>;</div><div class=\"line\"> <span class=\"keyword\">private</span> Integer threadId;</div><div class=\"line\"> <span class=\"keyword\">private</span> String ip;</div><div class=\"line\"> <span class=\"keyword\">private</span> String clientName;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ClientThread</span><span class=\"params\">(Integer threadId, String ip, String clientName)</span> <span class=\"keyword\">throws</span> IOException, KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.threadId = threadId;</div><div class=\"line\">  <span class=\"keyword\">this</span>.ip = ip;</div><div class=\"line\">  <span class=\"keyword\">this</span>.clientName = clientName;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * watch到节点变化后，调用刷新节点数和模数</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">refresh</span><span class=\"params\">(<span class=\"keyword\">int</span> mod)</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</span></div><div class=\"line\"> &#123;</div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(\"================\");</span></div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(this + \":freshing...\");</span></div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>.modNum)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">this</span>.modNum = threadId + mod * Constant.THREAD_COUNT;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":\"</span> + modNum + <span class=\"string\">\"/\"</span> + CommonUtil.BASE);</div><div class=\"line\"> </div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(this + \":end freshing...\");</span></div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(\"================\");</span></div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">long</span> start = System.currentTimeMillis();</div><div class=\"line\">  <span class=\"keyword\">while</span> (System.currentTimeMillis() - start &lt; Constant.DURATION)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"comment\">// 处理数据</span></div><div class=\"line\">   processData();</div><div class=\"line\">   <span class=\"keyword\">try</span></div><div class=\"line\">   &#123;</div><div class=\"line\">    Thread.sleep(<span class=\"number\">5000</span>); <span class=\"comment\">//等待2秒</span></div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</span></div><div class=\"line\">  * 模拟处理数据逻辑：打印属于本线程的数据</div><div class=\"line\">  */</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">processData</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (CommonUtil.BASE.equals(<span class=\"number\">0</span>) || modNum.equals(-<span class=\"number\">1</span>))</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.err(<span class=\"keyword\">this</span> + <span class=\"string\">\": did not get server_count and modNum!!!\"</span>);</div><div class=\"line\">   <span class=\"keyword\">return</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  StringBuilder sb = <span class=\"keyword\">new</span> StringBuilder(<span class=\"keyword\">this</span> + <span class=\"string\">\"-\"</span> + modNum + <span class=\"string\">\"/\"</span> + CommonUtil.BASE + <span class=\"string\">\":\"</span>);</div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;Constant.NUMBERS.length; ++i)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">int</span> n = Constant.NUMBERS[i];</div><div class=\"line\">   <span class=\"keyword\">if</span> (n % CommonUtil.BASE == modNum)</div><div class=\"line\">   &#123;</div><div class=\"line\">    sb.append(n).append(<span class=\"string\">\" \"</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  CommonUtil.log(sb.toString());</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"string\">\"ClientThread_\"</span> + <span class=\"keyword\">this</span>.clientName + <span class=\"string\">\"@\"</span> + <span class=\"keyword\">this</span>.ip + <span class=\"string\">\"-thread_\"</span> + <span class=\"keyword\">this</span>.threadId;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">getModNum</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> modNum;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">setModNum</span><span class=\"params\">(Integer modNum)</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.modNum = modNum;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getClientName</span><span class=\"params\">()</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> clientName;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>NodeStateWatcher.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NodeStateWatcher</span> <span class=\"keyword\">implements</span> <span class=\"title\">Watcher</span></span></div><div class=\"line\">&#123;</div><div class=\"line\"> <span class=\"keyword\">private</span> Server server;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">NodeStateWatcher</span><span class=\"params\">(Server server)</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.server = server;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(WatchedEvent event)</span></span></div><div class=\"line\"> &#123;</div><div class=\"line\">  StringBuilder outputStr = <span class=\"keyword\">new</span> StringBuilder();</div><div class=\"line\">  <span class=\"keyword\">if</span> (server.getName() != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   outputStr.append(server.getName() + <span class=\"string\">\" get an event.\"</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\">  outputStr.append(<span class=\"string\">\"Path:\"</span> + event.getPath());</div><div class=\"line\">  outputStr.append(<span class=\"string\">\",state:\"</span> + event.getState());</div><div class=\"line\">  outputStr.append(<span class=\"string\">\",type:\"</span> + event.getType());</div><div class=\"line\">  CommonUtil.log(outputStr.toString());</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 发现子节点有变化</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (event.getType() == EventType.NodeChildrenChanged</div><div class=\"line\">    || event.getType() == EventType.NodeDataChanged</div><div class=\"line\">    || event.getType() == EventType.NodeDeleted)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"In event: \"</span> + event.getType());</div><div class=\"line\">   <span class=\"keyword\">try</span></div><div class=\"line\">   &#123;</div><div class=\"line\">    server.refresh();</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"End event: \"</span> + event.getType());</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>ZkOperationImpl.java 部分zk操作代码<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">apendPresistentNode</span><span class=\"params\">(String path, String data)</span></span></div><div class=\"line\">   <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   zk.create(path, data.getBytes(), Ids.OPEN_ACL_UNSAFE,</div><div class=\"line\">     CreateMode.PERSISTENT);</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">delNode</span><span class=\"params\">(String path)</span> <span class=\"keyword\">throws</span> KeeperException,</span></div><div class=\"line\">   InterruptedException</div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   zk.delete(path, -<span class=\"number\">1</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">exist</span><span class=\"params\">(String path)</span> <span class=\"keyword\">throws</span> KeeperException,</span></div><div class=\"line\">   InterruptedException</div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">return</span> zk.exists(path, <span class=\"keyword\">true</span>) != <span class=\"keyword\">null</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Main.java：主类，启动demo</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Main</span></span></div><div class=\"line\">&#123;</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"> &#123;</div><div class=\"line\">  Server c1 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerA\"</span>, <span class=\"string\">\"1.1.1.1\"</span>);</div><div class=\"line\">  Server c2 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerB\"</span>, <span class=\"string\">\"1.1.1.2\"</span>);</div><div class=\"line\">  Server c3 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerC\"</span>, <span class=\"string\">\"1.1.1.3\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  c1.start();</div><div class=\"line\">  c2.start();</div><div class=\"line\">  c3.start();</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h2><p>由于Server的3个实例在同一台机器上运行，连接到zookeeper时，用的是一个session，所以demo中没有通过程序断开server与zookeeper的连接，如果serverA断开，那么serverB和serverC与zookeeper的session连接也会失效，达不到演示效果，所以我们只能暂时在zookeeper客户端手工更改zookeeper上的配置信息，用于模拟server与zookeeper集群断开连接和增加server的情形。server启动后，会先向zookeeper注册节点，因此我们先手工删除节点，再手工添加节点。<br>手工执行的命令如下：  </p>\n<blockquote>\n<p>[zk: localhost:2181(CONNECTED) 141] delete /demo/1.1.1.3<br>[zk: localhost:2181(CONNECTED) 142] delete /demo/1.1.1.2<br>[zk: localhost:2181(CONNECTED) 143] delete /demo/1.1.1.1<br>[zk: localhost:2181(CONNECTED) 144] create -e /demo/1.1.1.1 0<br>[zk: localhost:2181(CONNECTED) 145] create -e /demo/1.1.1.2 1<br>[zk: localhost:2181(CONNECTED) 146] create -e /demo/1.1.1.3 2  </p>\n</blockquote>\n<p>可以通过程序打印信息发现，在节点配置信息每个服务器(Server)上的线程会动态的获取属于自己的数据并打印。当然，这里对数据的处理逻辑很简单，仅仅是打印出来，处理的数据也只是内存中的一个数组，对于类似这样的但是更复杂的应用场景，zookeeper同样适用，但是需要更多的考虑服务器与zookeeper集群连接的可靠性（比如session超时重连）、权限机制等等。<br>上面的demo程序打印信息如下：  </p>\n<blockquote>\n<p>[2012-11-14 15:18:42] New zk connection session: 0<br>[2012-11-14 15:18:42] ================<br>[2012-11-14 15:18:42] ServerA@1.1.1.1 initializing…<br>[2012-11-14 15:18:47] Thread-0 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:47] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:47] In event: NodeChildrenChanged<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:47] SYSTEM VERSION: 1<br>[2012-11-14 15:18:47] SYSTEM VERSION: 1<br>[2012-11-14 15:18:47] Server count:1<br>[2012-11-14 15:18:47] Server count:1<br>[2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:18:47] ServerA@1.1.1.1 finish initializing…<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:18:47] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] End event: NodeChildrenChanged<br>[2012-11-14 15:18:47] New zk connection session: 0<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerB@1.1.1.2 initializing…<br>[2012-11-14 15:18:51] Thread-6 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:51] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:51] In event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:51] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:51] In event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] ServerA@1.1.1.1, mod=0,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_0:0/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_1:1/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_2:2/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_3:3/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_4:4/10<br>[2012-11-14 15:18:51] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] End event: NodeChildrenChanged<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:18:51] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] End event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:18:51] ServerB@1.1.1.2 finish initializing…<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] New zk connection session: 0<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerC@1.1.1.3 initializing…<br>[2012-11-14 15:18:56] Thread-12 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:56] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:56] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:18:56] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerA@1.1.1.1, mod=0,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0:0/15<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1:1/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2:2/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3:3/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4:4/15<br>[2012-11-14 15:18:56] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerB@1.1.1.2, mod=1,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0:5/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1:6/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2:7/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3:8/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4:9/15<br>[2012-11-14 15:18:56] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:18:56] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:18:56] ServerC@1.1.1.3 finish initializing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_1<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_1<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_1<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:02] Thread-0 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:02] Thread-12 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:02] Thread-6 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] Server count:2<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:07] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:07] Thread-12 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:07] In event: NodeDeleted<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:07] Thread-6 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:07] In event: NodeDeleted<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeDeleted<br>[2012-11-14 15:19:07] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeDeleted<br>[2012-11-14 15:19:07] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_1-6/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_0-10/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_0-5/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_1-11/5:<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_3-8/5:<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_2-7/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_4-9/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_2-12/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_4-14/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_3-13/5:<br>[2012-11-14 15:19:12] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:12] Thread-12 get an event.Path:/demo/1.1.1.3,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:12] In event: NodeDeleted<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:12] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeDeleted<br>[2012-11-14 15:19:12] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:20] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:20] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:20] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] Server count:1<br>Did not get the mod number for ServerC@1.1.1.3<br>[2012-11-14 15:19:20] Server count:1<br>[2012-11-14 15:19:20] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:19:20] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:20] Server count:1<br>[2012-11-14 15:19:20] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_1-6/5:<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_0-10/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_0-5/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_1-11/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_3-8/5:<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_2-7/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_2-12/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_4-9/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_4-14/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_3-13/5:<br>[2012-11-14 15:19:25] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:25] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:25] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:25] ================<br>Did not get the mod number for ServerC@1.1.1.3<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:25] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ServerA@1.1.1.1, mod=0,base=10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_0:0/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_1:1/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_2:2/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_3:3/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_4:4/10<br>[2012-11-14 15:19:25] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:31] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:31] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:31] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] ServerA@1.1.1.1, mod=0,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0:0/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1:1/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2:2/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3:3/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4:4/15<br>[2012-11-14 15:19:31] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:19:31] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ServerB@1.1.1.2, mod=1,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0:5/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1:6/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2:7/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3:8/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4:9/15<br>[2012-11-14 15:19:31] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28</p>\n</blockquote>\n","excerpt":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p><strong>Zookeeper</strong>是hadoop的子项目，是google的chubby的开源实现，是一个针对大规模分布式系统的可靠的分布式协调系统。Zookeeper一般部署在一个集群上，通过在集群间维护一个数据树，使得连接到集群的client能够获得统一的数据信息，比如系统公共配置信息、节点存活状态等等。因此，在互联网公司中，zookeeper被广泛运用于统一配置管理、名字服务、分布式同步等。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>我们看下这样一种场景：<br>前台系统每时每刻都生成大量数据，这些原生数据由后台系统处理完毕后再作他用，我们暂且不谈这些数据的存储形式，只关注如何能够尽可能高效的处理。举个例子，前台系统可能是微博的前端发布系统、搜索引擎上的广告投放系统，或者是任务发布系统，后台系统则可能是对微博和广告信息的审查系统，比如用户发的微博如果包含近期敏感信息则不予显示，若是任务，后台系统则负责处理任务具体的执行。<br>若数据量和任务量较小，单节点的后台系统或许可以处理得过来，但是如果数据量和任务量很大（比如新浪微博，龙年正月初一0点0分0秒，共有32312条微博同时发布），单节点的后台系统肯定吃不消，这时候，可想而知的是多节点同时处理前台过来的数据。<br>最简单的方法是，按消息id对后台节点数取模（msgid%server_num=mod），每个后台节点取自己那份数据进行处理，这就需要每个节点都知晓当前有多少个后台节点以及本节点所应取的mod数。但是，当某个节点宕机时，这个节点所应处理的数据无法被继续处理了，势必会造成阻塞，除非重新配置各节点上的参数，将节点数server_num减1，并修改各节点取数据的mod数。<br>毋庸置疑，这样非常麻烦！如果能够将这种配置信息（实际上是数据在节点间分配的控制信息）统一管理起来，在配置信息发生变化时，各个后台节点能够及时知晓其变化，就可以避免上述情况的发生。<br>因此，采用多节点处理数据时，有两个问题：<br>1.避免多个节点重复处理同一条数据，否则造成资源浪费。<br>2.不能有数据被遗漏处理，尤其是在有后台节点down掉的时候。<br>也就是说，采用多节点同时处理数据时，需要将数据隔离开，分别给不同的节点处理，而且在有节点宕机的情况下，所有数据也必须可以无误的被其他可用节点处理。如何做到这一点呢，使用zookeeper吧！<br>","more":"</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>我们通过zookeeper维护一个目录（比如/app/config），服务器启动时连接zookeeper集群并在该目录下创建表示自己的临时节点（CreateMode.EPHEMERAL），相当于注册一个节点，节点名可以是本服务器的ip，节点的值为该服务器的mod值，按注册顺序从0递增，即第一个注册的节点值为0，第二个为1，依次下去，因此/app/config的子节点数就是注册到zookeeper的服务器数。同时，各服务器监听/app/config目录，当其发生变化（新加入子节点、子节点失效等）时，每个服务器都将获取到这个事件并进行相应的处理。  </p>\n<h2 id=\"demo\"><a href=\"#demo\" class=\"headerlink\" title=\"demo\"></a>demo</h2><p>下面针对以上场景给出一个示例demo。<br><strong>Server类</strong>：服务器<br><strong>ClientThread类</strong>：服务器上的单个线程<br><strong>NodeStateWatcher类</strong>：服务器监听zookeeper集群的监听器<br><strong>ZkOperationImpl类</strong>：zookeeper的操作封装（实现ZkOperation接口）<br>Server.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Server</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\"> <span class=\"keyword\">private</span> ClientThread[] clients = <span class=\"keyword\">new</span> ClientThread[Constant.THREAD_COUNT]; <span class=\"comment\">// 数据处理线程</span></div><div class=\"line\"> <span class=\"keyword\">private</span> ZkOperation operationCient = <span class=\"keyword\">null</span>; <span class=\"comment\">// 与zookeeper的连接</span></div><div class=\"line\"> <span class=\"keyword\">private</span> Watcher nodeWatcher = <span class=\"keyword\">null</span>;  <span class=\"comment\">// 向zookeeper注册的监听器</span></div><div class=\"line\"> <span class=\"keyword\">private</span> String name; <span class=\"comment\">// 服务器名</span></div><div class=\"line\"> <span class=\"keyword\">private</span> String ip; <span class=\"comment\">// 服务器ip</span></div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Server</span><span class=\"params\">(String name, String ip)</span> <span class=\"keyword\">throws</span> IOException, KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.name = name;</div><div class=\"line\">  <span class=\"keyword\">this</span>.ip = ip;</div><div class=\"line\">  <span class=\"keyword\">this</span>.operationCient = <span class=\"keyword\">new</span> ZkOperationImpl();</div><div class=\"line\">  <span class=\"keyword\">this</span>.nodeWatcher = <span class=\"keyword\">new</span> NodeStateWatcher(<span class=\"keyword\">this</span>);</div><div class=\"line\">  <span class=\"keyword\">this</span>.operationCient.init(Constant.ZK_ADDRESS, nodeWatcher);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;Constant.THREAD_COUNT; ++i)</div><div class=\"line\">  &#123;</div><div class=\"line\">   ClientThread c = <span class=\"keyword\">new</span> ClientThread(i, ip, name);</div><div class=\"line\">   <span class=\"keyword\">this</span>.clients[i]= c;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  initialize();</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * 向zookeeper集群注册</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">registerServer</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  List&lt;String&gt; children = operationCient.getChilds(Constant.ROOT_PATH);</div><div class=\"line\">  <span class=\"keyword\">int</span> max = -<span class=\"number\">1</span>;</div><div class=\"line\">  <span class=\"keyword\">for</span> (String childName : children)</div><div class=\"line\">  &#123;</div><div class=\"line\">   String childPath = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + childName;</div><div class=\"line\">   <span class=\"keyword\">int</span> mod = Integer.parseInt(operationCient.getData(childPath));</div><div class=\"line\">   <span class=\"keyword\">if</span> (mod &gt; max)</div><div class=\"line\">    max = mod;</div><div class=\"line\">  &#125;</div><div class=\"line\">  String path = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + ip;</div><div class=\"line\">  operationCient.apendTempNode(path, String.valueOf(max&lt;<span class=\"number\">0</span> ? <span class=\"number\">0</span> : ++max));</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * 启动数据处理线程</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> Exception</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">for</span> (ClientThread c : clients)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"Start thread-\"</span> + c);</div><div class=\"line\">   c.start();</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * 服务器初始化</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">initialize</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\" initializing...\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 配置信息的上级目录不存在</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (!operationCient.exist(Constant.ROOT_PATH))</div><div class=\"line\">  &#123;</div><div class=\"line\">   System.err.println(<span class=\"string\">\"Root path \"</span> + Constant.ROOT_PATH + <span class=\"string\">\"does not exist!!! Create root path...\"</span>);</div><div class=\"line\">   operationCient.apendPresistentNode(Constant.ROOT_PATH, <span class=\"string\">\"1\"</span>);</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"Create root path \"</span> + Constant.ROOT_PATH + <span class=\"string\">\" successfully!\"</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  registerServer();</div><div class=\"line\"> </div><div class=\"line\">  refreshConfig();</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\" finish initializing...\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * watch到节点变化后，刷新节点数和模数</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">refresh</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":freshing...\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  refreshConfig();</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":end freshing...\"</span>);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"================\"</span>);</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">refreshConfig</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  String version = operationCient.getData(Constant.ROOT_PATH);</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"SYSTEM VERSION: \"</span> + version);</div><div class=\"line\">  List&lt;String&gt; children = operationCient.getChilds(Constant.ROOT_PATH);</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 1. 服务器数量为子节点的个数</span></div><div class=\"line\">  <span class=\"keyword\">int</span> nodeCount = children.size();</div><div class=\"line\">  CommonUtil.log(<span class=\"string\">\"Server count:\"</span> + nodeCount);</div><div class=\"line\">  <span class=\"keyword\">synchronized</span> (CommonUtil.BASE)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.BASE = nodeCount * Constant.THREAD_COUNT;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">if</span> (CommonUtil.BASE.intValue() == <span class=\"number\">0</span>)</div><div class=\"line\">   <span class=\"keyword\">return</span>;</div><div class=\"line\"> </div><div class=\"line\">  Integer mod = <span class=\"keyword\">null</span>;</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">for</span> (String childName : children)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"comment\">// 2. 获取本服务器的模数</span></div><div class=\"line\">   <span class=\"keyword\">if</span> (childName.equals(ip))</div><div class=\"line\">   &#123;</div><div class=\"line\">    String childPath = Constant.ROOT_PATH + <span class=\"string\">\"/\"</span> + childName;</div><div class=\"line\">    mod = Integer.parseInt(operationCient.getData(childPath));</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// 3. 刷新数据处理线程的取模数</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (mod == <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   System.err.println(<span class=\"string\">\"Did not get the mod number for \"</span> + <span class=\"keyword\">this</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\", mod=\"</span> + mod + <span class=\"string\">\",base=\"</span> + CommonUtil.BASE);</div><div class=\"line\">   <span class=\"keyword\">for</span> (ClientThread c : clients)</div><div class=\"line\">   &#123;</div><div class=\"line\">    c.refresh(mod);</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.name + <span class=\"string\">\"@\"</span> + <span class=\"keyword\">this</span>.ip + <span class=\"string\">\"\"</span>;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"keyword\">public</span> ClientThread[] getClients()</div><div class=\"line\"> &#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> clients;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> ZkOperation <span class=\"title\">getOperationCient</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> operationCient;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> Watcher <span class=\"title\">getNodeWatcher</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> nodeWatcher;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getIp</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> ip;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>ClientThread.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ClientThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"keyword\">private</span> Integer modNum = -<span class=\"number\">1</span>;</div><div class=\"line\"> <span class=\"keyword\">private</span> Integer threadId;</div><div class=\"line\"> <span class=\"keyword\">private</span> String ip;</div><div class=\"line\"> <span class=\"keyword\">private</span> String clientName;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ClientThread</span><span class=\"params\">(Integer threadId, String ip, String clientName)</span> <span class=\"keyword\">throws</span> IOException, KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.threadId = threadId;</div><div class=\"line\">  <span class=\"keyword\">this</span>.ip = ip;</div><div class=\"line\">  <span class=\"keyword\">this</span>.clientName = clientName;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * watch到节点变化后，调用刷新节点数和模数</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> InterruptedException</div><div class=\"line\">  * <span class=\"doctag\">@throws</span> KeeperException</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">refresh</span><span class=\"params\">(<span class=\"keyword\">int</span> mod)</span> <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(\"================\");</span></div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(this + \":freshing...\");</span></div><div class=\"line\"> </div><div class=\"line\">  <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>.modNum)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">this</span>.modNum = threadId + mod * Constant.THREAD_COUNT;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  CommonUtil.log(<span class=\"keyword\">this</span> + <span class=\"string\">\":\"</span> + modNum + <span class=\"string\">\"/\"</span> + CommonUtil.BASE);</div><div class=\"line\"> </div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(this + \":end freshing...\");</span></div><div class=\"line\"><span class=\"comment\">//  CommonUtil.log(\"================\");</span></div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">long</span> start = System.currentTimeMillis();</div><div class=\"line\">  <span class=\"keyword\">while</span> (System.currentTimeMillis() - start &lt; Constant.DURATION)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"comment\">// 处理数据</span></div><div class=\"line\">   processData();</div><div class=\"line\">   <span class=\"keyword\">try</span></div><div class=\"line\">   &#123;</div><div class=\"line\">    Thread.sleep(<span class=\"number\">5000</span>); <span class=\"comment\">//等待2秒</span></div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"comment\">/**</div><div class=\"line\">  * 模拟处理数据逻辑：打印属于本线程的数据</div><div class=\"line\">  */</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">processData</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (CommonUtil.BASE.equals(<span class=\"number\">0</span>) || modNum.equals(-<span class=\"number\">1</span>))</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.err(<span class=\"keyword\">this</span> + <span class=\"string\">\": did not get server_count and modNum!!!\"</span>);</div><div class=\"line\">   <span class=\"keyword\">return</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\"> </div><div class=\"line\">  StringBuilder sb = <span class=\"keyword\">new</span> StringBuilder(<span class=\"keyword\">this</span> + <span class=\"string\">\"-\"</span> + modNum + <span class=\"string\">\"/\"</span> + CommonUtil.BASE + <span class=\"string\">\":\"</span>);</div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;Constant.NUMBERS.length; ++i)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">int</span> n = Constant.NUMBERS[i];</div><div class=\"line\">   <span class=\"keyword\">if</span> (n % CommonUtil.BASE == modNum)</div><div class=\"line\">   &#123;</div><div class=\"line\">    sb.append(n).append(<span class=\"string\">\" \"</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  CommonUtil.log(sb.toString());</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"string\">\"ClientThread_\"</span> + <span class=\"keyword\">this</span>.clientName + <span class=\"string\">\"@\"</span> + <span class=\"keyword\">this</span>.ip + <span class=\"string\">\"-thread_\"</span> + <span class=\"keyword\">this</span>.threadId;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">getModNum</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> modNum;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">setModNum</span><span class=\"params\">(Integer modNum)</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.modNum = modNum;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getClientName</span><span class=\"params\">()</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> clientName;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>NodeStateWatcher.java<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NodeStateWatcher</span> <span class=\"keyword\">implements</span> <span class=\"title\">Watcher</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\"> <span class=\"keyword\">private</span> Server server;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">NodeStateWatcher</span><span class=\"params\">(Server server)</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>.server = server;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(WatchedEvent event)</span></div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  StringBuilder outputStr = <span class=\"keyword\">new</span> StringBuilder();</div><div class=\"line\">  <span class=\"keyword\">if</span> (server.getName() != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   outputStr.append(server.getName() + <span class=\"string\">\" get an event.\"</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\">  outputStr.append(<span class=\"string\">\"Path:\"</span> + event.getPath());</div><div class=\"line\">  outputStr.append(<span class=\"string\">\",state:\"</span> + event.getState());</div><div class=\"line\">  outputStr.append(<span class=\"string\">\",type:\"</span> + event.getType());</div><div class=\"line\">  CommonUtil.log(outputStr.toString());</div><div class=\"line\"> </div><div class=\"line\">  <span class=\"comment\">// 发现子节点有变化</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (event.getType() == EventType.NodeChildrenChanged</div><div class=\"line\">    || event.getType() == EventType.NodeDataChanged</div><div class=\"line\">    || event.getType() == EventType.NodeDeleted)</div><div class=\"line\">  &#123;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"In event: \"</span> + event.getType());</div><div class=\"line\">   <span class=\"keyword\">try</span></div><div class=\"line\">   &#123;</div><div class=\"line\">    server.refresh();</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (KeeperException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"keyword\">catch</span> (InterruptedException e)</div><div class=\"line\">   &#123;</div><div class=\"line\">    e.printStackTrace();</div><div class=\"line\">   &#125;</div><div class=\"line\">   CommonUtil.log(<span class=\"string\">\"End event: \"</span> + event.getType());</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>ZkOperationImpl.java 部分zk操作代码<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">apendPresistentNode</span><span class=\"params\">(String path, String data)</span></div><div class=\"line\">   <span class=\"keyword\">throws</span> KeeperException, InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   zk.create(path, data.getBytes(), Ids.OPEN_ACL_UNSAFE,</div><div class=\"line\">     CreateMode.PERSISTENT);</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">delNode</span><span class=\"params\">(String path)</span> <span class=\"keyword\">throws</span> KeeperException,</div><div class=\"line\">   InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   zk.delete(path, -<span class=\"number\">1</span>);</div><div class=\"line\">  &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"> </div><div class=\"line\"> <span class=\"meta\">@Override</span></div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">exist</span><span class=\"params\">(String path)</span> <span class=\"keyword\">throws</span> KeeperException,</div><div class=\"line\">   InterruptedException</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (zk != <span class=\"keyword\">null</span>)</div><div class=\"line\">  &#123;</div><div class=\"line\">   <span class=\"keyword\">return</span> zk.exists(path, <span class=\"keyword\">true</span>) != <span class=\"keyword\">null</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Main.java：主类，启动demo</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Main</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception</div><div class=\"line\"> </span>&#123;</div><div class=\"line\">  Server c1 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerA\"</span>, <span class=\"string\">\"1.1.1.1\"</span>);</div><div class=\"line\">  Server c2 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerB\"</span>, <span class=\"string\">\"1.1.1.2\"</span>);</div><div class=\"line\">  Server c3 = <span class=\"keyword\">new</span> Server(<span class=\"string\">\"ServerC\"</span>, <span class=\"string\">\"1.1.1.3\"</span>);</div><div class=\"line\"> </div><div class=\"line\">  c1.start();</div><div class=\"line\">  c2.start();</div><div class=\"line\">  c3.start();</div><div class=\"line\"> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h2><p>由于Server的3个实例在同一台机器上运行，连接到zookeeper时，用的是一个session，所以demo中没有通过程序断开server与zookeeper的连接，如果serverA断开，那么serverB和serverC与zookeeper的session连接也会失效，达不到演示效果，所以我们只能暂时在zookeeper客户端手工更改zookeeper上的配置信息，用于模拟server与zookeeper集群断开连接和增加server的情形。server启动后，会先向zookeeper注册节点，因此我们先手工删除节点，再手工添加节点。<br>手工执行的命令如下：  </p>\n<blockquote>\n<p>[zk: localhost:2181(CONNECTED) 141] delete /demo/1.1.1.3<br>[zk: localhost:2181(CONNECTED) 142] delete /demo/1.1.1.2<br>[zk: localhost:2181(CONNECTED) 143] delete /demo/1.1.1.1<br>[zk: localhost:2181(CONNECTED) 144] create -e /demo/1.1.1.1 0<br>[zk: localhost:2181(CONNECTED) 145] create -e /demo/1.1.1.2 1<br>[zk: localhost:2181(CONNECTED) 146] create -e /demo/1.1.1.3 2  </p>\n</blockquote>\n<p>可以通过程序打印信息发现，在节点配置信息每个服务器(Server)上的线程会动态的获取属于自己的数据并打印。当然，这里对数据的处理逻辑很简单，仅仅是打印出来，处理的数据也只是内存中的一个数组，对于类似这样的但是更复杂的应用场景，zookeeper同样适用，但是需要更多的考虑服务器与zookeeper集群连接的可靠性（比如session超时重连）、权限机制等等。<br>上面的demo程序打印信息如下：  </p>\n<blockquote>\n<p>[2012-11-14 15:18:42] New zk connection session: 0<br>[2012-11-14 15:18:42] ================<br>[2012-11-14 15:18:42] ServerA@1.1.1.1 initializing…<br>[2012-11-14 15:18:47] Thread-0 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:47] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:47] In event: NodeChildrenChanged<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:47] SYSTEM VERSION: 1<br>[2012-11-14 15:18:47] SYSTEM VERSION: 1<br>[2012-11-14 15:18:47] Server count:1<br>[2012-11-14 15:18:47] Server count:1<br>[2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:18:47] ServerA@1.1.1.1 finish initializing…<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:18:47] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:18:47] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] End event: NodeChildrenChanged<br>[2012-11-14 15:18:47] New zk connection session: 0<br>[2012-11-14 15:18:47] ================<br>[2012-11-14 15:18:47] ServerB@1.1.1.2 initializing…<br>[2012-11-14 15:18:51] Thread-6 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:51] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:51] In event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:51] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:51] In event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] SYSTEM VERSION: 1<br>[2012-11-14 15:18:51] ServerA@1.1.1.1, mod=0,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_0:0/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_1:1/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_2:2/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_3:3/10<br>[2012-11-14 15:18:51] ClientThread_ServerA@1.1.1.1-thread_4:4/10<br>[2012-11-14 15:18:51] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] End event: NodeChildrenChanged<br>[2012-11-14 15:18:51] Server count:2<br>[2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:18:51] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] End event: NodeChildrenChanged<br>[2012-11-14 15:18:51] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:18:51] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:18:51] ServerB@1.1.1.2 finish initializing…<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] New zk connection session: 0<br>[2012-11-14 15:18:51] ================<br>[2012-11-14 15:18:51] ServerC@1.1.1.3 initializing…<br>[2012-11-14 15:18:56] Thread-12 get an event.Path:null,state:SyncConnected,type:None<br>[2012-11-14 15:18:56] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:18:56] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:18:56] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:18:56] In event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerA@1.1.1.1, mod=0,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0:0/15<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1:1/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2:2/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3:3/15<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4:4/15<br>[2012-11-14 15:18:56] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] SYSTEM VERSION: 1<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerB@1.1.1.2, mod=1,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0:5/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1:6/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2:7/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3:8/15<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4:9/15<br>[2012-11-14 15:18:56] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] Server count:3<br>[2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:18:56] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] End event: NodeChildrenChanged<br>[2012-11-14 15:18:56] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:18:56] ServerC@1.1.1.3 finish initializing…<br>[2012-11-14 15:18:56] ================<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_0<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_1<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_1<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerA@1.1.1.1-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_1<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerB@1.1.1.2-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:18:56] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_2<br>[2012-11-14 15:18:56] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_3<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:18:56] Start thread-ClientThread_ServerC@1.1.1.3-thread_4<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:18:56] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:01] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:01] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:19:01] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:02] Thread-0 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:02] Thread-12 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:02] Thread-6 get an event.Path:/demo/1.1.1.1,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:02] In event: NodeDeleted<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] Server count:2<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeDeleted<br>[2012-11-14 15:19:02] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:02] In event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:02] SYSTEM VERSION: 1<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] Server count:2<br>[2012-11-14 15:19:02] ServerC@1.1.1.3, mod=2,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_0:10/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_1:11/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_2:12/10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_3:13/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:02] ClientThread_ServerC@1.1.1.3-thread_4:14/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:02] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:02] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:02] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:02] ================<br>[2012-11-14 15:19:02] End event: NodeChildrenChanged<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:06] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:06] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:06] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:07] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:07] Thread-12 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:07] In event: NodeDeleted<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:07] Thread-6 get an event.Path:/demo/1.1.1.2,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:07] In event: NodeDeleted<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>Did not get the mod number for ServerA@1.1.1.1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeDeleted<br>[2012-11-14 15:19:07] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:freshing…<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeDeleted<br>[2012-11-14 15:19:07] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:07] In event: NodeChildrenChanged<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:07] SYSTEM VERSION: 1<br>[2012-11-14 15:19:07] Server count:1<br>[2012-11-14 15:19:07] ServerC@1.1.1.3, mod=2,base=5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_0:10/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_1:11/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_2:12/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_3:13/5<br>[2012-11-14 15:19:07] ClientThread_ServerC@1.1.1.3-thread_4:14/5<br>[2012-11-14 15:19:07] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:07] ================<br>[2012-11-14 15:19:07] End event: NodeChildrenChanged<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_1-6/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_0-10/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_0-5/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_1-11/5:<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_3-8/5:<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_2-7/5:<br>[2012-11-14 15:19:11] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29<br>[2012-11-14 15:19:11] ClientThread_ServerB@1.1.1.2-thread_4-9/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_2-12/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_4-14/5:<br>[2012-11-14 15:19:11] ClientThread_ServerC@1.1.1.3-thread_3-13/5:<br>[2012-11-14 15:19:12] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:12] Thread-12 get an event.Path:/demo/1.1.1.3,state:SyncConnected,type:NodeDeleted<br>[2012-11-14 15:19:12] In event: NodeDeleted<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:12] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeDeleted<br>[2012-11-14 15:19:12] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:12] In event: NodeChildrenChanged<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:12] SYSTEM VERSION: 1<br>[2012-11-14 15:19:12] Server count:0<br>[2012-11-14 15:19:12] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:12] ================<br>[2012-11-14 15:19:12] End event: NodeChildrenChanged<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_0: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerA@1.1.1.1-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_1: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerB@1.1.1.2-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_2: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_3: did not get server_count and modNum!!!<br>[2012-11-14 15:19:16] ClientThread_ServerC@1.1.1.3-thread_4: did not get server_count and modNum!!!<br>[2012-11-14 15:19:20] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:20] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:20] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:20] In event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] SYSTEM VERSION: 1<br>[2012-11-14 15:19:20] Server count:1<br>Did not get the mod number for ServerC@1.1.1.3<br>[2012-11-14 15:19:20] Server count:1<br>[2012-11-14 15:19:20] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>[2012-11-14 15:19:20] ServerA@1.1.1.1, mod=0,base=5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_0:0/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_1:1/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_2:2/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_3:3/5<br>[2012-11-14 15:19:20] ClientThread_ServerA@1.1.1.1-thread_4:4/5<br>[2012-11-14 15:19:20] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>Did not get the mod number for ServerB@1.1.1.2<br>[2012-11-14 15:19:20] Server count:1<br>[2012-11-14 15:19:20] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:20] ================<br>[2012-11-14 15:19:20] End event: NodeChildrenChanged<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_1-6/5:<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_0-0/5:5 10 15 20 25 30<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_2-2/5:2 7 12 17 22 27<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_1-1/5:1 6 11 16 21 26<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_3-3/5:3 8 13 18 23 28<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_0-10/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_0-5/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_1-11/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_3-8/5:<br>[2012-11-14 15:19:21] ClientThread_ServerA@1.1.1.1-thread_4-4/5:4 9 14 19 24 29<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_2-7/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_2-12/5:<br>[2012-11-14 15:19:21] ClientThread_ServerB@1.1.1.2-thread_4-9/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_4-14/5:<br>[2012-11-14 15:19:21] ClientThread_ServerC@1.1.1.3-thread_3-13/5:<br>[2012-11-14 15:19:25] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:25] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:25] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:25] In event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] SYSTEM VERSION: 1<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:25] ================<br>Did not get the mod number for ServerC@1.1.1.3<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] Server count:2<br>[2012-11-14 15:19:25] ServerB@1.1.1.2, mod=1,base=10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_0:5/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_1:6/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_2:7/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_3:8/10<br>[2012-11-14 15:19:25] ClientThread_ServerB@1.1.1.2-thread_4:9/10<br>[2012-11-14 15:19:25] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:25] ServerA@1.1.1.1, mod=0,base=10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_0:0/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_1:1/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_2:2/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_3:3/10<br>[2012-11-14 15:19:25] ClientThread_ServerA@1.1.1.1-thread_4:4/10<br>[2012-11-14 15:19:25] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:25] ================<br>[2012-11-14 15:19:25] End event: NodeChildrenChanged<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:26] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:26] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:26] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1-1/10:1 11 21<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0-0/10:10 20 30<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0-5/10:5 15 25<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0-10/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2-2/10:2 12 22<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3-3/10:3 13 23<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1-6/10:6 16 26<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1-11/10:<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4-4/10:4 14 24<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3-8/10:8 18 28<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2-7/10:7 17 27<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4-9/10:9 19 29<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2-12/10:<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3-13/10:<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4-14/10:<br>[2012-11-14 15:19:31] Thread-0 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerA@1.1.1.1:freshing…<br>[2012-11-14 15:19:31] Thread-12 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerC@1.1.1.3:freshing…<br>[2012-11-14 15:19:31] Thread-6 get an event.Path:/demo,state:SyncConnected,type:NodeChildrenChanged<br>[2012-11-14 15:19:31] In event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] ServerB@1.1.1.2:freshing…<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] SYSTEM VERSION: 1<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] ServerA@1.1.1.1, mod=0,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_0:0/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_1:1/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_2:2/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_3:3/15<br>[2012-11-14 15:19:31] ClientThread_ServerA@1.1.1.1-thread_4:4/15<br>[2012-11-14 15:19:31] ServerA@1.1.1.1:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:31] Server count:3<br>[2012-11-14 15:19:31] ServerC@1.1.1.3, mod=2,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_0:10/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_1:11/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_2:12/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_3:13/15<br>[2012-11-14 15:19:31] ClientThread_ServerC@1.1.1.3-thread_4:14/15<br>[2012-11-14 15:19:31] ServerC@1.1.1.3:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:31] ServerB@1.1.1.2, mod=1,base=15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_0:5/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_1:6/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_2:7/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_3:8/15<br>[2012-11-14 15:19:31] ClientThread_ServerB@1.1.1.2-thread_4:9/15<br>[2012-11-14 15:19:31] ServerB@1.1.1.2:end freshing…<br>[2012-11-14 15:19:31] ================<br>[2012-11-14 15:19:31] End event: NodeChildrenChanged<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:36] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:36] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:36] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_0-10/15:10 25<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_0-0/15:15 30<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_1-6/15:6 21<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_3-3/15:3 18<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_0-5/15:5 20<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_1-1/15:1 16<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_2-2/15:2 17<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_3-8/15:8 23<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_2-7/15:7 22<br>[2012-11-14 15:19:41] ClientThread_ServerA@1.1.1.1-thread_4-4/15:4 19<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_1-11/15:11 26<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_2-12/15:12 27<br>[2012-11-14 15:19:41] ClientThread_ServerB@1.1.1.2-thread_4-9/15:9 24<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_4-14/15:14 29<br>[2012-11-14 15:19:41] ClientThread_ServerC@1.1.1.3-thread_3-13/15:13 28</p>\n</blockquote>"}],"PostAsset":[],"PostCategory":[{"post_id":"ciy320aid0002ifs67an9x82r","category_id":"ciy320ain0005ifs6dzlnba6z","_id":"ciy320aj0000cifs61e9jaiij"},{"post_id":"ciy320aii0003ifs6w7m9s1oj","category_id":"ciy320aix000aifs6bleqwoza","_id":"ciy320aj6000iifs6gtv5q96j"},{"post_id":"ciy320aj3000gifs632u1shq2","category_id":"ciy320aj2000fifs6l8nt5ft1","_id":"ciy320aj8000nifs6j8n94sls"},{"post_id":"ciy320aio0006ifs6zlkq920m","category_id":"ciy320aj2000fifs6l8nt5ft1","_id":"ciy320aja000rifs6qq1q3wk8"},{"post_id":"ciy320air0007ifs6u0ptvqng","category_id":"ciy320aj6000kifs6g9ksjiex","_id":"ciy320ajh000wifs6ut3js4e9"},{"post_id":"ciy320ait0009ifs6vrz84mzo","category_id":"ciy320aix000aifs6bleqwoza","_id":"ciy320ajl0012ifs6f0njnfto"},{"post_id":"ciy320aiy000bifs65yltt3lk","category_id":"ciy320ajh000xifs6istm24ao","_id":"ciy320ajs0018ifs6nrsewb9g"},{"post_id":"ciy320ajm0014ifs6j9g6f22o","category_id":"ciy320aix000aifs6bleqwoza","_id":"ciy320ajt001bifs6h6cum08m"},{"post_id":"ciy320aj1000difs6qafrusls","category_id":"ciy320aj2000fifs6l8nt5ft1","_id":"ciy320aju001difs6xffa47pj"},{"post_id":"ciy320aj5000hifs6ql2q06q0","category_id":"ciy320ajt0019ifs6btbm5z6j","_id":"ciy320ajv001hifs6vo1vk2qf"},{"post_id":"ciy320aj7000lifs6yuyf0aty","category_id":"ciy320aju001fifs6gl5fnma6","_id":"ciy320ajy001lifs6ltih2juq"},{"post_id":"ciy320ajb000tifs6xajsgejs","category_id":"ciy320ajv001iifs6wl78olid","_id":"ciy320ak3001pifs6e360oc7w"},{"post_id":"ciy320ajd000vifs6kpl1n75r","category_id":"ciy320ajv001iifs6wl78olid","_id":"ciy320ak5001tifs6936qwrf7"},{"post_id":"ciy320aji000zifs6p6j3ri0y","category_id":"ciy320ak3001qifs6y1wqwe9r","_id":"ciy320ak8001yifs6ry9788ns"},{"post_id":"ciy320ajj0011ifs6qph0xggi","category_id":"ciy320ajv001iifs6wl78olid","_id":"ciy320aka0021ifs6qxnmw7mv"},{"post_id":"ciy320ajp0017ifs68kn2x92u","category_id":"ciy320ak3001qifs6y1wqwe9r","_id":"ciy320akb0023ifs652qn874a"}],"PostTag":[{"post_id":"ciy320aid0002ifs67an9x82r","tag_id":"ciy320ail0004ifs6ubbtri8q","_id":"ciy320aj8000mifs6r69xppfd"},{"post_id":"ciy320aid0002ifs67an9x82r","tag_id":"ciy320ait0008ifs6pdsesuaw","_id":"ciy320aja000pifs6zyxu7tax"},{"post_id":"ciy320aid0002ifs67an9x82r","tag_id":"ciy320aj2000eifs6xd8rvnz0","_id":"ciy320ajd000uifs6rmlgt2w8"},{"post_id":"ciy320aii0003ifs6w7m9s1oj","tag_id":"ciy320aj6000jifs6anxaavff","_id":"ciy320ajj0010ifs6bmps8nq3"},{"post_id":"ciy320aii0003ifs6w7m9s1oj","tag_id":"ciy320aja000qifs6ivmoyxb0","_id":"ciy320ajl0013ifs6b9qyqeie"},{"post_id":"ciy320aio0006ifs6zlkq920m","tag_id":"ciy320ajh000yifs6jh8tmmdy","_id":"ciy320ajt001cifs6jveqrhck"},{"post_id":"ciy320aio0006ifs6zlkq920m","tag_id":"ciy320ajo0016ifs6cvw6op8v","_id":"ciy320aju001eifs6o4e9xa6z"},{"post_id":"ciy320air0007ifs6u0ptvqng","tag_id":"ciy320ajt001aifs6mkrb1boj","_id":"ciy320ajy001kifs62l03osy8"},{"post_id":"ciy320air0007ifs6u0ptvqng","tag_id":"ciy320aju001gifs6ad87fzm8","_id":"ciy320ajz001mifs6szo4jfj5"},{"post_id":"ciy320ait0009ifs6vrz84mzo","tag_id":"ciy320aj6000jifs6anxaavff","_id":"ciy320ak4001sifs64ycyf86h"},{"post_id":"ciy320ait0009ifs6vrz84mzo","tag_id":"ciy320ak0001oifs6xbz86szb","_id":"ciy320ak6001uifs6zhhbgsui"},{"post_id":"ciy320aiy000bifs65yltt3lk","tag_id":"ciy320ak4001rifs682s5jwda","_id":"ciy320ak7001xifs6dacwtube"},{"post_id":"ciy320aj1000difs6qafrusls","tag_id":"ciy320ajh000yifs6jh8tmmdy","_id":"ciy320akc0025ifs6iw5qfy6f"},{"post_id":"ciy320aj1000difs6qafrusls","tag_id":"ciy320ak80020ifs6116nhhb8","_id":"ciy320akc0026ifs64wvprbsd"},{"post_id":"ciy320aj1000difs6qafrusls","tag_id":"ciy320aka0022ifs607y3kfpv","_id":"ciy320akc0028ifs6010x2owe"},{"post_id":"ciy320aj3000gifs632u1shq2","tag_id":"ciy320akb0024ifs6leahdo7f","_id":"ciy320akc002bifs65misptpc"},{"post_id":"ciy320aj3000gifs632u1shq2","tag_id":"ciy320akc0027ifs6i8n35qku","_id":"ciy320akc002cifs62in71xrs"},{"post_id":"ciy320aj3000gifs632u1shq2","tag_id":"ciy320akc0029ifs62gq46v9u","_id":"ciy320akd002eifs6vbp5n626"},{"post_id":"ciy320aj5000hifs6ql2q06q0","tag_id":"ciy320akc002aifs62h9txc2f","_id":"ciy320akd002gifs6cod8wpar"},{"post_id":"ciy320aj5000hifs6ql2q06q0","tag_id":"ciy320akd002difs6mmdvkcsr","_id":"ciy320akd002hifs6kq4zdift"},{"post_id":"ciy320aj7000lifs6yuyf0aty","tag_id":"ciy320akd002fifs6j3eq7hni","_id":"ciy320ake002kifs6tbs75wri"},{"post_id":"ciy320aj7000lifs6yuyf0aty","tag_id":"ciy320akd002iifs6gldut12w","_id":"ciy320ake002lifs6732pho86"},{"post_id":"ciy320aj8000oifs6cxiq9le2","tag_id":"ciy320ak4001rifs682s5jwda","_id":"ciy320ake002oifs697gd0t34"},{"post_id":"ciy320aj8000oifs6cxiq9le2","tag_id":"ciy320ake002mifs632722x9m","_id":"ciy320ake002pifs6ho4enta0"},{"post_id":"ciy320ajb000tifs6xajsgejs","tag_id":"ciy320ake002nifs6grjbabcn","_id":"ciy320akf002sifs6hg9lfrtn"},{"post_id":"ciy320ajb000tifs6xajsgejs","tag_id":"ciy320ake002qifs6eu73w0wv","_id":"ciy320akf002tifs6jdeqcyun"},{"post_id":"ciy320ajd000vifs6kpl1n75r","tag_id":"ciy320ake002rifs6dh1b0i7z","_id":"ciy320akf002xifs6t1zvy5n9"},{"post_id":"ciy320ajd000vifs6kpl1n75r","tag_id":"ciy320akf002uifs6t6mwgkeg","_id":"ciy320akf002yifs6xkz7h4fy"},{"post_id":"ciy320ajd000vifs6kpl1n75r","tag_id":"ciy320akf002vifs664rxirk8","_id":"ciy320akf0030ifs6yy2d4gkd"},{"post_id":"ciy320aji000zifs6p6j3ri0y","tag_id":"ciy320akf002wifs6a7arvegf","_id":"ciy320akg0033ifs692yelwfr"},{"post_id":"ciy320aji000zifs6p6j3ri0y","tag_id":"ciy320akf002zifs666ch9cjd","_id":"ciy320akh0034ifs6o1kizr8d"},{"post_id":"ciy320aji000zifs6p6j3ri0y","tag_id":"ciy320akg0031ifs6xlxyq9ac","_id":"ciy320akh0036ifs62fzq5jsj"},{"post_id":"ciy320ajj0011ifs6qph0xggi","tag_id":"ciy320akg0032ifs62pf6f6tn","_id":"ciy320akh0038ifs692sd7lp1"},{"post_id":"ciy320ajj0011ifs6qph0xggi","tag_id":"ciy320akh0035ifs615srlcww","_id":"ciy320akh0039ifs6o9ygj6xr"},{"post_id":"ciy320ajm0014ifs6j9g6f22o","tag_id":"ciy320aj6000jifs6anxaavff","_id":"ciy320akj003difs6fxf0hi1b"},{"post_id":"ciy320ajm0014ifs6j9g6f22o","tag_id":"ciy320akh0037ifs6ijtkpida","_id":"ciy320akj003eifs61hjhvezl"},{"post_id":"ciy320ajm0014ifs6j9g6f22o","tag_id":"ciy320akh003aifs6gn4g25rh","_id":"ciy320akj003gifs6gk1m2in3"},{"post_id":"ciy320ajm0014ifs6j9g6f22o","tag_id":"ciy320akf002zifs666ch9cjd","_id":"ciy320akj003hifs6zdz8jfw4"},{"post_id":"ciy320ajp0017ifs68kn2x92u","tag_id":"ciy320akf002wifs6a7arvegf","_id":"ciy320akk003jifs6a8s4i0oi"},{"post_id":"ciy320ajp0017ifs68kn2x92u","tag_id":"ciy320akj003fifs662iy9rwa","_id":"ciy320akk003kifs6fk662m28"},{"post_id":"ciy320ajp0017ifs68kn2x92u","tag_id":"ciy320akg0031ifs6xlxyq9ac","_id":"ciy320akk003lifs6kjbi8akk"}],"Tag":[{"name":"Spark","_id":"ciy320ail0004ifs6ubbtri8q"},{"name":"Yarn","_id":"ciy320ait0008ifs6pdsesuaw"},{"name":"内存分配","_id":"ciy320aj2000eifs6xd8rvnz0"},{"name":"hadoop","_id":"ciy320aj6000jifs6anxaavff"},{"name":"yarn","_id":"ciy320aja000qifs6ivmoyxb0"},{"name":"Storm","_id":"ciy320ajh000yifs6jh8tmmdy"},{"name":"实时计算","_id":"ciy320ajo0016ifs6cvw6op8v"},{"name":"kafka","_id":"ciy320ajt001aifs6mkrb1boj"},{"name":"源码分析","_id":"ciy320aju001gifs6ad87fzm8"},{"name":"eclipse","_id":"ciy320ak0001oifs6xbz86szb"},{"name":"shell","_id":"ciy320ak4001rifs682s5jwda"},{"name":"Supervisor","_id":"ciy320ak80020ifs6116nhhb8"},{"name":"异常排查","_id":"ciy320aka0022ifs607y3kfpv"},{"name":"storm","_id":"ciy320akb0024ifs6leahdo7f"},{"name":"源码编译","_id":"ciy320akc0027ifs6i8n35qku"},{"name":"本地调试","_id":"ciy320akc0029ifs62gq46v9u"},{"name":"redis","_id":"ciy320akc002aifs62h9txc2f"},{"name":"连接断开","_id":"ciy320akd002difs6mmdvkcsr"},{"name":"hexo","_id":"ciy320akd002fifs6j3eq7hni"},{"name":"gitpage","_id":"ciy320akd002iifs6gldut12w"},{"name":"linux","_id":"ciy320ake002mifs632722x9m"},{"name":"httpclient","_id":"ciy320ake002nifs6grjbabcn"},{"name":"tcp连接数","_id":"ciy320ake002qifs6eu73w0wv"},{"name":"swap分区","_id":"ciy320ake002rifs6dh1b0i7z"},{"name":"问题分析","_id":"ciy320akf002uifs6t6mwgkeg"},{"name":"Jvm调优","_id":"ciy320akf002vifs664rxirk8"},{"name":"zookeeper","_id":"ciy320akf002wifs6a7arvegf"},{"name":"分布式应用","_id":"ciy320akf002zifs666ch9cjd"},{"name":"分布式协调","_id":"ciy320akg0031ifs6xlxyq9ac"},{"name":"web开发","_id":"ciy320akg0032ifs62pf6f6tn"},{"name":"java","_id":"ciy320akh0035ifs615srlcww"},{"name":"RPC","_id":"ciy320akh0037ifs6ijtkpida"},{"name":"任务调度","_id":"ciy320akh003aifs6gn4g25rh"},{"name":"横向扩展","_id":"ciy320akj003fifs662iy9rwa"}]}}